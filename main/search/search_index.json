{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p><code>batchtensor</code> is a lightweight library built on top of PyTorch to manipulate nested data structures with PyTorch tensors. This library provides functions for tensors where the first dimension is the batch dimension. It also provides functions for tensors representing a batch of sequences where the first dimension is the batch dimension and the second dimension is the sequence dimension.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Nested Structure Support: Work with dictionaries, lists, and tuples containing tensors</li> <li>Batch Operations: Efficiently process batches of data along the batch dimension</li> <li>Sequence Operations: Handle sequential/temporal data along the sequence dimension</li> <li>Consistent API: Unified interface for both single tensors and nested structures</li> <li>Type Safety: Fully typed with comprehensive type hints</li> <li>Well Documented: Extensive documentation with examples for all functions</li> <li>Lightweight: Minimal dependencies (PyTorch and coola)</li> <li>Performance: Leverages PyTorch's optimized operations</li> </ul>"},{"location":"#main-modules","title":"Main Modules","text":"<ul> <li>batchtensor.nested: Operations for nested data structures</li> <li>batchtensor.tensor: Operations for individual tensors</li> <li>batchtensor.utils: Utility functions for seed management</li> <li>batchtensor.constants: Dimension constants</li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>Let's imagine you have a batch which is represented by a dictionary with three tensors, and you want to take the first 2 items. <code>batchtensor</code> provides the function <code>slice_along_batch</code> that allows slicing all the tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n...     \"c\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n... }\n&gt;&gt;&gt; slice_along_batch(batch, stop=2)\n{'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3]), 'c': tensor([1., 2.])}\n</code></pre> <p>Similarly, it is possible to split a batch into multiple batches by using the function <code>split_along_batch</code>:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n...     \"c\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n... }\n&gt;&gt;&gt; split_along_batch(batch, split_size_or_sections=2)\n({'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3]), 'c': tensor([1., 2.])},\n {'a': tensor([[4, 9], [8, 1]]), 'b': tensor([2, 1]), 'c': tensor([3., 4.])},\n {'a': tensor([[5, 7]]), 'b': tensor([0]), 'c': tensor([5.])})\n</code></pre> <p>Please check the user guide and API reference to see all the implemented functions and detailed examples.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Get Started: Installation instructions</li> <li>User Guide: Comprehensive tutorials and examples</li> <li>API Reference: Complete function documentation</li> <li>GitHub Repository: Source code and issue tracker</li> </ul>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>batchtensor</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>batchtensor</code> to a new version will possibly break any code that was using the old version of <code>batchtensor</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>batchtensor</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"architecture/","title":"Architecture and Design","text":"<p>This document provides an overview of the <code>batchtensor</code> library architecture, design principles, and implementation details.</p>"},{"location":"architecture/#library-structure","title":"Library Structure","text":"<p>The <code>batchtensor</code> library is organized into four main modules:</p> <pre><code>batchtensor/\n\u251c\u2500\u2500 nested/         # Operations for nested data structures\n\u251c\u2500\u2500 tensor/         # Operations for individual tensors\n\u251c\u2500\u2500 utils/          # Utility functions (seed management)\n\u2514\u2500\u2500 constants.py    # Dimension constants\n</code></pre>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#1-consistency","title":"1. Consistency","text":"<p>All functions in batchtensor follow consistent conventions:</p> <ul> <li>Batch dimension is always dimension 0: The first dimension of tensors represents the batch</li> <li>Sequence dimension is always dimension 1: The second dimension represents sequences/time steps</li> <li>Function naming: Functions are named with the pattern <code>operation_along_dimension</code></li> </ul>"},{"location":"architecture/#2-separation-of-concerns","title":"2. Separation of Concerns","text":"<p>The library separates operations into two levels:</p> <ul> <li><code>tensor</code> module: Low-level operations on individual tensors</li> <li><code>nested</code> module: High-level operations that recursively apply tensor operations to nested   structures</li> </ul> <p>This separation allows users to:</p> <ul> <li>Use tensor operations directly when working with single tensors</li> <li>Use nested operations when working with complex data structures</li> <li>Compose operations from both modules as needed</li> </ul>"},{"location":"architecture/#3-minimal-dependencies","title":"3. Minimal Dependencies","text":"<p>The library has minimal dependencies:</p> <ul> <li>PyTorch: Core tensor operations</li> <li>coola: Recursive operations on nested structures</li> </ul> <p>This keeps the library lightweight and reduces potential conflicts with other packages.</p>"},{"location":"architecture/#4-type-safety","title":"4. Type Safety","text":"<p>All functions include comprehensive type hints:</p> <ul> <li>Input and output types are explicitly declared</li> <li>Generic types are used appropriately</li> <li>TYPE_CHECKING blocks avoid runtime overhead</li> </ul>"},{"location":"architecture/#module-details","title":"Module Details","text":""},{"location":"architecture/#constants-module","title":"Constants Module","text":"<p>Defines dimension indices used throughout the library:</p> <ul> <li><code>BATCH_DIM = 0</code>: Identifies the batch dimension</li> <li><code>SEQ_DIM = 1</code>: Identifies the sequence dimension</li> </ul> <p>Using constants instead of magic numbers improves code clarity and maintainability.</p>"},{"location":"architecture/#tensor-module","title":"Tensor Module","text":"<p>The tensor module provides low-level operations for individual tensors. It's organized into sub-modules by operation type:</p> <ul> <li>slicing.py: Slice, chunk, split operations</li> <li>indexing.py: Index selection operations</li> <li>joining.py: Concatenation and repetition</li> <li>reduction.py: Sum, mean, min, max, median, etc.</li> <li>comparison.py: Sorting and comparison</li> <li>math.py: Cumulative operations</li> <li>permutation.py: Shuffling and permuting</li> </ul> <p>Each function:</p> <ul> <li>Operates on a single PyTorch tensor</li> <li>Assumes standard dimension conventions (batch=0, seq=1)</li> <li>Returns a new tensor (or tuple of tensors)</li> <li>Includes comprehensive docstrings with examples</li> </ul>"},{"location":"architecture/#nested-module","title":"Nested Module","text":"<p>The nested module provides high-level operations for nested data structures. It's organized to mirror the tensor module:</p> <ul> <li>slicing.py: Nested slicing operations</li> <li>indexing.py: Nested index selection</li> <li>joining.py: Nested concatenation and repetition</li> <li>reduction.py: Nested reductions</li> <li>comparison.py: Nested sorting</li> <li>math.py: Nested cumulative operations</li> <li>permutation.py: Nested shuffling and permuting</li> <li>conversion.py: NumPy conversion</li> <li>pointwise.py: Element-wise operations</li> <li>trigo.py: Trigonometric functions</li> <li>misc.py: Miscellaneous utilities</li> </ul> <p>Each nested function:</p> <ul> <li>Recursively applies the corresponding tensor operation</li> <li>Preserves the nested structure (dict, list, tuple)</li> <li>Uses <code>coola.recursive_apply</code> for recursive traversal</li> <li>Handles arbitrary nesting depth</li> </ul>"},{"location":"architecture/#utils-module","title":"Utils Module","text":"<p>The utils module provides supporting functionality:</p> <ul> <li>seed.py: Random seed management for reproducibility</li> <li><code>get_random_seed()</code>: Generate deterministic random seeds</li> <li><code>get_torch_generator()</code>: Create PyTorch generators</li> <li><code>setup_torch_generator()</code>: Flexible generator setup</li> </ul>"},{"location":"architecture/#implementation-patterns","title":"Implementation Patterns","text":""},{"location":"architecture/#pattern-1-tensor-operations-use-constants","title":"Pattern 1: Tensor Operations Use Constants","text":"<pre><code>import torch\nfrom batchtensor.constants import BATCH_DIM\n\n\ndef sum_along_batch(tensor: torch.Tensor, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Sum all elements along the batch dimension.\n\n    Args:\n        tensor: The input tensor.\n        keepdim: Whether to keep the reduced dimension.\n\n    Returns:\n        The sum along the batch dimension.\n    \"\"\"\n    return tensor.sum(dim=BATCH_DIM, keepdim=keepdim)\n</code></pre> <p>This ensures consistency and makes the code self-documenting.</p>"},{"location":"architecture/#pattern-2-nested-operations-delegate-to-tensor-operations","title":"Pattern 2: Nested Operations Delegate to Tensor Operations","text":"<pre><code>from functools import partial\nfrom typing import Any\nfrom coola.recursive import recursive_apply\nfrom batchtensor import tensor as bt\n\n\ndef slice_along_batch(\n    data: Any,\n    start: int | None = None,\n    stop: int | None = None,\n    step: int | None = None,\n) -&gt; Any:\n    \"\"\"Slice all tensors along the batch dimension.\n\n    Args:\n        data: Nested structure containing tensors.\n        start: Start index.\n        stop: Stop index.\n        step: Step size.\n\n    Returns:\n        Sliced nested structure.\n    \"\"\"\n    return recursive_apply(\n        data, partial(bt.slice_along_batch, start=start, stop=stop, step=step)\n    )\n</code></pre> <p>This reduces code duplication and ensures nested operations behave consistently.</p>"},{"location":"architecture/#pattern-3-dictionary-operations-preserve-structure","title":"Pattern 3: Dictionary Operations Preserve Structure","text":"<pre><code>from collections.abc import Hashable\nimport torch\nfrom batchtensor import tensor as bt\n\n\ndef chunk_along_batch(\n    data: dict[Hashable, torch.Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, torch.Tensor], ...]:\n    \"\"\"Split all tensors into chunks along the batch dimension.\n\n    Args:\n        data: Dictionary of tensors.\n        chunks: Number of chunks.\n\n    Returns:\n        Tuple of dictionaries with chunked tensors.\n    \"\"\"\n    keys = data.keys()\n    return tuple(\n        dict(zip(keys, values))\n        for values in zip(\n            *[bt.chunk_along_batch(tensor, chunks) for tensor in data.values()]\n        )\n    )\n</code></pre> <p>This pattern ensures the output structure matches the input structure.</p>"},{"location":"architecture/#extension-points","title":"Extension Points","text":"<p>The library can be extended in several ways:</p>"},{"location":"architecture/#adding-new-tensor-operations","title":"Adding New Tensor Operations","text":"<ol> <li>Add the function to the appropriate sub-module in <code>tensor/</code></li> <li>Follow existing naming conventions</li> <li>Include comprehensive docstring with example</li> <li>Export from <code>tensor/__init__.py</code></li> </ol>"},{"location":"architecture/#adding-new-nested-operations","title":"Adding New Nested Operations","text":"<ol> <li>Add the corresponding function to <code>nested/</code></li> <li>Use <code>recursive_apply</code> to delegate to tensor operations</li> <li>Export from <code>nested/__init__.py</code></li> </ol>"},{"location":"architecture/#adding-new-data-types","title":"Adding New Data Types","text":"<p>The nested operations work with any data structure that <code>coola.recursive_apply</code> supports:</p> <ul> <li>Dictionaries</li> <li>Lists</li> <li>Tuples</li> <li>Custom classes (with appropriate handlers)</li> </ul>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Operations use views when possible (e.g., <code>slice</code>, <code>select</code>)</li> <li>Avoid unnecessary copies</li> <li>Leverage PyTorch's memory management</li> </ul>"},{"location":"architecture/#computational-efficiency","title":"Computational Efficiency","text":"<ul> <li>Delegate to PyTorch's optimized operations</li> <li>Minimize Python overhead</li> <li>Support GPU acceleration through PyTorch</li> </ul>"},{"location":"architecture/#nested-structure-overhead","title":"Nested Structure Overhead","text":"<ul> <li>Recursive operations have minimal overhead</li> <li>Dictionary access is O(1)</li> <li>Most time is spent in PyTorch operations, not structure traversal</li> </ul>"},{"location":"architecture/#testing-strategy","title":"Testing Strategy","text":"<p>The library uses comprehensive testing:</p> <ul> <li>Unit tests: Test individual functions with various inputs</li> <li>Integration tests: Test combinations of operations</li> <li>Doctests: Verify examples in docstrings</li> <li>Type checking: Use pyright for static type checking</li> </ul>"},{"location":"architecture/#future-directions","title":"Future Directions","text":"<p>Potential areas for expansion:</p> <ol> <li>Additional operations: More mathematical and statistical functions</li> <li>Custom data structures: Support for more complex nested types</li> <li>Performance optimizations: Specialized implementations for common patterns</li> <li>Batch dataset utilities: Higher-level abstractions for common workflows</li> </ol>"},{"location":"architecture/#see-also","title":"See Also","text":"<ul> <li>Tensor Operations Guide</li> <li>Nested Operations Guide</li> <li>Utils Guide</li> <li>Constants Documentation</li> </ul>"},{"location":"get_started/","title":"Get Started","text":"<p>This guide will help you install <code>batchtensor</code> and verify your installation.</p>"},{"location":"get_started/#prerequisites","title":"Prerequisites","text":"<p><code>batchtensor</code> requires:</p> <ul> <li>Python 3.10 or later</li> <li>PyTorch 2.4 or later</li> <li>A compatible operating system (Linux or macOS)</li> </ul> <p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-uv-pip-recommended","title":"Installing with <code>uv pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>uv pip install batchtensor\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>batchtensor</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>uv pip install 'batchtensor[all]'\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>batchtensor</code> from source, you can follow the steps below.</p>"},{"location":"get_started/#prerequisites_1","title":"Prerequisites","text":"<p>The project uses <code>uv</code> for dependency management. Please refer to the uv documentation for installation instructions.</p> <p>You can verify the installation by running:</p> <pre><code>uv --version\n</code></pre>"},{"location":"get_started/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone git@github.com:durandtibo/batchtensor.git\ncd batchtensor\n</code></pre>"},{"location":"get_started/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>It is recommended to create a Python 3.10+ virtual environment. You can create a virtual environment with <code>uv</code>:</p> <pre><code>inv create-venv\n</code></pre> <p>Alternatively, you can use the Makefile shortcut which also installs all dependencies:</p> <pre><code>make setup-venv\nsource .venv/bin/activate\n</code></pre>"},{"location":"get_started/#install-dependencies","title":"Install dependencies","text":"<p>Install all dependencies using <code>uv</code>:</p> <pre><code>inv install\n</code></pre> <p>To install with documentation dependencies:</p> <pre><code>inv install --docs-deps\n</code></pre>"},{"location":"get_started/#verify-the-installation","title":"Verify the installation","text":"<p>Run the test suite to verify everything is working:</p> <pre><code>inv unit-test --cov\n</code></pre>"},{"location":"get_started/#next-steps","title":"Next Steps","text":"<p>After installation, explore the documentation:</p> <ul> <li>Tensor Operations Guide: Learn about single tensor operations</li> <li>Nested Operations Guide: Learn about nested structure operations</li> <li>Utils Guide: Learn about utility functions</li> <li>API Reference: Browse the complete API</li> </ul>"},{"location":"get_started/#quick-example","title":"Quick Example","text":"<p>Here's a simple example to verify your installation:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; # Take the first 2 samples\n&gt;&gt;&gt; slice_along_batch(batch, stop=2)\n{'features': tensor([[1, 2], [3, 4]]), 'labels': tensor([0, 1])}\n</code></pre> <p>If this runs without errors, your installation is successful!</p>"},{"location":"get_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"get_started/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure that:</p> <ol> <li>You're using Python 3.10 or later</li> <li>PyTorch is properly installed</li> <li>Your virtual environment is activated (if using one)</li> </ol>"},{"location":"get_started/#pytorch-installation","title":"PyTorch Installation","text":"<p>If PyTorch is not installed, install it following the official PyTorch installation guide.</p>"},{"location":"refs/constants/","title":"Constants API Reference","text":"<p>This page provides the complete API reference for the <code>batchtensor.constants</code> module.</p> <p>The constants module defines dimension indices used throughout the library to identify batch and sequence dimensions, ensuring consistency across all operations.</p> <p>For more information about dimension conventions, see the Constants User Guide.</p>"},{"location":"refs/constants/#batchtensor.constants","title":"batchtensor.constants","text":"<p>Defines important constants for batch and sequence dimensions.</p> <p>This module provides standardized dimension indices used throughout the batchtensor package. These constants ensure consistent dimension handling across all tensor operations.</p> Constants <p>BATCH_DIM: The batch dimension index (0). This is the first dimension     of tensors and represents independent samples in a batch. For example,     in a tensor of shape (batch_size, seq_len, feature_dim), this refers     to the batch_size dimension. SEQ_DIM: The sequence dimension index (1). This is the second dimension     of tensors and represents sequential data within each batch item.     For example, in a tensor of shape (batch_size, seq_len, feature_dim),     this refers to the seq_len dimension.</p> Example <pre><code>&gt;&gt;&gt; from batchtensor.constants import BATCH_DIM, SEQ_DIM\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; # Create a batch of 3 sequences, each of length 5\n&gt;&gt;&gt; tensor = torch.randn(3, 5, 10)  # (batch, seq, features)\n&gt;&gt;&gt; # BATCH_DIM=0 refers to the dimension with size 3\n&gt;&gt;&gt; # SEQ_DIM=1 refers to the dimension with size 5\n&gt;&gt;&gt; tensor.shape[BATCH_DIM]\n3\n&gt;&gt;&gt; tensor.shape[SEQ_DIM]\n5\n</code></pre>"},{"location":"refs/constants/#batchtensor.constants.BATCH_DIM","title":"batchtensor.constants.BATCH_DIM  <code>module-attribute</code>","text":"<pre><code>BATCH_DIM = 0\n</code></pre> <p>int: The index of the batch dimension in tensors.</p> <p>This constant is used throughout batchtensor to identify the batch dimension, which is always assumed to be the first dimension (index 0) of tensors.</p>"},{"location":"refs/constants/#batchtensor.constants.SEQ_DIM","title":"batchtensor.constants.SEQ_DIM  <code>module-attribute</code>","text":"<pre><code>SEQ_DIM = 1\n</code></pre> <p>int: The index of the sequence dimension in tensors.</p> <p>This constant is used throughout batchtensor to identify the sequence dimension, which is always assumed to be the second dimension (index 1) of tensors.</p>"},{"location":"refs/nested/","title":"Nested Module API Reference","text":"<p>This page provides the complete API reference for the <code>batchtensor.nested</code> module.</p> <p>The nested module contains functions for manipulating nested data structures (dictionaries, lists, tuples) containing PyTorch tensors. All operations recursively apply to tensors within the nested structure while preserving the structure.</p> <p>For usage examples and tutorials, see the Nested Operations User Guide.</p>"},{"location":"refs/nested/#batchtensor.nested","title":"batchtensor.nested","text":"<p>Functions for manipulating nested data structures containing PyTorch tensors.</p> <p>This module provides functions for working with nested data structures (dictionaries, lists, tuples) that contain PyTorch tensors. All functions recursively apply operations to every tensor in the structure while preserving the structure itself.</p> <p>The module uses the <code>coola</code> library's recursive application capabilities to handle arbitrary nesting levels and mixed data structures. This allows you to work with complex batched data (e.g., a dictionary of lists of tensors) using the same simple API as individual tensors.</p> All functions in this module follow these conventions <ul> <li>Functions ending with <code>_along_batch</code> operate on the batch dimension (dim=0)</li> <li>Functions ending with <code>_along_seq</code> operate on the sequence dimension (dim=1)</li> <li>All tensors in the nested structure must have compatible shapes for the   operation being performed</li> <li>The output structure mirrors the input structure</li> </ul> Function Categories <p>Reduction operations: Aggregate values along a dimension     - <code>sum_along_batch</code>, <code>mean_along_batch</code>, <code>max_along_batch</code>, etc.</p> <p>Slicing operations: Extract subsets of data     - <code>slice_along_batch</code>, <code>select_along_batch</code>, <code>chunk_along_batch</code>     - <code>slice_along_seq</code>, <code>select_along_seq</code>, <code>chunk_along_seq</code></p> <p>Joining operations: Combine multiple nested structures     - <code>cat_along_batch</code>, <code>cat_along_seq</code>, <code>repeat_along_seq</code></p> <p>Permutation operations: Reorder elements     - <code>permute_along_batch</code>, <code>shuffle_along_batch</code>     - <code>permute_along_seq</code>, <code>shuffle_along_seq</code></p> <p>Indexing operations: Select specific elements     - <code>index_select_along_batch</code>, <code>index_select_along_seq</code></p> <p>Comparison operations: Sort and find extrema     - <code>sort_along_batch</code>, <code>argsort_along_batch</code></p> <p>Mathematical operations: Element-wise and cumulative operations     - <code>cumsum_along_batch</code>, <code>cumprod_along_batch</code>     - <code>abs</code>, <code>exp</code>, <code>log</code>, <code>clamp</code></p> <p>Trigonometric operations: Standard trigonometric functions     - <code>sin</code>, <code>cos</code>, <code>tan</code>, <code>asin</code>, <code>acos</code>, <code>atan</code>     - <code>sinh</code>, <code>cosh</code>, <code>tanh</code>, <code>asinh</code>, <code>acosh</code>, <code>atanh</code></p> <p>Conversion operations: Convert between tensor types and formats     - <code>as_tensor</code>, <code>from_numpy</code>, <code>to_numpy</code>, <code>to</code></p> Example <p>Working with nested dictionaries:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor import nested as bn\n&gt;&gt;&gt; # Create a nested batch structure\n&gt;&gt;&gt; batch = {\n...     \"inputs\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 0]),\n...     \"metadata\": {\"ids\": torch.tensor([10, 20, 30])},\n... }\n&gt;&gt;&gt; # Slice the first 2 samples from all tensors\n&gt;&gt;&gt; bn.slice_along_batch(batch, stop=2)\n{'inputs': tensor([[1, 2], [3, 4]]),\n 'labels': tensor([0, 1]),\n 'metadata': {'ids': tensor([10, 20])}}\n</code></pre> <p>Working with nested lists:</p> <pre><code>&gt;&gt;&gt; data = [\n...     torch.tensor([[1, 2], [3, 4]]),\n...     torch.tensor([[5, 6], [7, 8]]),\n... ]\n&gt;&gt;&gt; # Apply absolute value to all tensors\n&gt;&gt;&gt; bn.abs(data)\n[tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]])]\n</code></pre> See Also <p><code>batchtensor.tensor</code>: Similar functions for individual tensors.</p>"},{"location":"refs/nested/#batchtensor.nested.abs","title":"batchtensor.nested.abs","text":"<pre><code>abs(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the absolute value of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The absolute value of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import abs\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[-4, -3], [-2, -1], [0, 1], [2, 3], [4, 5]]),\n...     \"b\": torch.tensor([2, 1, 0, -1, -2]),\n... }\n&gt;&gt;&gt; out = abs(data)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3], [2, 1], [0, 1], [2, 3], [4, 5]]), 'b': tensor([2, 1, 0, 1, 2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.acos","title":"batchtensor.nested.acos","text":"<pre><code>acos(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse cosine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import acos\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = acos(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.acosh","title":"batchtensor.nested.acosh","text":"<pre><code>acosh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse hyperbolic cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse hyperbolic cosine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import acosh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = acosh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_batch","title":"batchtensor.nested.amax_along_batch","text":"<pre><code>amax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = amax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([8, 9]), 'b': tensor(4)}\n&gt;&gt;&gt; out = amax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_seq","title":"batchtensor.nested.amax_along_seq","text":"<pre><code>amax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = amax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 9]), 'b': tensor([4])}\n&gt;&gt;&gt; out = amax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [9]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_batch","title":"batchtensor.nested.amin_along_batch","text":"<pre><code>amin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = amin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 1]), 'b': tensor(0)}\n&gt;&gt;&gt; out = amin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_seq","title":"batchtensor.nested.amin_along_seq","text":"<pre><code>amin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = amin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 5]), 'b': tensor([0])}\n&gt;&gt;&gt; out = amin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [5]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmax_along_batch","title":"batchtensor.nested.argmax_along_batch","text":"<pre><code>argmax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argmax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 4]), 'b': tensor(0)}\n&gt;&gt;&gt; out = argmax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 4]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmax_along_seq","title":"batchtensor.nested.argmax_along_seq","text":"<pre><code>argmax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = argmax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 4]), 'b': tensor([0])}\n&gt;&gt;&gt; out = argmax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [4]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmin_along_batch","title":"batchtensor.nested.argmin_along_batch","text":"<pre><code>argmin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmin_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argmin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 0]), 'b': tensor(4)}\n&gt;&gt;&gt; out = argmin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 0]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmin_along_seq","title":"batchtensor.nested.argmin_along_seq","text":"<pre><code>argmin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmin_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = argmin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 0]), 'b': tensor([4])}\n&gt;&gt;&gt; out = argmin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [0]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_batch","title":"batchtensor.nested.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the batch dimension</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argsort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]), 'b': tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = argsort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_seq","title":"batchtensor.nested.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort each tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = argsort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]), 'b': tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argsort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.as_tensor","title":"batchtensor.nested.as_tensor","text":"<pre><code>as_tensor(\n    data: Any,\n    dtype: dtype | None = None,\n    device: device | None = None,\n) -&gt; Any\n</code></pre> <p>Create a new nested data structure with <code>torch.Tensor</code>s.</p> <p>This function recursively converts all array-like data (lists, tuples, numpy arrays, scalars, or existing tensors) to PyTorch tensors within a nested structure. Unlike <code>torch.tensor()</code>, this function shares memory with the input data when possible.</p> Note <p>This function preserves the structure of the input data while converting all array-like values to tensors. The output structure mirrors the input structure.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a value compatible with <code>torch.as_tensor</code>, such as lists, tuples, numpy arrays, Python scalars, or existing tensors. Can be a nested structure of dictionaries, lists, tuples containing such values.</p> required <code>dtype</code> <code>dtype | None</code> <p>The desired data type of returned tensors. If <code>None</code>, it infers data type from the input data. Common dtypes include <code>torch.float32</code>, <code>torch.int64</code>, <code>torch.bool</code>, etc.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>The device of the constructed tensors (e.g., <code>torch.device('cuda')</code> or <code>'cpu'</code>). If <code>None</code> and data contains a tensor, the device of that tensor is used. If <code>None</code> and data is not a tensor, the result tensor is constructed on the CPU.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A nested data structure with <code>torch.Tensor</code>s. The output data has the same structure as the input, with all array-like values converted to tensors.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from batchtensor.nested import as_tensor\n&gt;&gt;&gt; # Convert mixed types in nested structure\n&gt;&gt;&gt; data = {\"a\": np.ones((2, 5), dtype=np.float32), \"b\": np.arange(5), \"c\": 42}\n&gt;&gt;&gt; out = as_tensor(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]),\n 'b': tensor([0, 1, 2, 3, 4]),\n 'c': tensor(42)}\n&gt;&gt;&gt; # Specify dtype for all tensors\n&gt;&gt;&gt; out = as_tensor({\"values\": [1, 2, 3]}, dtype=torch.float32)\n&gt;&gt;&gt; out\n{'values': tensor([1., 2., 3.])}\n</code></pre> See Also <p><code>batchtensor.nested.from_numpy</code>: Convert numpy arrays to tensors (shares memory). <code>batchtensor.nested.to</code>: Convert tensor dtype/device for existing tensors.</p>"},{"location":"refs/nested/#batchtensor.nested.asin","title":"batchtensor.nested.asin","text":"<pre><code>asin(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the arcsine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The arcsine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import asin\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = asin(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.asinh","title":"batchtensor.nested.asinh","text":"<pre><code>asinh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse hyperbolic sine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse hyperbolic sine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import asinh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = asinh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.atan","title":"batchtensor.nested.atan","text":"<pre><code>atan(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the arctangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The arctangent of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import atan\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = atan(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.atanh","title":"batchtensor.nested.atanh","text":"<pre><code>atanh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse hyperbolic tangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse hyperbolic tangent of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import atanh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = atanh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_batch","title":"batchtensor.nested.cat_along_batch","text":"<pre><code>cat_along_batch(\n    data: Sequence[dict[Hashable, Tensor]],\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_batch\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7, 8, 9]]), \"b\": torch.tensor([[17, 18, 19]])},\n... ]\n&gt;&gt;&gt; out = cat_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [4, 5, 6], [7, 8, 9]]),\n 'b': tensor([[10, 11, 12], [13, 14, 15], [17, 18, 19]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_seq","title":"batchtensor.nested.cat_along_seq","text":"<pre><code>cat_along_seq(\n    data: Sequence[dict[Hashable, Tensor]],\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_seq\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7], [8]]), \"b\": torch.tensor([[17], [18]])},\n... ]\n&gt;&gt;&gt; out = cat_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 7], [4, 5, 6, 8]]),\n 'b': tensor([[10, 11, 12, 17], [13, 14, 15, 18]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_batch","title":"batchtensor.nested.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; outputs = chunk_along_batch(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_seq","title":"batchtensor.nested.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; outputs = chunk_along_seq(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.clamp","title":"batchtensor.nested.clamp","text":"<pre><code>clamp(\n    data: Any,\n    min: float | None = None,\n    max: float | None = None,\n) -&gt; Any\n</code></pre> <p>Clamp all elements in input into the range <code>[min, max]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>min</code> <code>float | None</code> <p>The lower-bound of the range to be clamped to.</p> <code>None</code> <code>max</code> <code>float | None</code> <p>The upper-bound of the range to be clamped to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The clamp value of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import clamp\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = clamp(data, min=1, max=5)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 2], [3, 4], [5, 5], [5, 5], [5, 5]]), 'b': tensor([5, 4, 3, 2, 1])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cos","title":"batchtensor.nested.cos","text":"<pre><code>cos(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cosine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cos\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = cos(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cosh","title":"batchtensor.nested.cosh","text":"<pre><code>cosh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the hyperbolic cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The hyperbolic cosine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cosh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = cosh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumprod_along_batch","title":"batchtensor.nested.cumprod_along_batch","text":"<pre><code>cumprod_along_batch(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative product of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative product of elements of input in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumprod_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = cumprod_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[   1,    2], [   3,    8], [  15,   48], [ 105,  384], [ 945, 3840]]), 'b': tensor([ 4, 12, 24, 24,  0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumprod_along_seq","title":"batchtensor.nested.cumprod_along_seq","text":"<pre><code>cumprod_along_seq(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative product of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative product of elements of input in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumprod_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = cumprod_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[    1,     2,     6,    24,   120], [    6,    42,   336,  3024, 30240]]), 'b': tensor([[ 4, 12, 24, 24,  0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumsum_along_batch","title":"batchtensor.nested.cumsum_along_batch","text":"<pre><code>cumsum_along_batch(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative sum of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative sum of elements of input in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = cumsum_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[ 0,  1], [ 2,  4], [ 6,  9], [12, 16], [20, 25]]), 'b': tensor([ 4,  7,  9, 10, 10])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumsum_along_seq","title":"batchtensor.nested.cumsum_along_seq","text":"<pre><code>cumsum_along_seq(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative sum of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative sum of elements of input in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = cumsum_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[ 0,  1,  3,  6, 10], [ 5, 11, 18, 26, 35]]), 'b': tensor([[ 4,  7,  9, 10, 10]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.exp","title":"batchtensor.nested.exp","text":"<pre><code>exp(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the exponential of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The exponential of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import exp\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = exp(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.exp2","title":"batchtensor.nested.exp2","text":"<pre><code>exp2(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the base two exponential of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The base two exponential of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import exp2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = exp2(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.expm1","title":"batchtensor.nested.expm1","text":"<pre><code>expm1(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the exponential of the elements minus 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The exponential of the elements minus 1. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import expm1\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = expm1(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.from_numpy","title":"batchtensor.nested.from_numpy","text":"<pre><code>from_numpy(data: Any) -&gt; Any\n</code></pre> <p>Create a new nested data structure where the <code>numpy.ndarray</code>s are converted to <code>torch.Tensor</code>s.</p> <p>This function recursively converts all numpy arrays in a nested structure to PyTorch tensors. The conversion uses <code>torch.from_numpy()</code>, which creates tensors that share memory with the original arrays.</p> Note <p>The returned <code>torch.Tensor</code>s and <code>numpy.ndarray</code>s share the same underlying memory. Modifications to the <code>torch.Tensor</code>s will be reflected in the <code>numpy.ndarray</code>s and vice versa. This is efficient but requires caution when modifying data.</p> Warning <p>Since memory is shared, modifying the tensor will modify the original numpy array. If you need independent copies, use <code>torch.tensor()</code> or call <code>.clone()</code> on the result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item should be a <code>numpy.ndarray</code>. Can be a nested structure of dictionaries, lists, tuples containing numpy arrays.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A nested data structure with <code>torch.Tensor</code>s instead of <code>numpy.ndarray</code>s. The output data has the same structure as the input, and the tensors share memory with the original arrays.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from batchtensor.nested import from_numpy\n&gt;&gt;&gt; data = {\"a\": np.ones((2, 5), dtype=np.float32), \"b\": np.arange(5)}\n&gt;&gt;&gt; out = from_numpy(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]), 'b': tensor([0, 1, 2, 3, 4])}\n&gt;&gt;&gt; # Demonstrate memory sharing\n&gt;&gt;&gt; data[\"a\"][0, 0] = 999\n&gt;&gt;&gt; out[\"a\"][0, 0]  # Reflects the change\ntensor(999.)\n</code></pre> See Also <p><code>batchtensor.nested.to_numpy</code>: Convert tensors to numpy arrays (shares memory). <code>batchtensor.nested.as_tensor</code>: More flexible conversion supporting various input types.</p>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_batch","title":"batchtensor.nested.index_select_along_batch","text":"<pre><code>index_select_along_batch(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_batch\n&gt;&gt;&gt; tensors = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [8, 9]]), 'b': tensor([2, 0])}\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9], [6, 7], [4, 5], [2, 3], [0, 1]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_seq","title":"batchtensor.nested.index_select_along_seq","text":"<pre><code>index_select_along_seq(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_seq\n&gt;&gt;&gt; tensors = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 4], [7, 9]]), 'b': tensor([[2, 0]])}\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3, 2, 1, 0], [9, 8, 7, 6, 5]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log","title":"batchtensor.nested.log","text":"<pre><code>log(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the natural logarithm of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The natural logarithm of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log10","title":"batchtensor.nested.log10","text":"<pre><code>log10(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the logarithm to the base 10 of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The logarithm to the base 10 of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log10\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log10(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log1p","title":"batchtensor.nested.log1p","text":"<pre><code>log1p(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the natural logarithm of <code>(1 + input)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The natural logarithm of <code>(1 + input)</code>. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log1p\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log1p(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log2","title":"batchtensor.nested.log2","text":"<pre><code>log2(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the logarithm to the base 2 of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The logarithm to the base 2 of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log2(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.max_along_batch","title":"batchtensor.nested.max_along_batch","text":"<pre><code>max_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the maximum values and  the second tensor, which must have dtype long, with their  indices in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import max_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = max_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4])),\n'b': torch.return_types.max(\nvalues=tensor(4),\nindices=tensor(0))}\n&gt;&gt;&gt; out = max_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]])),\n'b': torch.return_types.max(\nvalues=tensor([4]),\nindices=tensor([0]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.max_along_seq","title":"batchtensor.nested.max_along_seq","text":"<pre><code>max_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import max_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = max_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4])),\n'b': torch.return_types.max(\nvalues=tensor([4]),\nindices=tensor([0]))}\n&gt;&gt;&gt; out = max_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]])),\n'b': torch.return_types.max(\nvalues=tensor([[4]]),\nindices=tensor([[0]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.mean_along_batch","title":"batchtensor.nested.mean_along_batch","text":"<pre><code>mean_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The mean of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import mean_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0], dtype=torch.float),\n... }\n&gt;&gt;&gt; out = mean_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([4., 5.]), 'b': tensor(2.)}\n&gt;&gt;&gt; out = mean_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4., 5.]]), 'b': tensor([2.])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.mean_along_seq","title":"batchtensor.nested.mean_along_seq","text":"<pre><code>mean_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The mean of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import mean_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]], dtype=torch.float),\n... }\n&gt;&gt;&gt; out = mean_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([2., 7.]), 'b': tensor([2.])}\n&gt;&gt;&gt; out = mean_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[2.], [7.]]), 'b': tensor([[2.]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.median_along_batch","title":"batchtensor.nested.median_along_batch","text":"<pre><code>median_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import median_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = median_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2])),\n'b': torch.return_types.median(\nvalues=tensor(2),\nindices=tensor(2))}\n&gt;&gt;&gt; out = median_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]])),\n'b': torch.return_types.median(\nvalues=tensor([2]),\nindices=tensor([2]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.median_along_seq","title":"batchtensor.nested.median_along_seq","text":"<pre><code>median_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import median_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = median_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2])),\n'b': torch.return_types.median(\nvalues=tensor([2]),\nindices=tensor([2]))}\n&gt;&gt;&gt; out = median_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]])),\n'b': torch.return_types.median(\nvalues=tensor([[2]]),\nindices=tensor([[2]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.min_along_batch","title":"batchtensor.nested.min_along_batch","text":"<pre><code>min_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import min_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = min_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0])),\n'b': torch.return_types.min(\nvalues=tensor(0),\nindices=tensor(4))}\n&gt;&gt;&gt; out = min_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]])),\n'b': torch.return_types.min(\nvalues=tensor([0]),\nindices=tensor([4]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.min_along_seq","title":"batchtensor.nested.min_along_seq","text":"<pre><code>min_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import min_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = min_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0])),\n'b': torch.return_types.min(\nvalues=tensor([0]),\nindices=tensor([4]))}\n&gt;&gt;&gt; out = min_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]])),\n'b': torch.return_types.min(\nvalues=tensor([[0]]),\nindices=tensor([[4]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_batch","title":"batchtensor.nested.permute_along_batch","text":"<pre><code>permute_along_batch(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the batch dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = permute_along_batch(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [2, 3], [6, 7], [0, 1], [8, 9]]), 'b': tensor([2, 3, 1, 4, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_seq","title":"batchtensor.nested.permute_along_seq","text":"<pre><code>permute_along_seq(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = permute_along_seq(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 3, 0, 4], [7, 6, 8, 5, 9]]), 'b': tensor([[2, 3, 1, 4, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.prod_along_batch","title":"batchtensor.nested.prod_along_batch","text":"<pre><code>prod_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The product of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import prod_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = prod_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([  0, 945]), 'b': tensor(120)}\n&gt;&gt;&gt; out = prod_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[  0, 945]]), 'b': tensor([120])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.prod_along_seq","title":"batchtensor.nested.prod_along_seq","text":"<pre><code>prod_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The product of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import prod_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[5, 4, 3, 2, 1]]),\n... }\n&gt;&gt;&gt; out = prod_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([    0, 15120]), 'b': tensor([120])}\n&gt;&gt;&gt; out = prod_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[    0], [15120]]), 'b': tensor([[120]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.repeat_along_seq","title":"batchtensor.nested.repeat_along_seq","text":"<pre><code>repeat_along_seq(data: Any, repeats: int) -&gt; Any\n</code></pre> <p>Repeat all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>repeats</code> <code>int</code> <p>The number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The tensors repeated along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import repeat_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = repeat_along_seq(data, 2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4], [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]]),\n 'b': tensor([[4, 3, 2, 1, 0, 4, 3, 2, 1, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_batch","title":"batchtensor.nested.select_along_batch","text":"<pre><code>select_along_batch(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = select_along_batch(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([4, 5]), 'b': tensor(2)}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_seq","title":"batchtensor.nested.select_along_seq","text":"<pre><code>select_along_seq(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = select_along_seq(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([2, 7]), 'b': tensor([2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_batch","title":"batchtensor.nested.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    data: Any, generator: Generator | None = None\n) -&gt; Any\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data with shuffled tensors along the batch dimension. The output data has the same structure as the input data.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = shuffle_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_seq","title":"batchtensor.nested.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    data: Any, generator: Generator | None = None\n) -&gt; Any\n</code></pre> <p>Shuffle all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = shuffle_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([[...]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sin","title":"batchtensor.nested.sin","text":"<pre><code>sin(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the sine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sin\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = sin(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sinh","title":"batchtensor.nested.sinh","text":"<pre><code>sinh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the hyperbolic sine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The hyperbolic sine of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sinh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = sinh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_batch","title":"batchtensor.nested.slice_along_batch","text":"<pre><code>slice_along_batch(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = slice_along_batch(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [6, 7], [8, 9]]), 'b': tensor([2, 1, 0])}\n&gt;&gt;&gt; out = slice_along_batch(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [2, 3], [4, 5]]), 'b': tensor([4, 3, 2])}\n&gt;&gt;&gt; out = slice_along_batch(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [4, 5], [8, 9]]), 'b': tensor([4, 2, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_seq","title":"batchtensor.nested.slice_along_seq","text":"<pre><code>slice_along_seq(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = slice_along_seq(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 3, 4], [7, 8, 9]]), 'b': tensor([[2, 1, 0]])}\n&gt;&gt;&gt; out = slice_along_seq(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [5, 6, 7]]), 'b': tensor([[4, 3, 2]])}\n&gt;&gt;&gt; out = slice_along_seq(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 2, 4], [5, 7, 9]]), 'b': tensor([[4, 2, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sort_along_batch","title":"batchtensor.nested.sort_along_batch","text":"<pre><code>sort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Sort the elements of the input tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A similar object where each tensor is replaced by a namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = sort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]]),\nindices=tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])),\n'b': torch.return_types.sort(\nvalues=tensor([0, 1, 2, 3, 4]),\nindices=tensor([4, 3, 2, 1, 0]))}\n&gt;&gt;&gt; out = sort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[8, 9], [5, 7], [4, 6], [2, 3], [0, 1]]),\nindices=tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])),\n'b': torch.return_types.sort(\nvalues=tensor([4, 3, 2, 1, 0]),\nindices=tensor([0, 1, 2, 3, 4]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sort_along_seq","title":"batchtensor.nested.sort_along_seq","text":"<pre><code>sort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Sort the elements of the input tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A similar object where each tensor is replaced by a namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = sort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[0, 3, 5, 7, 8], [1, 2, 4, 6, 9]]),\nindices=tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]])),\n'b': torch.return_types.sort(\nvalues=tensor([[0, 1, 2, 3, 4]]),\nindices=tensor([[4, 3, 2, 1, 0]]))}\n&gt;&gt;&gt; out = sort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[8, 7, 5, 3, 0], [9, 6, 4, 2, 1]]),\nindices=tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]])),\n'b': torch.return_types.sort(\nvalues=tensor([[4, 3, 2, 1, 0]]),\nindices=tensor([[0, 1, 2, 3, 4]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_batch","title":"batchtensor.nested.split_along_batch","text":"<pre><code>split_along_batch(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; outputs = split_along_batch(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_seq","title":"batchtensor.nested.split_along_seq","text":"<pre><code>split_along_seq(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; outputs = split_along_seq(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sum_along_batch","title":"batchtensor.nested.sum_along_batch","text":"<pre><code>sum_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sum of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sum_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = sum_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([20, 25]), 'b': tensor(10)}\n&gt;&gt;&gt; out = sum_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[20, 25]]), 'b': tensor([10])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sum_along_seq","title":"batchtensor.nested.sum_along_seq","text":"<pre><code>sum_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sum of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sum_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = sum_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([10, 35]), 'b': tensor([10])}\n&gt;&gt;&gt; out = sum_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[10], [35]]), 'b': tensor([[10]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.tan","title":"batchtensor.nested.tan","text":"<pre><code>tan(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the tangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The tangent of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import tan\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = tan(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.tanh","title":"batchtensor.nested.tanh","text":"<pre><code>tanh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the hyperbolic tangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The hyperbolic tangent of the elements. The output has the same structure as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import tanh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = tanh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.to","title":"batchtensor.nested.to","text":"<pre><code>to(data: Any, *args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Perform Tensor dtype and/or device conversion on all tensors in nested data.</p> <p>This function recursively applies <code>torch.Tensor.to()</code> to all tensors in the nested data structure, allowing you to convert dtypes, move to different devices, or change other tensor properties for all tensors at once.</p> Note <p>This function preserves the structure of the input data while converting all tensors within it.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input nested data structure. Can be a dictionary, list, tuple, or any combination of these containing tensors. All leaf values in the structure must be tensors.</p> required <code>args</code> <code>Any</code> <p>Positional arguments passed to <code>torch.Tensor.to</code>. Common usage includes passing a device (e.g., <code>torch.device('cuda')</code>), dtype (e.g., <code>torch.float32</code>), or another tensor to match device and dtype.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments passed to <code>torch.Tensor.to</code>. Supports arguments like <code>dtype</code>, <code>device</code>, <code>non_blocking</code>, <code>copy</code>, and <code>memory_format</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data after conversion. The structure is preserved, with all tensors converted according to the specified arguments.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; # Convert to float dtype\n&gt;&gt;&gt; out = to(data, dtype=torch.float)\n&gt;&gt;&gt; out\n{'a': tensor([[0., 1.], [2., 3.], [4., 5.], [6., 7.], [8., 9.]]),\n 'b': tensor([4., 3., 2., 1., 0.])}\n&gt;&gt;&gt; # Move to GPU (if available) with float32 dtype\n&gt;&gt;&gt; # out = to(data, device='cuda', dtype=torch.float32)\n</code></pre> See Also <p><code>batchtensor.nested.as_tensor</code>: Convert data to tensor format. <code>batchtensor.nested.from_numpy</code>: Convert numpy arrays to tensors.</p>"},{"location":"refs/nested/#batchtensor.nested.to_numpy","title":"batchtensor.nested.to_numpy","text":"<pre><code>to_numpy(data: Any) -&gt; Any\n</code></pre> <p>Create a new nested data structure where the <code>torch.Tensor</code>s are converted to <code>numpy.ndarray</code>s.</p> <p>This function recursively converts all PyTorch tensors in a nested structure to numpy arrays. The conversion uses <code>tensor.numpy()</code>, which creates arrays that share memory with the original tensors when possible (for tensors on CPU with compatible dtypes).</p> Note <p>The returned <code>torch.Tensor</code>s and <code>numpy.ndarray</code>s share the same underlying memory when tensors are on CPU with compatible dtypes. Modifications to the <code>torch.Tensor</code>s will be reflected in the <code>numpy.ndarray</code>s and vice versa.</p> Warning <ul> <li>Tensors on CUDA devices will be copied to CPU before conversion.</li> <li>Tensors with gradients enabled will raise an error unless you call   <code>.detach()</code> first.</li> <li>Since memory is shared (for CPU tensors), modifying the array will   modify the original tensor.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a <code>torch.Tensor</code>. Can be a nested structure of dictionaries, lists, tuples containing tensors. All tensors should be on CPU without gradients for efficient conversion.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A nested data structure with <code>numpy.ndarray</code>s instead of <code>torch.Tensor</code>s. The output data has the same structure as the input. For CPU tensors with compatible dtypes, the arrays share memory with the original tensors.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to_numpy\n&gt;&gt;&gt; data = {\"a\": torch.ones(2, 5), \"b\": torch.tensor([0, 1, 2, 3, 4])}\n&gt;&gt;&gt; out = to_numpy(data)\n&gt;&gt;&gt; out\n{'a': array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]], dtype=float32), 'b': array([0, 1, 2, 3, 4])}\n&gt;&gt;&gt; # Demonstrate memory sharing for CPU tensors\n&gt;&gt;&gt; data[\"a\"][0, 0] = 999\n&gt;&gt;&gt; out[\"a\"][0, 0]  # Reflects the change\n999.0\n</code></pre> See Also <p><code>batchtensor.nested.from_numpy</code>: Convert numpy arrays to tensors (shares memory). <code>batchtensor.nested.as_tensor</code>: Convert various data types to tensors.</p>"},{"location":"refs/tensor/","title":"Tensor Module API Reference","text":"<p>This page provides the complete API reference for the <code>batchtensor.tensor</code> module.</p> <p>The tensor module contains functions for manipulating individual PyTorch tensors along batch and sequence dimensions. These functions operate on single tensors and are the foundation for the nested operations.</p> <p>For usage examples and tutorials, see the Tensor Operations User Guide.</p>"},{"location":"refs/tensor/#batchtensor.tensor","title":"batchtensor.tensor","text":"<p>Functions for manipulating individual PyTorch tensors with batch and sequence dimensions.</p> <p>This module provides a collection of functions for working with PyTorch tensors where the first dimension (index 0) is the batch dimension, and optionally the second dimension (index 1) is the sequence dimension.</p> All functions in this module follow these conventions <ul> <li>Functions ending with <code>_along_batch</code> operate on the batch dimension (dim=0)</li> <li>Functions ending with <code>_along_seq</code> operate on the sequence dimension (dim=1)</li> <li>The batch dimension always represents independent samples</li> <li>The sequence dimension represents sequential/temporal data within each sample</li> </ul> Function Categories <p>Reduction operations: Aggregate values along a dimension     - <code>sum_along_batch</code>, <code>mean_along_batch</code>, <code>max_along_batch</code>, etc.     - <code>sum_along_seq</code>, <code>mean_along_seq</code>, <code>max_along_seq</code>, etc.</p> <p>Slicing operations: Extract subsets of data     - <code>slice_along_batch</code>, <code>select_along_batch</code>, <code>chunk_along_batch</code>     - <code>slice_along_seq</code>, <code>select_along_seq</code>, <code>chunk_along_seq</code></p> <p>Joining operations: Combine multiple tensors     - <code>cat_along_batch</code>, <code>cat_along_seq</code>, <code>repeat_along_seq</code></p> <p>Permutation operations: Reorder elements     - <code>permute_along_batch</code>, <code>shuffle_along_batch</code>     - <code>permute_along_seq</code>, <code>shuffle_along_seq</code></p> <p>Indexing operations: Select specific elements     - <code>index_select_along_batch</code>, <code>index_select_along_seq</code></p> <p>Comparison operations: Sort and find extrema     - <code>sort_along_batch</code>, <code>argsort_along_batch</code>     - <code>sort_along_seq</code>, <code>argsort_along_seq</code></p> <p>Mathematical operations: Cumulative operations     - <code>cumsum_along_batch</code>, <code>cumprod_along_batch</code>     - <code>cumsum_along_seq</code>, <code>cumprod_along_seq</code></p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor import tensor as bt\n&gt;&gt;&gt; # Create a batch of 5 samples, each with 2 features\n&gt;&gt;&gt; data = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n&gt;&gt;&gt; # Compute mean along batch dimension (result has shape matching feature dim)\n&gt;&gt;&gt; bt.mean_along_batch(data)\ntensor([5., 6.])\n&gt;&gt;&gt; # Slice first 3 samples from the batch\n&gt;&gt;&gt; bt.slice_along_batch(data, stop=3)\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n</code></pre> See Also <p><code>batchtensor.nested</code>: Similar functions for nested data structures containing tensors.</p>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_batch","title":"batchtensor.tensor.amax_along_batch","text":"<pre><code>amax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = amax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([8, 9])\n&gt;&gt;&gt; out = amax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_seq","title":"batchtensor.tensor.amax_along_seq","text":"<pre><code>amax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = amax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 9])\n&gt;&gt;&gt; out = amax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_batch","title":"batchtensor.tensor.amin_along_batch","text":"<pre><code>amin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = amin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 1])\n&gt;&gt;&gt; out = amin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_seq","title":"batchtensor.tensor.amin_along_seq","text":"<pre><code>amin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = amin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 5])\n&gt;&gt;&gt; out = amin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_batch","title":"batchtensor.tensor.argmax_along_batch","text":"<pre><code>argmax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = argmax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4, 4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_seq","title":"batchtensor.tensor.argmax_along_seq","text":"<pre><code>argmax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = argmax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_batch","title":"batchtensor.tensor.argmin_along_batch","text":"<pre><code>argmin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = argmin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_seq","title":"batchtensor.tensor.argmin_along_seq","text":"<pre><code>argmin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = argmin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_batch","title":"batchtensor.tensor.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> <p>This function returns the indices needed to sort the tensor along the batch dimension. The actual sorted values can be obtained by using <code>sort_along_batch</code> or by indexing with the returned indices.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to sort.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order. If <code>False</code> (default), sorts in ascending order. If <code>True</code>, sorts in descending order.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>torch.argsort</code>. Supports arguments like <code>stable</code> for stable sorting.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of indices that would sort the input tensor along the batch dimension. Has the same shape as the input tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; # Sort in ascending order\n&gt;&gt;&gt; out = argsort_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])\n&gt;&gt;&gt; # Sort in descending order\n&gt;&gt;&gt; out = argsort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])\n</code></pre> See Also <p><code>sort_along_batch</code>: Returns both sorted values and indices. <code>argsort_along_seq</code>: Sort indices along sequence dimension. <code>torch.argsort</code>: PyTorch's general argsort function.</p>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_seq","title":"batchtensor.tensor.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[2, 1, 4, 0, 3],\n        [0, 4, 3, 2, 1]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 0, 4, 1, 2],\n        [1, 2, 3, 4, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_batch","title":"batchtensor.tensor.cat_along_batch","text":"<pre><code>cat_along_batch(\n    tensors: list[Tensor] | tuple[Tensor, ...],\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>This function concatenates multiple tensors along their batch dimension, stacking batch items from all input tensors into a single tensor. The batch dimension sizes can differ, but all other dimensions must match.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>A sequence (list or tuple) of tensors to concatenate. All tensors must have the same number of dimensions and the same shape except in the batch dimension. At least one tensor must be provided.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensor along the batch dimension. If the input tensors have batch sizes <code>[b1, b2, ..., bn]</code>, the output will have batch size <code>b1 + b2 + ... + bn</code>.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_batch\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11, 12], [13, 14, 15]]),\n... ]\n&gt;&gt;&gt; out = cat_along_batch(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2],\n        [ 4,  5,  6],\n        [10, 11, 12],\n        [13, 14, 15]])\n&gt;&gt;&gt; # Concatenating tensors with different batch sizes\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[1, 2]]),  # batch size 1\n...     torch.tensor([[3, 4], [5, 6], [7, 8]]),  # batch size 3\n... ]\n&gt;&gt;&gt; out = cat_along_batch(tensors)\n&gt;&gt;&gt; out\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n</code></pre> See Also <p><code>cat_along_seq</code>: Concatenate along the sequence dimension instead. <code>split_along_batch</code>: Inverse operation - split a tensor into chunks. <code>torch.cat</code>: PyTorch's general concatenation function.</p>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_seq","title":"batchtensor.tensor.cat_along_seq","text":"<pre><code>cat_along_seq(\n    tensors: list[Tensor] | tuple[Tensor, ...],\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>The tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_seq\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11], [12, 13]]),\n... ]\n&gt;&gt;&gt; out = cat_along_seq(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2, 10, 11],\n        [ 4,  5,  6, 12, 13]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_batch","title":"batchtensor.tensor.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor. This operation attempts to split the tensor into the specified number of chunks of approximately equal size. If the tensor size is not evenly divisible by the number of chunks, the last chunk will be smaller.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split. Must have at least one dimension.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return. Must be a positive integer. If <code>chunks</code> is greater than the batch size, some chunks may be empty.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>A tuple of tensor chunks along the batch dimension. Each chunk is a view of the original tensor (shares memory).</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; outputs = chunk_along_batch(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre> See Also <p><code>split_along_batch</code>: Split with specified sizes for each chunk. <code>slice_along_batch</code>: Extract a slice from the batch dimension. <code>chunk_along_seq</code>: Split along the sequence dimension instead.</p>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_seq","title":"batchtensor.tensor.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; outputs = chunk_along_seq(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumprod_along_batch","title":"batchtensor.tensor.cumprod_along_batch","text":"<pre><code>cumprod_along_batch(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative product of elements of input in the batch dimension.</p> <p>This function computes the cumulative product along the batch dimension, where each element in the output is the product of all elements up to that position in the batch. This is useful for computing running products or compound growth factors over batch items.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> Warning <p>Cumulative products can quickly overflow for large tensors or grow very large. Consider using log-space computations if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor. Must have at least one dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the cumulative product of elements along the batch dimension. Has the same shape and dtype as the input tensor. Element <code>output[i]</code> is the product of <code>input[0]</code> through <code>input[i]</code> along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumprod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n&gt;&gt;&gt; out = cumprod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[   1,    2], [   3,    8], [  15,   48], [ 105,  384], [ 945, 3840]])\n&gt;&gt;&gt; # Each row is the product of all previous rows\n&gt;&gt;&gt; # Row 0: [1, 2]\n&gt;&gt;&gt; # Row 1: [1, 2] * [3, 4] = [3, 8]\n&gt;&gt;&gt; # Row 2: [3, 8] * [5, 6] = [15, 48]\n&gt;&gt;&gt; # etc.\n</code></pre> See Also <p><code>cumsum_along_batch</code>: Cumulative sum instead of product. <code>cumprod_along_seq</code>: Cumulative product along sequence dimension. <code>prod_along_batch</code>: Total product (single value per feature). <code>torch.cumprod</code>: PyTorch's general cumulative product function.</p>"},{"location":"refs/tensor/#batchtensor.tensor.cumprod_along_seq","title":"batchtensor.tensor.cumprod_along_seq","text":"<pre><code>cumprod_along_seq(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative product of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative product of elements of input in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumprod_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; out = cumprod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[    1,     2,     6,    24,   120],\n        [    6,    42,   336,  3024, 30240]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumsum_along_batch","title":"batchtensor.tensor.cumsum_along_batch","text":"<pre><code>cumsum_along_batch(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative sum of elements of input in the batch dimension.</p> <p>This function computes the cumulative sum along the batch dimension, where each element in the output is the sum of all elements up to that position in the batch. This is useful for computing running totals or prefix sums over batch items.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor. Must have at least one dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the cumulative sum of elements along the batch dimension. Has the same shape and dtype as the input tensor. Element <code>output[i]</code> is the sum of <code>input[0]</code> through <code>input[i]</code> along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = cumsum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[ 0,  1], [ 2,  4], [ 6,  9], [12, 16], [20, 25]])\n&gt;&gt;&gt; # Each row is the sum of all previous rows\n&gt;&gt;&gt; # Row 0: [0, 1]\n&gt;&gt;&gt; # Row 1: [0, 1] + [2, 3] = [2, 4]\n&gt;&gt;&gt; # Row 2: [2, 4] + [4, 5] = [6, 9]\n&gt;&gt;&gt; # etc.\n</code></pre> See Also <p><code>cumprod_along_batch</code>: Cumulative product instead of sum. <code>cumsum_along_seq</code>: Cumulative sum along sequence dimension. <code>sum_along_batch</code>: Total sum (single value per feature). <code>torch.cumsum</code>: PyTorch's general cumulative sum function.</p>"},{"location":"refs/tensor/#batchtensor.tensor.cumsum_along_seq","title":"batchtensor.tensor.cumsum_along_seq","text":"<pre><code>cumsum_along_seq(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative sum of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative sum of elements of input in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = cumsum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  3,  6, 10],\n        [ 5, 11, 18, 26, 35]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_batch","title":"batchtensor.tensor.index_select_along_batch","text":"<pre><code>index_select_along_batch(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> <p>This function selects specific batch items based on the provided indices. Unlike <code>select_along_batch</code> which selects a single item and reduces dimensionality, this function maintains the batch dimension and can select multiple items, duplicate items, or reorder items.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor. Must have at least one dimension.</p> required <code>index</code> <code>Tensor</code> <p>A 1-D tensor containing the indices to select. Must be a <code>LongTensor</code>. Can contain duplicate indices to repeat batch items, or be shorter/longer than the batch dimension to select a subset or create a larger output.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the batch dimension. The output has shape <code>(index.size(0), *tensor.shape[1:])</code> where the batch dimension size matches the length of the index tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; # Select specific batch items\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [8, 9]])\n&gt;&gt;&gt; # Reverse order\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[8, 9],\n        [6, 7],\n        [4, 5],\n        [2, 3],\n        [0, 1]])\n&gt;&gt;&gt; # Duplicate batch items\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([0, 0, 1]))\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [0, 1],\n        [2, 3]])\n</code></pre> See Also <p><code>select_along_batch</code>: Select a single batch item (reduces dimension). <code>permute_along_batch</code>: Reorder all batch items with a permutation. <code>slice_along_batch</code>: Select a contiguous range of batch items. <code>index_select_along_seq</code>: Index select along sequence dimension.</p>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_seq","title":"batchtensor.tensor.index_select_along_seq","text":"<pre><code>index_select_along_seq(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 4],\n        [7, 9]])\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[4, 3, 2, 1, 0],\n        [9, 8, 7, 6, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_batch","title":"batchtensor.tensor.max_along_batch","text":"<pre><code>max_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = max_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_seq","title":"batchtensor.tensor.max_along_seq","text":"<pre><code>max_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = max_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_batch","title":"batchtensor.tensor.mean_along_batch","text":"<pre><code>mean_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]])\n&gt;&gt;&gt; out = mean_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4., 5.])\n&gt;&gt;&gt; out = mean_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4., 5.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_seq","title":"batchtensor.tensor.mean_along_seq","text":"<pre><code>mean_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]])\n&gt;&gt;&gt; out = mean_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([2., 7.])\n&gt;&gt;&gt; out = mean_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[2.], [7.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_batch","title":"batchtensor.tensor.median_along_batch","text":"<pre><code>median_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = median_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_seq","title":"batchtensor.tensor.median_along_seq","text":"<pre><code>median_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = median_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_batch","title":"batchtensor.tensor.min_along_batch","text":"<pre><code>min_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = min_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_seq","title":"batchtensor.tensor.min_along_seq","text":"<pre><code>min_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = min_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_batch","title":"batchtensor.tensor.permute_along_batch","text":"<pre><code>permute_along_batch(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the batch dimension.</p> <p>This function reorders the elements along the batch dimension according to a specified permutation. The permutation defines a mapping from new positions to original positions: <code>output[i] = input[permutation[i]]</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to permute. Must have at least one dimension.</p> required <code>permutation</code> <code>Tensor</code> <p>A 1-D tensor containing the indices of the permutation. Must be a <code>LongTensor</code> with shape <code>(batch_size,)</code>. Each index must be in the range <code>[0, batch_size-1]</code> and all indices should be unique (though duplicates are allowed if you want to repeat certain batch items).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the batch dimension. The shape is unchanged, but elements are reordered according to the permutation.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; # Reverse the batch order\n&gt;&gt;&gt; out = permute_along_batch(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[8, 9],\n        [6, 7],\n        [4, 5],\n        [2, 3],\n        [0, 1]])\n&gt;&gt;&gt; # Custom permutation\n&gt;&gt;&gt; out = permute_along_batch(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [2, 3],\n        [6, 7],\n        [0, 1],\n        [8, 9]])\n</code></pre> See Also <p><code>shuffle_along_batch</code>: Apply a random permutation. <code>permute_along_seq</code>: Permute the sequence dimension instead. <code>index_select_along_batch</code>: Select specific indices (can duplicate or omit items).</p>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_seq","title":"batchtensor.tensor.permute_along_seq","text":"<pre><code>permute_along_seq(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the sequence dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = permute_along_seq(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 1, 3, 0, 4],\n        [7, 6, 8, 5, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_batch","title":"batchtensor.tensor.prod_along_batch","text":"<pre><code>prod_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = prod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([  0, 945])\n&gt;&gt;&gt; out = prod_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[  0, 945]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_seq","title":"batchtensor.tensor.prod_along_seq","text":"<pre><code>prod_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = prod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([    0, 15120])\n&gt;&gt;&gt; out = prod_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[    0], [15120]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.repeat_along_seq","title":"batchtensor.tensor.repeat_along_seq","text":"<pre><code>repeat_along_seq(tensor: Tensor, repeats: int) -&gt; Tensor\n</code></pre> <p>Repeat the data along the sequence dimension.</p> <p>This function repeats the sequence data a specified number of times, effectively duplicating the sequence content. The resulting tensor has a sequence length that is <code>repeats</code> times the original sequence length.</p> Note <p>This function assumes the sequence dimension is the second     dimension (index 1).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor. Must have at least two dimensions (batch and sequence).</p> required <code>repeats</code> <code>int</code> <p>The number of times to repeat the data along the sequence dimension. Must be a positive integer. If <code>repeats=1</code>, returns a copy of the input.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A new tensor with the data repeated along the sequence dimension. If the input has shape <code>(batch_size, seq_len, ...)</code>, the output will have shape <code>(batch_size, seq_len * repeats, ...)</code>.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import repeat_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = repeat_along_seq(tensor, 2)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])\n&gt;&gt;&gt; # Repeat 3 times\n&gt;&gt;&gt; out = repeat_along_seq(tensor, 3)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])\n</code></pre> See Also <p><code>cat_along_seq</code>: Concatenate different tensors (not repeating the same). <code>torch.repeat</code>: PyTorch's general repeat function for all dimensions.</p>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_batch","title":"batchtensor.tensor.select_along_batch","text":"<pre><code>select_along_batch(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed. It selects a single element from the batch dimension, reducing the tensor's dimensionality by one.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor. Must have at least one dimension.</p> required <code>index</code> <code>int</code> <p>The index to select. Can be negative for indexing from the end (e.g., -1 for the last batch item). Must be in the range <code>[-batch_size, batch_size-1]</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor with the batch dimension removed. The shape is <code>tensor.shape[1:]</code>.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = select_along_batch(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([4, 5])\n&gt;&gt;&gt; # Negative indexing from the end\n&gt;&gt;&gt; out = select_along_batch(tensor, index=-1)\n&gt;&gt;&gt; out\ntensor([8, 9])\n</code></pre> See Also <p><code>slice_along_batch</code>: Extract a range of batch items (keeps batch dimension). <code>index_select_along_batch</code>: Select multiple indices (keeps batch dimension). <code>select_along_seq</code>: Select from the sequence dimension instead.</p>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_seq","title":"batchtensor.tensor.select_along_seq","text":"<pre><code>select_along_seq(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = select_along_seq(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([2, 7])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_batch","title":"batchtensor.tensor.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> <p>This function randomly reorders the elements along the batch dimension, creating a random permutation of the batch items. All elements within each batch item are kept together and maintain their relative positions.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to shuffle. Must have at least one dimension.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator for reproducible shuffling. If provided, the shuffling will be deterministic based on the generator's state. If <code>None</code>, uses PyTorch's default random state.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor. The shape is unchanged, but elements along the batch dimension are reordered randomly.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = shuffle_along_batch(tensor)\n&gt;&gt;&gt; out  # Order is random\ntensor([[...]])\n&gt;&gt;&gt; # For reproducible shuffling\n&gt;&gt;&gt; generator = torch.Generator().manual_seed(42)\n&gt;&gt;&gt; out = shuffle_along_batch(tensor, generator=generator)\n&gt;&gt;&gt; out\ntensor([[6, 7], [2, 3], [8, 9], [0, 1], [4, 5]])\n</code></pre> See Also <p><code>permute_along_batch</code>: Apply a specific permutation (not random). <code>shuffle_along_seq</code>: Shuffle the sequence dimension instead. <code>batchtensor.utils.manual_seed</code>: Set global random seed for reproducibility.</p>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_seq","title":"batchtensor.tensor.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = shuffle_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_batch","title":"batchtensor.tensor.slice_along_batch","text":"<pre><code>slice_along_batch(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [6, 7],\n        [8, 9]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [4, 5],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_seq","title":"batchtensor.tensor.slice_along_seq","text":"<pre><code>slice_along_seq(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [9, 8, 7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[2, 3, 4],\n        [7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2],\n        [9, 8, 7]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 2, 4],\n        [9, 7, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sort_along_batch","title":"batchtensor.tensor.sort_along_batch","text":"<pre><code>sort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; sort\n</code></pre> <p>Sort the elements of the input tensor along the batch dimension in ascending order by value.</p> <p>This function sorts the tensor along the batch dimension and returns both the sorted values and the corresponding indices. This is useful when you need both the sorted data and the permutation that created it.</p> Note <p>This function assumes the batch dimension is the first     dimension (index 0).</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to sort.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order. If <code>False</code> (default), sorts in ascending order. If <code>True</code>, sorts in descending order.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>torch.sort</code>. Supports arguments like <code>stable</code> for stable sorting.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sort</code> <p>A namedtuple of (values, indices), where: - <code>values</code>: The sorted tensor along the batch dimension. - <code>indices</code>: The indices that would produce the sorted values   from the original tensor.</p> <code>sort</code> <p>Both tensors have the same shape as the input.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; out = sort_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]]),\nindices=tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]))\n&gt;&gt;&gt; # Access sorted values and indices\n&gt;&gt;&gt; out.values\ntensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]])\n&gt;&gt;&gt; out.indices\ntensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])\n&gt;&gt;&gt; # Sort in descending order\n&gt;&gt;&gt; out = sort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[8, 9], [5, 7], [4, 6], [2, 3], [0, 1]]),\nindices=tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]))\n</code></pre> See Also <p><code>argsort_along_batch</code>: Returns only the sorting indices. <code>sort_along_seq</code>: Sort along the sequence dimension. <code>torch.sort</code>: PyTorch's general sort function.</p>"},{"location":"refs/tensor/#batchtensor.tensor.sort_along_seq","title":"batchtensor.tensor.sort_along_seq","text":"<pre><code>sort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; sort\n</code></pre> <p>Sort the elements of the input tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sort</code> <p>A namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = sort_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[0, 3, 5, 7, 8], [1, 2, 4, 6, 9]]),\nindices=tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]))\n&gt;&gt;&gt; out = sort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[8, 7, 5, 3, 0], [9, 6, 4, 2, 1]]),\nindices=tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_batch","title":"batchtensor.tensor.split_along_batch","text":"<pre><code>split_along_batch(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; outputs = split_along_batch(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_seq","title":"batchtensor.tensor.split_along_seq","text":"<pre><code>split_along_seq(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; outputs = split_along_seq(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_batch","title":"batchtensor.tensor.sum_along_batch","text":"<pre><code>sum_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the batch dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = sum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([20, 25])\n&gt;&gt;&gt; out = sum_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[20, 25]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_seq","title":"batchtensor.tensor.sum_along_seq","text":"<pre><code>sum_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the sequence dimension.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = sum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([10, 35])\n&gt;&gt;&gt; out = sum_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[10], [35]])\n</code></pre>"},{"location":"refs/utils/","title":"Utils API Reference","text":"<p>This page provides the complete API reference for the <code>batchtensor.utils</code> module.</p> <p>The utils module contains utility functions that support the main tensor operations, particularly for managing random seeds and ensuring reproducibility.</p> <p>For usage examples and tutorials, see the Utils User Guide.</p>"},{"location":"refs/utils/#seed-management","title":"Seed Management","text":""},{"location":"refs/utils/#batchtensor.utils.seed","title":"batchtensor.utils.seed","text":"<p>Implements utility functions to manage random seeds for reproducible tensor operations.</p>"},{"location":"refs/utils/#batchtensor.utils.seed.get_random_seed","title":"batchtensor.utils.seed.get_random_seed","text":"<pre><code>get_random_seed(seed: int) -&gt; int\n</code></pre> <p>Get a random seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>A random seed to make the process reproducible.</p> required <p>Returns:</p> Type Description <code>int</code> <p>A random seed. The value is between <code>-2 ** 63</code> and <code>2 ** 63 - 1</code>.</p> Example <pre><code>&gt;&gt;&gt; from batchtensor.utils.seed import get_random_seed\n&gt;&gt;&gt; get_random_seed(44)\n6176747449835261347\n</code></pre>"},{"location":"refs/utils/#batchtensor.utils.seed.get_torch_generator","title":"batchtensor.utils.seed.get_torch_generator","text":"<pre><code>get_torch_generator(\n    random_seed: int = 1,\n    device: device | str | None = \"cpu\",\n) -&gt; Generator\n</code></pre> <p>Create a <code>torch.Generator</code> initialized with a given seed.</p> <p>Parameters:</p> Name Type Description Default <code>random_seed</code> <code>int</code> <p>A random seed.</p> <code>1</code> <code>device</code> <code>device | str | None</code> <p>The desired device for the generator.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Generator</code> <p>A <code>torch.Generator</code> object.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils.seed import get_torch_generator\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; torch.rand(2, 4, generator=generator)\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936]])\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; torch.rand(2, 4, generator=generator)\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936]])\n</code></pre>"},{"location":"refs/utils/#batchtensor.utils.seed.setup_torch_generator","title":"batchtensor.utils.seed.setup_torch_generator","text":"<pre><code>setup_torch_generator(\n    generator_or_seed: int | Generator,\n) -&gt; Generator\n</code></pre> <p>Set up a <code>torch.Generator</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>generator_or_seed</code> <code>int | Generator</code> <p>A <code>torch.Generator</code> object or a random seed.</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>A <code>torch.Generator</code> object.</p> Example <pre><code>&gt;&gt;&gt; from batchtensor.utils.seed import setup_torch_generator\n&gt;&gt;&gt; generator = setup_torch_generator(42)\n&gt;&gt;&gt; generator\n&lt;torch._C.Generator object at 0x...&gt;\n</code></pre>"},{"location":"uguide/constants/","title":"Constants","text":"<p>The <code>batchtensor.constants</code> module defines important constants used throughout the library to identify batch and sequence dimensions.</p>"},{"location":"uguide/constants/#overview","title":"Overview","text":"<p>The constants module provides standardized dimension indices that ensure consistency across all batchtensor operations. These constants are used internally by the library and can also be used in your own code when working with batchtensor functions.</p>"},{"location":"uguide/constants/#available-constants","title":"Available Constants","text":""},{"location":"uguide/constants/#batch_dim","title":"BATCH_DIM","text":"<p>The batch dimension constant identifies the dimension used for batching in tensors.</p> <pre><code>&gt;&gt;&gt; from batchtensor.constants import BATCH_DIM\n&gt;&gt;&gt; BATCH_DIM\n0\n</code></pre> <p>Usage: This constant is set to <code>0</code>, indicating that the batch dimension is always the first dimension (dimension 0) of tensors when using batchtensor functions.</p> <p>Convention:</p> <ul> <li>For batch tensors: shape is <code>(batch_size, *)</code></li> <li>The batch dimension contains independent samples</li> <li>Operations along this dimension process each sample in the batch</li> </ul>"},{"location":"uguide/constants/#seq_dim","title":"SEQ_DIM","text":"<p>The sequence dimension constant identifies the dimension used for sequences in tensors.</p> <pre><code>&gt;&gt;&gt; from batchtensor.constants import SEQ_DIM\n&gt;&gt;&gt; SEQ_DIM\n1\n</code></pre> <p>Usage: This constant is set to <code>1</code>, indicating that the sequence dimension is always the second dimension (dimension 1) of tensors when using batchtensor functions.</p> <p>Convention:</p> <ul> <li>For sequence tensors: shape is <code>(batch_size, seq_len, *)</code></li> <li>The sequence dimension contains sequential/temporal data</li> <li>Operations along this dimension process time steps or sequence positions</li> </ul>"},{"location":"uguide/constants/#why-use-constants","title":"Why Use Constants?","text":"<p>Using constants instead of hard-coded numbers provides several benefits:</p> <ol> <li>Code Clarity: <code>BATCH_DIM</code> is more readable than <code>0</code></li> <li>Consistency: Ensures all operations use the same dimension conventions</li> <li>Maintainability: If dimension conventions ever change, updating the constant updates all uses</li> <li>Self-Documentation: Code using these constants is self-explanatory</li> </ol>"},{"location":"uguide/constants/#practical-examples","title":"Practical Examples","text":""},{"location":"uguide/constants/#using-constants-in-your-code","title":"Using Constants in Your Code","text":"<p>When working with batchtensor functions, you can use these constants to make your code more explicit:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.constants import BATCH_DIM, SEQ_DIM\n&gt;&gt;&gt; # Create a batch of sequences\n&gt;&gt;&gt; # Shape: (batch_size=2, seq_len=3, features=4)\n&gt;&gt;&gt; data = torch.randn(2, 3, 4)\n&gt;&gt;&gt; # Check dimensions\n&gt;&gt;&gt; batch_size = data.size(BATCH_DIM)\n&gt;&gt;&gt; seq_len = data.size(SEQ_DIM)\n&gt;&gt;&gt; print(f\"Batch size: {batch_size}, Sequence length: {seq_len}\")\nBatch size: 2, Sequence length: 3\n</code></pre>"},{"location":"uguide/constants/#manual-operations-with-constants","title":"Manual Operations with Constants","text":"<p>When you need to perform operations directly with PyTorch but want to maintain consistency with batchtensor conventions:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.constants import BATCH_DIM, SEQ_DIM\n&gt;&gt;&gt; data = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n&gt;&gt;&gt; # Sum along batch dimension\n&gt;&gt;&gt; batch_sum = data.sum(dim=BATCH_DIM)\n&gt;&gt;&gt; batch_sum\ntensor([[ 6,  8],\n        [10, 12]])\n&gt;&gt;&gt; # Sum along sequence dimension\n&gt;&gt;&gt; seq_sum = data.sum(dim=SEQ_DIM)\n&gt;&gt;&gt; seq_sum\ntensor([[ 4,  6],\n        [12, 14]])\n</code></pre>"},{"location":"uguide/constants/#verifying-tensor-shapes","title":"Verifying Tensor Shapes","text":"<p>Use constants to verify that your tensors have the expected shape:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.constants import BATCH_DIM, SEQ_DIM\n&gt;&gt;&gt; def validate_batch_tensor(tensor, expected_batch_size):\n...     \"\"\"Validate that a tensor has the expected batch size.\"\"\"\n...     actual_batch_size = tensor.size(BATCH_DIM)\n...     if actual_batch_size != expected_batch_size:\n...         raise ValueError(\n...             f\"Expected batch size {expected_batch_size}, \" f\"got {actual_batch_size}\"\n...         )\n...     return True\n...\n&gt;&gt;&gt; tensor = torch.randn(32, 10)  # batch_size=32, features=10\n&gt;&gt;&gt; validate_batch_tensor(tensor, expected_batch_size=32)\nTrue\n</code></pre>"},{"location":"uguide/constants/#creating-batches-and-sequences","title":"Creating Batches and Sequences","text":"<p>Use constants when manually creating or reshaping tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.constants import BATCH_DIM, SEQ_DIM\n&gt;&gt;&gt; # Create individual samples\n&gt;&gt;&gt; sample1 = torch.tensor([[1, 2], [3, 4]])  # seq_len=2, features=2\n&gt;&gt;&gt; sample2 = torch.tensor([[5, 6], [7, 8]])\n&gt;&gt;&gt; # Stack into a batch along batch dimension\n&gt;&gt;&gt; batch = torch.stack([sample1, sample2], dim=BATCH_DIM)\n&gt;&gt;&gt; batch.shape\ntorch.Size([2, 2, 2])\n&gt;&gt;&gt; # Verify dimensions\n&gt;&gt;&gt; print(f\"Batch size: {batch.size(BATCH_DIM)}\")\nBatch size: 2\n&gt;&gt;&gt; print(f\"Sequence length: {batch.size(SEQ_DIM)}\")\nSequence length: 2\n</code></pre>"},{"location":"uguide/constants/#internal-usage","title":"Internal Usage","text":"<p>These constants are used internally throughout batchtensor. For example (simplified pseudocode):</p> <pre><code># In batchtensor.tensor.reduction\ndef sum_along_batch(tensor: torch.Tensor, keepdim: bool = False) -&gt; torch.Tensor:\n    \"\"\"Sum tensor along the batch dimension.\"\"\"\n    return tensor.sum(dim=BATCH_DIM, keepdim=keepdim)\n\n\n# In batchtensor.tensor.slicing (simplified for illustration)\ndef select_along_seq(tensor: torch.Tensor, index: int) -&gt; torch.Tensor:\n    \"\"\"Select a specific index along the sequence dimension.\"\"\"\n    return tensor.select(dim=SEQ_DIM, index=index)\n</code></pre> <p>This ensures consistency across all functions in the library.</p>"},{"location":"uguide/constants/#compatibility-with-other-libraries","title":"Compatibility with Other Libraries","text":"<p>While batchtensor uses these specific dimension conventions, they are compatible with common PyTorch practices:</p> <ul> <li>Batch-first convention: Most PyTorch modules (like <code>nn.Linear</code>, <code>nn.GRU</code> with   <code>batch_first=True</code>) expect batch as the first dimension</li> <li>Standard computer vision: Images are typically <code>(batch, channels, height, width)</code>, compatible   with <code>BATCH_DIM=0</code></li> <li>NLP sequence models: With <code>batch_first=True</code>, sequences are <code>(batch, seq_len, features)</code>,   matching our conventions</li> </ul>"},{"location":"uguide/constants/#best-practices","title":"Best Practices","text":"<ol> <li>Import the constants when you need to reference dimensions explicitly</li> <li>Use in assertions to validate tensor shapes in your code</li> <li>Prefer batchtensor functions over manual dimension handling when possible</li> <li>Document assumptions about tensor shapes in your own functions</li> </ol>"},{"location":"uguide/constants/#see-also","title":"See Also","text":"<ul> <li>Tensor Operations - Functions that use these dimension constants</li> <li>Nested Operations - Nested operations following the same conventions</li> </ul>"},{"location":"uguide/nested/","title":"Nested Data Manipulation","text":"<p>The <code>batchtensor.nested</code> module provides functions to manipulate nested data structures containing PyTorch tensors. These functions work with dictionaries, lists, tuples, and other nested structures where tensors share batch or sequence dimensions.</p>"},{"location":"uguide/nested/#overview","title":"Overview","text":"<p>When working with complex data pipelines, you often have batches represented as nested structures ( e.g., dictionaries of tensors). The nested module makes it easy to apply operations across all tensors in these structures while preserving the nested structure.</p> <p>Key Benefits:</p> <ul> <li>Automatic recursion: Operations are applied to all tensors in the structure</li> <li>Structure preservation: The nested structure (dict, list, tuple) is maintained</li> <li>Consistency: All tensors are processed with the same operation</li> <li>Convenience: No need to manually iterate and apply operations</li> </ul> <p>Supported Structures:</p> <ul> <li>Dictionaries: <code>{'key1': tensor1, 'key2': tensor2, ...}</code></li> <li>Lists: <code>[tensor1, tensor2, ...]</code></li> <li>Tuples: <code>(tensor1, tensor2, ...)</code></li> <li>Nested combinations: <code>{'key': [tensor1, tensor2], ...}</code></li> </ul> <p>Dimension Conventions:</p> <ul> <li>Batch dimension: Always dimension 0</li> <li>Sequence dimension: Always dimension 1</li> <li>See Constants for more details</li> </ul>"},{"location":"uguide/nested/#slicing-operations","title":"Slicing Operations","text":""},{"location":"uguide/nested/#slicing-along-batch-dimension","title":"Slicing Along Batch Dimension","text":"<p>Extract a subset of the batch from all tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]]),\n...     \"labels\": torch.tensor([0, 1, 0, 1]),\n...     \"weights\": torch.tensor([1.0, 0.5, 0.8, 1.2]),\n... }\n&gt;&gt;&gt; # Take first 2 items\n&gt;&gt;&gt; slice_along_batch(batch, stop=2)\n{'features': tensor([[1, 2], [3, 4]]), 'labels': tensor([0, 1]), 'weights': tensor([1.0000, 0.5000])}\n</code></pre>"},{"location":"uguide/nested/#slicing-along-sequence-dimension","title":"Slicing Along Sequence Dimension","text":"<p>For sequential data with shape <code>(batch_size, seq_len, *)</code>:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_seq\n&gt;&gt;&gt; batch = {\n...     \"tokens\": torch.tensor([[[1], [2], [3], [4]], [[5], [6], [7], [8]]]),\n...     \"attention\": torch.tensor([[1.0, 0.9, 0.8, 0.7], [1.0, 0.95, 0.9, 0.85]]),\n... }\n&gt;&gt;&gt; # Take first 2 timesteps\n&gt;&gt;&gt; slice_along_seq(batch, stop=2)\n{'tokens': tensor([[[1], [2]], [[5], [6]]]), 'attention': tensor([[1.0000, 0.9000], [1.0000, 0.9500]])}\n</code></pre>"},{"location":"uguide/nested/#chunking","title":"Chunking","text":"<p>Split tensors into equal chunks:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; chunks = chunk_along_batch(batch, chunks=3)\n&gt;&gt;&gt; len(chunks)\n3\n&gt;&gt;&gt; chunks[0]\n{'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])}\n</code></pre>"},{"location":"uguide/nested/#splitting","title":"Splitting","text":"<p>Split tensors by size:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; splits = split_along_batch(batch, split_size_or_sections=2)\n&gt;&gt;&gt; len(splits)\n3\n&gt;&gt;&gt; splits[0]\n{'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3])}\n</code></pre>"},{"location":"uguide/nested/#indexing-operations","title":"Indexing Operations","text":"<p>Select specific indices from all tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; indices = torch.tensor([2, 0])\n&gt;&gt;&gt; index_select_along_batch(batch, indices)\n{'features': tensor([[5, 6], [1, 2]]), 'labels': tensor([2, 0])}\n</code></pre>"},{"location":"uguide/nested/#joining-operations","title":"Joining Operations","text":""},{"location":"uguide/nested/#concatenation","title":"Concatenation","text":"<p>Combine multiple batches:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_batch\n&gt;&gt;&gt; batch1 = {\"a\": torch.tensor([[1, 2]]), \"b\": torch.tensor([10])}\n&gt;&gt;&gt; batch2 = {\"a\": torch.tensor([[3, 4]]), \"b\": torch.tensor([20])}\n&gt;&gt;&gt; cat_along_batch([batch1, batch2])\n{'a': tensor([[1, 2], [3, 4]]), 'b': tensor([10, 20])}\n</code></pre>"},{"location":"uguide/nested/#repetition","title":"Repetition","text":"<p>Repeat sequences:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import repeat_along_seq\n&gt;&gt;&gt; batch = {\n...     \"seq\": torch.tensor([[[1], [2]], [[3], [4]]]),\n... }\n&gt;&gt;&gt; repeat_along_seq(batch, 2)\n{'seq': tensor([[[1], [2], [1], [2]], [[3], [4], [3], [4]]])}\n</code></pre>"},{"location":"uguide/nested/#reduction-operations","title":"Reduction Operations","text":""},{"location":"uguide/nested/#sum-mean-min-max","title":"Sum, Mean, Min, Max","text":"<p>Compute statistics along dimensions:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import (\n...     sum_along_batch,\n...     mean_along_batch,\n...     amax_along_batch,\n...     amin_along_batch,\n... )\n&gt;&gt;&gt; batch = {\n...     \"scores\": torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),\n...     \"counts\": torch.tensor([10.0, 20.0, 30.0]),\n... }\n&gt;&gt;&gt; sum_along_batch(batch)\n{'scores': tensor([ 9., 12.]), 'counts': tensor(60.)}\n&gt;&gt;&gt; mean_along_batch(batch)\n{'scores': tensor([3., 4.]), 'counts': tensor(20.)}\n</code></pre>"},{"location":"uguide/nested/#argmax-and-argmin","title":"ArgMax and ArgMin","text":"<p>Find indices of extrema:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_batch, argmin_along_batch\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 5], [3, 2], [4, 6]])}\n&gt;&gt;&gt; argmax_along_batch(batch)\n{'values': tensor([2, 2])}\n&gt;&gt;&gt; argmin_along_batch(batch)\n{'values': tensor([0, 1])}\n</code></pre>"},{"location":"uguide/nested/#comparison-and-sorting","title":"Comparison and Sorting","text":"<p>Sort tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_batch\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[3, 1], [1, 4], [2, 2]])}\n&gt;&gt;&gt; sort_along_batch(batch)\n{'values': torch.return_types.sort(\nvalues=tensor([[1, 1],\n        [2, 2],\n        [3, 4]]),\nindices=tensor([[1, 0],\n        [2, 2],\n        [0, 1]]))}\n</code></pre>"},{"location":"uguide/nested/#mathematical-operations","title":"Mathematical Operations","text":""},{"location":"uguide/nested/#cumulative-operations","title":"Cumulative Operations","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_batch, cumprod_along_batch\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 2], [3, 4], [5, 6]])}\n&gt;&gt;&gt; cumsum_along_batch(batch)\n{'values': tensor([[ 1,  2],\n                   [ 4,  6],\n                   [ 9, 12]])}\n</code></pre>"},{"location":"uguide/nested/#trigonometric-functions","title":"Trigonometric Functions","text":"<p>Apply trigonometric functions element-wise:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sin\n&gt;&gt;&gt; batch = {\"angles\": torch.tensor([[0.0, 1.57], [3.14, 4.71]])}\n&gt;&gt;&gt; sin(batch)\n{'angles': tensor([[ 0.0000,  1.0000],\n                   [ 0.0016, -1.0000]])}\n</code></pre>"},{"location":"uguide/nested/#pointwise-operations","title":"Pointwise Operations","text":"<p>Apply element-wise operations:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import abs\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[-1.0, 2.0], [-3.0, 4.0]])}\n&gt;&gt;&gt; abs(batch)\n{'values': tensor([[1., 2.], [3., 4.]])}\n</code></pre>"},{"location":"uguide/nested/#permutation-operations","title":"Permutation Operations","text":""},{"location":"uguide/nested/#shuffling","title":"Shuffling","text":"<p>Randomly permute batch items:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; # Results will be random\n&gt;&gt;&gt; shuffle_along_batch(batch)  # doctest: +SKIP\n{'features': tensor([[5, 6], [1, 2], [3, 4]]), 'labels': tensor([2, 0, 1])}\n</code></pre>"},{"location":"uguide/nested/#permuting","title":"Permuting","text":"<p>Apply a specific permutation:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; permutation = torch.tensor([2, 0, 1])\n&gt;&gt;&gt; permute_along_batch(batch, permutation)\n{'features': tensor([[5, 6], [1, 2], [3, 4]]), 'labels': tensor([2, 0, 1])}\n</code></pre>"},{"location":"uguide/nested/#type-conversion","title":"Type Conversion","text":""},{"location":"uguide/nested/#change-device","title":"Change Device","text":"<p>Move all tensors to a specific device:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4]]),\n...     \"labels\": torch.tensor([0, 1]),\n... }\n&gt;&gt;&gt; # Move to CPU (already there in this example)\n&gt;&gt;&gt; to(batch, torch.device(\"cpu\"))\n{'features': tensor([[1, 2], [3, 4]]), 'labels': tensor([0, 1])}\n</code></pre>"},{"location":"uguide/nested/#change-data-type","title":"Change Data Type","text":"<p>Convert tensor dtypes:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 2], [3, 4]], dtype=torch.int32)}\n&gt;&gt;&gt; to(batch, dtype=torch.float32)\n{'values': tensor([[1., 2.], [3., 4.]])}\n</code></pre>"},{"location":"uguide/nested/#numpy-conversion","title":"NumPy Conversion","text":"<p>Convert between PyTorch tensors and NumPy arrays:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import from_numpy, to_numpy\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # From NumPy\n&gt;&gt;&gt; numpy_data = {\n...     \"a\": np.array([[1, 2], [3, 4]]),\n...     \"b\": np.array([5, 6]),\n... }\n&gt;&gt;&gt; batch = from_numpy(numpy_data)\n&gt;&gt;&gt; batch\n{'a': tensor([[1, 2], [3, 4]]), 'b': tensor([5, 6])}\n&gt;&gt;&gt; # To NumPy\n&gt;&gt;&gt; to_numpy(batch)\n{'a': array([[1, 2], [3, 4]]), 'b': array([5, 6])}\n</code></pre>"},{"location":"uguide/nested/#additional-operations","title":"Additional Operations","text":""},{"location":"uguide/nested/#type-conversion_1","title":"Type Conversion","text":"<p>Convert tensor types in nested structures:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 2], [3, 4]], dtype=torch.int32)}\n&gt;&gt;&gt; to(batch, dtype=torch.float32)\n{'values': tensor([[1., 2.], [3., 4.]])}\n</code></pre> <p>See the Type Conversion section for more examples.</p>"},{"location":"uguide/nested/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Dimensions: Ensure all tensors in your nested structure have compatible    batch/sequence dimensions</li> <li>Memory Efficiency: Many operations like <code>chunk</code> and <code>slice</code> return views when possible, not    copies</li> <li>Type Safety: Use the same data types across tensors in a batch for predictable behavior</li> <li>Error Handling: Functions will raise clear errors if tensors have incompatible shapes</li> </ol>"},{"location":"uguide/nested/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Operations are applied recursively to nested structures</li> <li>Views are used when possible to avoid copying data</li> <li>All functions leverage PyTorch's efficient tensor operations</li> <li>For very deep nesting, consider flattening your data structure</li> <li>Nested operations have minimal overhead compared to manual iteration</li> </ul>"},{"location":"uguide/nested/#see-also","title":"See Also","text":"<ul> <li>Tensor Operations - Operations for single tensors</li> <li>API Reference - Complete function reference</li> <li>Utils Module - Utility functions for seed management</li> <li>Constants - Dimension constants used throughout the library</li> </ul>"},{"location":"uguide/tensor/","title":"Tensor Operations","text":"<p>The <code>batchtensor.tensor</code> module provides functions to manipulate individual PyTorch tensors along batch and sequence dimensions. These functions are the foundation for the nested operations and can be used directly when working with single tensors.</p>"},{"location":"uguide/tensor/#overview","title":"Overview","text":"<p>The tensor module operates on individual tensors with two primary dimensional conventions:</p> <ul> <li>Batch dimension: The first dimension (dimension 0) represents the batch</li> <li>Sequence dimension: The second dimension (dimension 1) represents the sequence/time steps</li> </ul> <p>All functions in this module follow these conventions. The shape assumptions are:</p> <ul> <li>Batch operations: <code>(batch_size, *)</code> where <code>*</code> means any additional dimensions</li> <li>Sequence operations: <code>(batch_size, seq_len, *)</code> where <code>*</code> means any additional dimensions</li> </ul>"},{"location":"uguide/tensor/#slicing-operations","title":"Slicing Operations","text":""},{"location":"uguide/tensor/#slicing-along-batch-dimension","title":"Slicing Along Batch Dimension","text":"<p>Extract a subset of the batch:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n&gt;&gt;&gt; # Take first 2 items\n&gt;&gt;&gt; slice_along_batch(tensor, stop=2)\ntensor([[1, 2],\n        [3, 4]])\n&gt;&gt;&gt; # Take items 1 to 3\n&gt;&gt;&gt; slice_along_batch(tensor, start=1, stop=3)\ntensor([[3, 4],\n        [5, 6]])\n&gt;&gt;&gt; # Take every other item\n&gt;&gt;&gt; slice_along_batch(tensor, step=2)\ntensor([[1, 2],\n        [5, 6]])\n</code></pre>"},{"location":"uguide/tensor/#slicing-along-sequence-dimension","title":"Slicing Along Sequence Dimension","text":"<p>For sequential data:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n&gt;&gt;&gt; # Take first 2 timesteps\n&gt;&gt;&gt; slice_along_seq(tensor, stop=2)\ntensor([[1, 2],\n        [5, 6]])\n&gt;&gt;&gt; # Take last 2 timesteps\n&gt;&gt;&gt; slice_along_seq(tensor, start=2)\ntensor([[3, 4],\n        [7, 8]])\n</code></pre>"},{"location":"uguide/tensor/#selecting-single-items","title":"Selecting Single Items","text":"<p>Select a specific index:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_batch, select_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; # Select second batch item (removes batch dimension)\n&gt;&gt;&gt; select_along_batch(tensor, index=1)\ntensor([4, 5, 6])\n&gt;&gt;&gt; # Select first sequence position (removes sequence dimension)\n&gt;&gt;&gt; select_along_seq(tensor, index=0)\ntensor([1, 4, 7])\n</code></pre>"},{"location":"uguide/tensor/#chunking","title":"Chunking","text":"<p>Split tensors into equal chunks:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_batch, chunk_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; chunks = chunk_along_batch(tensor, chunks=3)\n&gt;&gt;&gt; len(chunks)\n3\n&gt;&gt;&gt; chunks[0]\ntensor([[0, 1],\n        [2, 3]])\n&gt;&gt;&gt; chunks[1]\ntensor([[4, 5],\n        [6, 7]])\n&gt;&gt;&gt; chunks[2]\ntensor([[8, 9]])\n</code></pre>"},{"location":"uguide/tensor/#splitting","title":"Splitting","text":"<p>Split tensors by specific sizes:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; # Split into chunks of size 2\n&gt;&gt;&gt; splits = split_along_batch(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; len(splits)\n3\n&gt;&gt;&gt; splits[0]\ntensor([[0, 1],\n        [2, 3]])\n&gt;&gt;&gt; splits[1]\ntensor([[4, 5],\n        [6, 7]])\n&gt;&gt;&gt; splits[2]\ntensor([[8, 9]])\n&gt;&gt;&gt; # Split with specific sizes\n&gt;&gt;&gt; splits = split_along_batch(tensor, split_size_or_sections=[2, 1, 2])\n&gt;&gt;&gt; len(splits)\n3\n&gt;&gt;&gt; splits[0]\ntensor([[0, 1],\n        [2, 3]])\n&gt;&gt;&gt; splits[1]\ntensor([[4, 5]])\n&gt;&gt;&gt; splits[2]\ntensor([[6, 7],\n        [8, 9]])\n</code></pre>"},{"location":"uguide/tensor/#indexing-operations","title":"Indexing Operations","text":""},{"location":"uguide/tensor/#index-selection","title":"Index Selection","text":"<p>Select specific indices from a tensor:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; indices = torch.tensor([2, 0, 1])\n&gt;&gt;&gt; index_select_along_batch(tensor, indices)\ntensor([[5, 6],\n        [1, 2],\n        [3, 4]])\n</code></pre> <p>For sequences:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n&gt;&gt;&gt; indices = torch.tensor([3, 0, 2])\n&gt;&gt;&gt; index_select_along_seq(tensor, indices)\ntensor([[4, 1, 3],\n        [8, 5, 7]])\n</code></pre>"},{"location":"uguide/tensor/#joining-operations","title":"Joining Operations","text":""},{"location":"uguide/tensor/#concatenation","title":"Concatenation","text":"<p>Combine multiple tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_batch\n&gt;&gt;&gt; tensor1 = torch.tensor([[1, 2]])\n&gt;&gt;&gt; tensor2 = torch.tensor([[3, 4]])\n&gt;&gt;&gt; tensor3 = torch.tensor([[5, 6]])\n&gt;&gt;&gt; cat_along_batch([tensor1, tensor2, tensor3])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n</code></pre> <p>Concatenate along sequence dimension:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_seq\n&gt;&gt;&gt; tensor1 = torch.tensor([[1], [2]])\n&gt;&gt;&gt; tensor2 = torch.tensor([[3], [4]])\n&gt;&gt;&gt; cat_along_seq([tensor1, tensor2])\ntensor([[1, 3],\n        [2, 4]])\n</code></pre>"},{"location":"uguide/tensor/#repetition","title":"Repetition","text":"<p>Repeat sequences:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import repeat_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4]])\n&gt;&gt;&gt; repeat_along_seq(tensor, repeats=3)\ntensor([[1, 2, 1, 2, 1, 2],\n        [3, 4, 3, 4, 3, 4]])\n</code></pre>"},{"location":"uguide/tensor/#reduction-operations","title":"Reduction Operations","text":""},{"location":"uguide/tensor/#sum-and-mean","title":"Sum and Mean","text":"<p>Compute sum along dimensions:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_batch, mean_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n&gt;&gt;&gt; sum_along_batch(tensor)\ntensor([ 9., 12.])\n&gt;&gt;&gt; mean_along_batch(tensor)\ntensor([3., 4.])\n&gt;&gt;&gt; # Keep dimensions\n&gt;&gt;&gt; sum_along_batch(tensor, keepdim=True)\ntensor([[ 9., 12.]])\n</code></pre> <p>Along sequence dimension:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_seq, mean_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n&gt;&gt;&gt; sum_along_seq(tensor)\ntensor([ 6., 15.])\n&gt;&gt;&gt; mean_along_seq(tensor)\ntensor([2., 5.])\n</code></pre>"},{"location":"uguide/tensor/#product","title":"Product","text":"<p>Compute product along dimensions:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 3], [4, 5]])\n&gt;&gt;&gt; prod_along_batch(tensor)\ntensor([ 8, 15])\n</code></pre>"},{"location":"uguide/tensor/#min-and-max","title":"Min and Max","text":"<p>Find minimum and maximum values:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import (\n...     amax_along_batch,\n...     amin_along_batch,\n...     max_along_batch,\n...     min_along_batch,\n... )\n&gt;&gt;&gt; tensor = torch.tensor([[1, 5], [3, 2], [4, 6]])\n&gt;&gt;&gt; amax_along_batch(tensor)\ntensor([4, 6])\n&gt;&gt;&gt; amin_along_batch(tensor)\ntensor([1, 2])\n&gt;&gt;&gt; # max and min return both values and indices\n&gt;&gt;&gt; max_along_batch(tensor)\ntorch.return_types.max(\nvalues=tensor([4, 6]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; min_along_batch(tensor)\ntorch.return_types.min(\nvalues=tensor([1, 2]),\nindices=tensor([0, 1]))\n</code></pre>"},{"location":"uguide/tensor/#argmax-and-argmin","title":"ArgMax and ArgMin","text":"<p>Find indices of extrema:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_batch, argmin_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 5], [3, 2], [4, 6]])\n&gt;&gt;&gt; argmax_along_batch(tensor)\ntensor([2, 2])\n&gt;&gt;&gt; argmin_along_batch(tensor)\ntensor([0, 1])\n</code></pre>"},{"location":"uguide/tensor/#median","title":"Median","text":"<p>Compute median values:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 5], [3, 2], [4, 6]])\n&gt;&gt;&gt; median_along_batch(tensor)\ntorch.return_types.median(\nvalues=tensor([3, 5]),\nindices=tensor([1, 0]))\n</code></pre>"},{"location":"uguide/tensor/#comparison-and-sorting","title":"Comparison and Sorting","text":""},{"location":"uguide/tensor/#sorting","title":"Sorting","text":"<p>Sort tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_batch, argsort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[3, 1], [1, 4], [2, 2]])\n&gt;&gt;&gt; sort_along_batch(tensor)\ntorch.return_types.sort(\nvalues=tensor([[1, 1],\n               [2, 2],\n               [3, 4]]),\nindices=tensor([[1, 0],\n                [2, 2],\n                [0, 1]]))\n&gt;&gt;&gt; # Get indices only\n&gt;&gt;&gt; argsort_along_batch(tensor)\ntensor([[1, 0],\n        [2, 2],\n        [0, 1]])\n&gt;&gt;&gt; # Sort in descending order\n&gt;&gt;&gt; sort_along_batch(tensor, descending=True)\ntorch.return_types.sort(\nvalues=tensor([[3, 4],\n               [2, 2],\n               [1, 1]]),\nindices=tensor([[0, 1],\n                [2, 2],\n                [1, 0]]))\n</code></pre>"},{"location":"uguide/tensor/#mathematical-operations","title":"Mathematical Operations","text":""},{"location":"uguide/tensor/#cumulative-operations","title":"Cumulative Operations","text":"<p>Compute cumulative sums and products:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_batch, cumprod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; cumsum_along_batch(tensor)\ntensor([[ 1,  2],\n        [ 4,  6],\n        [ 9, 12]])\n&gt;&gt;&gt; cumprod_along_batch(tensor)\ntensor([[ 1,  2],\n        [ 3,  8],\n        [15, 48]])\n</code></pre> <p>Along sequence dimension:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; cumsum_along_seq(tensor)\ntensor([[ 1,  3,  6],\n        [ 4,  9, 15]])\n</code></pre>"},{"location":"uguide/tensor/#permutation-operations","title":"Permutation Operations","text":""},{"location":"uguide/tensor/#shuffling","title":"Shuffling","text":"<p>Randomly permute batch items:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; # Results will be random\n&gt;&gt;&gt; shuffled = shuffle_along_batch(tensor)\n</code></pre> <p>For reproducible shuffling, use a generator:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; from batchtensor.utils.seed import get_torch_generator\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; shuffled = shuffle_along_batch(tensor, generator=generator)\n</code></pre>"},{"location":"uguide/tensor/#permuting-with-specific-order","title":"Permuting with Specific Order","text":"<p>Apply a specific permutation:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; permutation = torch.tensor([2, 0, 1])\n&gt;&gt;&gt; permute_along_batch(tensor, permutation)\ntensor([[5, 6],\n        [1, 2],\n        [3, 4]])\n</code></pre>"},{"location":"uguide/tensor/#best-practices","title":"Best Practices","text":"<ol> <li>Dimension Awareness: Always ensure your tensors follow the expected shape conventions    (batch dimension first, sequence dimension second)</li> <li>Memory Efficiency: Many operations like <code>slice</code> and <code>select</code> return views when possible,    not copies</li> <li>Batch Processing: These functions are optimized for batch processing, making them efficient    for handling multiple samples simultaneously</li> <li>Consistency: Use these functions instead of direct indexing to maintain code clarity and    reduce errors</li> </ol>"},{"location":"uguide/tensor/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Operations leverage PyTorch's optimized tensor operations</li> <li>Views are used when possible to avoid data copying</li> <li>Batch operations are more efficient than processing items one at a time</li> <li>GPU acceleration is available through PyTorch's device management</li> </ul>"},{"location":"uguide/tensor/#see-also","title":"See Also","text":"<ul> <li>Nested Operations - Operations for nested data structures</li> <li>API Reference - Complete function reference</li> <li>Constants - Dimension constants used throughout the library</li> </ul>"},{"location":"uguide/utils/","title":"Utility Functions","text":"<p>The <code>batchtensor.utils</code> module provides utility functions that support the main tensor operations, particularly for managing random seeds and ensuring reproducibility.</p>"},{"location":"uguide/utils/#overview","title":"Overview","text":"<p>The utils module currently contains seed management utilities that are essential for:</p> <ul> <li>Generating reproducible random operations</li> <li>Managing PyTorch random number generators</li> <li>Ensuring consistent behavior across different runs</li> </ul>"},{"location":"uguide/utils/#seed-management","title":"Seed Management","text":""},{"location":"uguide/utils/#random-seed-generation","title":"Random Seed Generation","text":"<p>Generate a random seed for reproducible operations:</p> <pre><code>&gt;&gt;&gt; from batchtensor.utils.seed import get_random_seed\n&gt;&gt;&gt; seed1 = get_random_seed(42)\n&gt;&gt;&gt; # Same input always produces same output\n&gt;&gt;&gt; seed2 = get_random_seed(42)\n&gt;&gt;&gt; # Different input produces different output\n&gt;&gt;&gt; seed3 = get_random_seed(100)\n</code></pre> <p>The <code>get_random_seed</code> function is useful when you need to derive additional random seeds from a master seed while maintaining reproducibility.</p> <p>Key Features:</p> <ul> <li>Returns values between <code>-2^63</code> and <code>2^63 - 1</code></li> <li>Deterministic: same input always produces same output</li> <li>Useful for creating multiple independent random streams</li> </ul>"},{"location":"uguide/utils/#pytorch-generator-creation","title":"PyTorch Generator Creation","text":"<p>Create a PyTorch generator with a specific seed:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils.seed import get_torch_generator\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; # Use with PyTorch random operations\n&gt;&gt;&gt; torch.rand(2, 3, generator=generator)\ntensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009]])\n&gt;&gt;&gt; # Create a new generator with the same seed for reproducibility\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; torch.rand(2, 3, generator=generator)\ntensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009]])\n</code></pre> <p>Parameters:</p> <ul> <li><code>random_seed</code> (int): The random seed to initialize the generator</li> <li><code>device</code> (torch.device | str | None): The device for the generator (default: \"cpu\")</li> </ul> <p>Use Cases:</p> <ul> <li>Creating reproducible random tensors</li> <li>Ensuring consistent shuffling operations</li> <li>Testing and debugging with deterministic behavior</li> </ul>"},{"location":"uguide/utils/#generator-setup","title":"Generator Setup","text":"<p>Set up a generator from either a seed or an existing generator:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils.seed import setup_torch_generator\n&gt;&gt;&gt; # From a seed\n&gt;&gt;&gt; generator = setup_torch_generator(42)\n&gt;&gt;&gt; generator\n&lt;torch._C.Generator object at 0x...&gt;\n&gt;&gt;&gt; # From an existing generator (returns the same generator)\n&gt;&gt;&gt; existing_generator = torch.Generator()\n&gt;&gt;&gt; existing_generator.manual_seed(100)\n&lt;torch._C.Generator object at 0x...&gt;\n&gt;&gt;&gt; result = setup_torch_generator(existing_generator)\n&gt;&gt;&gt; result is existing_generator\nTrue\n</code></pre> <p>This function is particularly useful in library code where you want to accept either a seed or a generator as input.</p>"},{"location":"uguide/utils/#practical-examples","title":"Practical Examples","text":""},{"location":"uguide/utils/#reproducible-shuffling","title":"Reproducible Shuffling","text":"<p>Use generators to ensure reproducible shuffling:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; from batchtensor.utils.seed import get_torch_generator\n&gt;&gt;&gt; data = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n&gt;&gt;&gt; # First run with seed 42\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; shuffled = shuffle_along_batch(data, generator=generator)\n&gt;&gt;&gt; # Second run with same seed produces same result\n&gt;&gt;&gt; generator = get_torch_generator(42)\n&gt;&gt;&gt; shuffled = shuffle_along_batch(data, generator=generator)\n</code></pre>"},{"location":"uguide/utils/#multiple-independent-random-streams","title":"Multiple Independent Random Streams","text":"<p>Create multiple independent random streams from a master seed:</p> <pre><code>&gt;&gt;&gt; from batchtensor.utils.seed import get_random_seed, get_torch_generator\n&gt;&gt;&gt; master_seed = 42\n&gt;&gt;&gt; # Create seeds for different purposes\n&gt;&gt;&gt; train_seed = get_random_seed(master_seed)\n&gt;&gt;&gt; val_seed = get_random_seed(master_seed + 1)\n&gt;&gt;&gt; test_seed = get_random_seed(master_seed + 2)\n&gt;&gt;&gt; # Create independent generators\n&gt;&gt;&gt; train_gen = get_torch_generator(train_seed)\n&gt;&gt;&gt; val_gen = get_torch_generator(val_seed)\n&gt;&gt;&gt; test_gen = get_torch_generator(test_seed)\n</code></pre>"},{"location":"uguide/utils/#device-specific-generators","title":"Device-Specific Generators","text":"<p>Create generators for different devices:</p> <pre><code>&gt;&gt;&gt; from batchtensor.utils.seed import get_torch_generator\n&gt;&gt;&gt; # CPU generator\n&gt;&gt;&gt; cpu_gen = get_torch_generator(42, device=\"cpu\")\n&gt;&gt;&gt; # GPU generator (if CUDA is available)\n&gt;&gt;&gt; if torch.cuda.is_available():  # doctest: +SKIP\n...     gpu_gen = get_torch_generator(42, device=\"cuda\")\n...\n</code></pre>"},{"location":"uguide/utils/#best-practices","title":"Best Practices","text":"<ol> <li>Always Use Seeds in Tests: When writing tests, always use seeded generators to ensure    reproducible results</li> <li>Document Random Behavior: When using random operations in your code, document the expected    behavior and how to control it with seeds</li> <li>Separate Random Streams: Use different seeds for different purposes (training, validation,    testing) to avoid correlations</li> <li>Generator Reuse: Reuse generators when you want to maintain a sequence of random operations,    create new ones when you want independence</li> </ol>"},{"location":"uguide/utils/#see-also","title":"See Also","text":"<ul> <li>Tensor Operations - Operations that use random number generators</li> <li>Nested Operations - Nested operations with randomness</li> <li>PyTorch Random Sampling - PyTorch's random number   generation documentation</li> </ul>"}]}