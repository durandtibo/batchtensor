{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p><code>batchtensor</code> is lightweight library built on top of PyTorch to manipulate nested data structure with PyTorch tensors. This library provides functions for tensors where the first dimension is the batch dimension. It also provides functions for tensors representing a batch of sequences where the first dimension is the batch dimension and the second dimension is the sequence dimension.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Let's imagine you have a batch which is represented by a dictionary with three tensors, and you want to take the first 2 items. <code>batchtensor</code> provides the function <code>slice_along_batch</code> that allows to slide all the tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n...     \"c\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n... }\n&gt;&gt;&gt; slice_along_batch(batch, stop=2)\n{'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3]), 'c': tensor([1., 2.])}\n</code></pre> <p>Similarly, it is possible to split a batch in multiple batches by using the function <code>split_along_batch</code>:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n...     \"c\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n... }\n&gt;&gt;&gt; split_along_batch(batch, split_size_or_sections=2)\n({'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3]), 'c': tensor([1., 2.])},\n {'a': tensor([[4, 9], [8, 1]]), 'b': tensor([2, 1]), 'c': tensor([3., 4.])},\n {'a': tensor([[5, 7]]), 'b': tensor([0]), 'c': tensor([5.])})\n</code></pre> <p>Please check the documentation to see all the implemented functions.</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>batchtensor</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>batchtensor</code> to a new version will possibly break any code that was using the old version of <code>batchtensor</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>batchtensor</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install batchtensor\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>batchtensor</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'batchtensor[all]'\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>batchtensor</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/batchtensor.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate batchtensor\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>batchtensor</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/nested/","title":"Nested","text":""},{"location":"refs/nested/#batchtensor.nested","title":"batchtensor.nested","text":"<p>Contain functions to manipulate nested data.</p>"},{"location":"refs/nested/#batchtensor.nested.abs","title":"batchtensor.nested.abs","text":"<pre><code>abs(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the absolute value of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The absolute value of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import abs\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[-4, -3], [-2, -1], [0, 1], [2, 3], [4, 5]]),\n...     \"b\": torch.tensor([2, 1, 0, -1, -2]),\n... }\n&gt;&gt;&gt; out = abs(data)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3], [2, 1], [0, 1], [2, 3], [4, 5]]), 'b': tensor([2, 1, 0, 1, 2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_batch","title":"batchtensor.nested.amax_along_batch","text":"<pre><code>amax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = amax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([8, 9]), 'b': tensor(4)}\n&gt;&gt;&gt; out = amax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_seq","title":"batchtensor.nested.amax_along_seq","text":"<pre><code>amax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = amax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 9]), 'b': tensor([4])}\n&gt;&gt;&gt; out = amax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [9]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_batch","title":"batchtensor.nested.amin_along_batch","text":"<pre><code>amin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = amin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 1]), 'b': tensor(0)}\n&gt;&gt;&gt; out = amin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_seq","title":"batchtensor.nested.amin_along_seq","text":"<pre><code>amin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = amin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 5]), 'b': tensor([0])}\n&gt;&gt;&gt; out = amin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [5]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmax_along_batch","title":"batchtensor.nested.argmax_along_batch","text":"<pre><code>argmax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = argmax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 4]), 'b': tensor(0)}\n&gt;&gt;&gt; out = argmax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 4]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmax_along_seq","title":"batchtensor.nested.argmax_along_seq","text":"<pre><code>argmax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argmax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 4]), 'b': tensor([0])}\n&gt;&gt;&gt; out = argmax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [4]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmin_along_batch","title":"batchtensor.nested.argmin_along_batch","text":"<pre><code>argmin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmin_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = argmin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 0]), 'b': tensor(4)}\n&gt;&gt;&gt; out = argmin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 0]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmin_along_seq","title":"batchtensor.nested.argmin_along_seq","text":"<pre><code>argmin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmin_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argmin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 0]), 'b': tensor([4])}\n&gt;&gt;&gt; out = argmin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [0]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_batch","title":"batchtensor.nested.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the batch dimension</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argsort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]), 'b': tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = argsort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_seq","title":"batchtensor.nested.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort each tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_seq\n&gt;&gt;&gt; data = {'a': torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argsort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]), 'b': tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argsort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_batch","title":"batchtensor.nested.cat_along_batch","text":"<pre><code>cat_along_batch(\n    data: Sequence[dict[Hashable, Tensor]]\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_batch\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7, 8, 9]]), \"b\": torch.tensor([[17, 18, 19]])},\n... ]\n&gt;&gt;&gt; out = cat_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [4, 5, 6], [7, 8, 9]]),\n 'b': tensor([[10, 11, 12], [13, 14, 15], [17, 18, 19]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_seq","title":"batchtensor.nested.cat_along_seq","text":"<pre><code>cat_along_seq(\n    data: Sequence[dict[Hashable, Tensor]]\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_seq\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7], [8]]), \"b\": torch.tensor([[17], [18]])},\n... ]\n&gt;&gt;&gt; out = cat_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 7], [4, 5, 6, 8]]),\n 'b': tensor([[10, 11, 12, 17], [13, 14, 15, 18]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_batch","title":"batchtensor.nested.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; outputs = chunk_along_batch(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_seq","title":"batchtensor.nested.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; outputs = chunk_along_seq(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.clamp","title":"batchtensor.nested.clamp","text":"<pre><code>clamp(\n    data: Any,\n    min: float | None = None,\n    max: float | None = None,\n) -&gt; Any\n</code></pre> <p>Clamp all elements in input into the range <code>[min, max]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>min</code> <code>float | None</code> <p>The lower-bound of the range to be clamped to.</p> <code>None</code> <code>max</code> <code>float | None</code> <p>The upper-bound of the range to be clamped to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The clamp value of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import clamp\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = clamp(data, min=1, max=5)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 2], [3, 4], [5, 5], [5, 5], [5, 5]]), 'b': tensor([5, 4, 3, 2, 1])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumprod_along_batch","title":"batchtensor.nested.cumprod_along_batch","text":"<pre><code>cumprod_along_batch(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative product of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative product of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumprod_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = cumprod_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[   1,    2], [   3,    8], [  15,   48], [ 105,  384], [ 945, 3840]]), 'b': tensor([ 4, 12, 24, 24,  0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumprod_along_seq","title":"batchtensor.nested.cumprod_along_seq","text":"<pre><code>cumprod_along_seq(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative product of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative product of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumprod_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = cumprod_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[    1,     2,     6,    24,   120], [    6,    42,   336,  3024, 30240]]), 'b': tensor([[ 4, 12, 24, 24,  0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumsum_along_batch","title":"batchtensor.nested.cumsum_along_batch","text":"<pre><code>cumsum_along_batch(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative sum of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative sum of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = cumsum_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[ 0,  1], [ 2,  4], [ 6,  9], [12, 16], [20, 25]]), 'b': tensor([ 4,  7,  9, 10, 10])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumsum_along_seq","title":"batchtensor.nested.cumsum_along_seq","text":"<pre><code>cumsum_along_seq(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative sum of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative sum of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = cumsum_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[ 0,  1,  3,  6, 10], [ 5, 11, 18, 26, 35]]), 'b': tensor([[ 4,  7,  9, 10, 10]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.exp","title":"batchtensor.nested.exp","text":"<pre><code>exp(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the exponential of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The exponential of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import exp\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = exp(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.exp2","title":"batchtensor.nested.exp2","text":"<pre><code>exp2(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the base two exponential of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The base two exponential of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import exp2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = exp2(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.expm1","title":"batchtensor.nested.expm1","text":"<pre><code>expm1(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the exponential of the elements minus 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The exponential of the elements minus 1. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import expm1\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = expm1(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_batch","title":"batchtensor.nested.index_select_along_batch","text":"<pre><code>index_select_along_batch(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_batch\n&gt;&gt;&gt; tensors = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [8, 9]]), 'b': tensor([2, 0])}\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9], [6, 7], [4, 5], [2, 3], [0, 1]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_seq","title":"batchtensor.nested.index_select_along_seq","text":"<pre><code>index_select_along_seq(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_seq\n&gt;&gt;&gt; tensors = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 4], [7, 9]]), 'b': tensor([[2, 0]])}\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3, 2, 1, 0], [9, 8, 7, 6, 5]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log","title":"batchtensor.nested.log","text":"<pre><code>log(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the natural logarithm of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The natural logarithm of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log10","title":"batchtensor.nested.log10","text":"<pre><code>log10(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the logarithm to the base 10 of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The with the logarithm to the base 10 of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log10\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log10(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log1p","title":"batchtensor.nested.log1p","text":"<pre><code>log1p(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the natural logarithm of <code>(1 + input)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The natural logarithm of <code>(1 + input)</code>. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log1p\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log1p(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log2","title":"batchtensor.nested.log2","text":"<pre><code>log2(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the logarithm to the base 2 of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The logarithm to the base 2 of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log2(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.max_along_batch","title":"batchtensor.nested.max_along_batch","text":"<pre><code>max_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the maximum values and  the second tensor, which must have dtype long, with their  indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import max_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = max_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4])),\n'b': torch.return_types.max(\nvalues=tensor(4),\nindices=tensor(0))}\n&gt;&gt;&gt; out = max_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]])),\n'b': torch.return_types.max(\nvalues=tensor([4]),\nindices=tensor([0]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.max_along_seq","title":"batchtensor.nested.max_along_seq","text":"<pre><code>max_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import max_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = max_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4])),\n'b': torch.return_types.max(\nvalues=tensor([4]),\nindices=tensor([0]))}\n&gt;&gt;&gt; out = max_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]])),\n'b': torch.return_types.max(\nvalues=tensor([[4]]),\nindices=tensor([[0]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.mean_along_batch","title":"batchtensor.nested.mean_along_batch","text":"<pre><code>mean_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The mean of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import mean_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10, dtype=torch.float).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0], dtype=torch.float)}\n&gt;&gt;&gt; out = mean_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([4., 5.]), 'b': tensor(2.)}\n&gt;&gt;&gt; out = mean_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4., 5.]]), 'b': tensor([2.])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.mean_along_seq","title":"batchtensor.nested.mean_along_seq","text":"<pre><code>mean_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import mean_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10, dtype=torch.float).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]], dtype=torch.float)}\n&gt;&gt;&gt; out = mean_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([2., 7.]), 'b': tensor([2.])}\n&gt;&gt;&gt; out = mean_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[2.], [7.]]), 'b': tensor([[2.]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.median_along_batch","title":"batchtensor.nested.median_along_batch","text":"<pre><code>median_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import median_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = median_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2])),\n'b': torch.return_types.median(\nvalues=tensor(2),\nindices=tensor(2))}\n&gt;&gt;&gt; out = median_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]])),\n'b': torch.return_types.median(\nvalues=tensor([2]),\nindices=tensor([2]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.median_along_seq","title":"batchtensor.nested.median_along_seq","text":"<pre><code>median_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import median_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = median_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2])),\n'b': torch.return_types.median(\nvalues=tensor([2]),\nindices=tensor([2]))}\n&gt;&gt;&gt; out = median_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]])),\n'b': torch.return_types.median(\nvalues=tensor([[2]]),\nindices=tensor([[2]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.min_along_batch","title":"batchtensor.nested.min_along_batch","text":"<pre><code>min_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import min_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = min_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0])),\n'b': torch.return_types.min(\nvalues=tensor(0),\nindices=tensor(4))}\n&gt;&gt;&gt; out = min_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]])),\n'b': torch.return_types.min(\nvalues=tensor([0]),\nindices=tensor([4]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.min_along_seq","title":"batchtensor.nested.min_along_seq","text":"<pre><code>min_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import min_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = min_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0])),\n'b': torch.return_types.min(\nvalues=tensor([0]),\nindices=tensor([4]))}\n&gt;&gt;&gt; out = min_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]])),\n'b': torch.return_types.min(\nvalues=tensor([[0]]),\nindices=tensor([[4]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_batch","title":"batchtensor.nested.permute_along_batch","text":"<pre><code>permute_along_batch(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the batch dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = permute_along_batch(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [2, 3], [6, 7], [0, 1], [8, 9]]), 'b': tensor([2, 3, 1, 4, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_seq","title":"batchtensor.nested.permute_along_seq","text":"<pre><code>permute_along_seq(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(2, 5), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = permute_along_seq(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 3, 0, 4], [7, 6, 8, 5, 9]]), 'b': tensor([[2, 3, 1, 4, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.prod_along_batch","title":"batchtensor.nested.prod_along_batch","text":"<pre><code>prod_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The product of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import prod_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([5, 4, 3, 2, 1])}\n&gt;&gt;&gt; out = prod_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([  0, 945]), 'b': tensor(120)}\n&gt;&gt;&gt; out = prod_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[  0, 945]]), 'b': tensor([120])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.prod_along_seq","title":"batchtensor.nested.prod_along_seq","text":"<pre><code>prod_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The product of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import prod_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[5, 4, 3, 2, 1]])}\n&gt;&gt;&gt; out = prod_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([    0, 15120]), 'b': tensor([120])}\n&gt;&gt;&gt; out = prod_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[    0], [15120]]), 'b': tensor([[120]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.repeat_along_seq","title":"batchtensor.nested.repeat_along_seq","text":"<pre><code>repeat_along_seq(data: Any, repeats: int) -&gt; Any\n</code></pre> <p>Repeat all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>repeats</code> <code>int</code> <p>Specifies the number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The tensors repeated along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import repeat_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = repeat_along_seq(data, 2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4], [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]]),\n 'b': tensor([[4, 3, 2, 1, 0, 4, 3, 2, 1, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_batch","title":"batchtensor.nested.select_along_batch","text":"<pre><code>select_along_batch(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = select_along_batch(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([4, 5]), 'b': tensor(2)}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_seq","title":"batchtensor.nested.select_along_seq","text":"<pre><code>select_along_seq(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = select_along_seq(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([2, 7]), 'b': tensor([2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_batch","title":"batchtensor.nested.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    data: dict[Hashable, Tensor],\n    generator: Generator | None = None,\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = shuffle_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_seq","title":"batchtensor.nested.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    data: dict[Hashable, Tensor],\n    generator: Generator | None = None,\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(2, 5), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = shuffle_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([[...]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_batch","title":"batchtensor.nested.slice_along_batch","text":"<pre><code>slice_along_batch(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = slice_along_batch(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [6, 7], [8, 9]]), 'b': tensor([2, 1, 0])}\n&gt;&gt;&gt; out = slice_along_batch(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [2, 3], [4, 5]]), 'b': tensor([4, 3, 2])}\n&gt;&gt;&gt; out = slice_along_batch(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [4, 5], [8, 9]]), 'b': tensor([4, 2, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_seq","title":"batchtensor.nested.slice_along_seq","text":"<pre><code>slice_along_seq(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = slice_along_seq(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 3, 4], [7, 8, 9]]), 'b': tensor([[2, 1, 0]])}\n&gt;&gt;&gt; out = slice_along_seq(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [5, 6, 7]]), 'b': tensor([[4, 3, 2]])}\n&gt;&gt;&gt; out = slice_along_seq(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 2, 4], [5, 7, 9]]), 'b': tensor([[4, 2, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sort_along_batch","title":"batchtensor.nested.sort_along_batch","text":"<pre><code>sort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Sort the elements of the input tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A similar object where each tensor is replaced by a namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = sort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]]),\nindices=tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])),\n'b': torch.return_types.sort(\nvalues=tensor([0, 1, 2, 3, 4]),\nindices=tensor([4, 3, 2, 1, 0]))}\n&gt;&gt;&gt; out = sort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[8, 9], [5, 7], [4, 6], [2, 3], [0, 1]]),\nindices=tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])),\n'b': torch.return_types.sort(\nvalues=tensor([4, 3, 2, 1, 0]),\nindices=tensor([0, 1, 2, 3, 4]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sort_along_seq","title":"batchtensor.nested.sort_along_seq","text":"<pre><code>sort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Sort the elements of the input tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A similar object where each tensor is replaced by a namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_seq\n&gt;&gt;&gt; data = {'a': torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = sort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[0, 3, 5, 7, 8], [1, 2, 4, 6, 9]]),\nindices=tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]])),\n'b': torch.return_types.sort(\nvalues=tensor([[0, 1, 2, 3, 4]]),\nindices=tensor([[4, 3, 2, 1, 0]]))}\n&gt;&gt;&gt; out = sort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[8, 7, 5, 3, 0], [9, 6, 4, 2, 1]]),\nindices=tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]])),\n'b': torch.return_types.sort(\nvalues=tensor([[4, 3, 2, 1, 0]]),\nindices=tensor([[0, 1, 2, 3, 4]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_batch","title":"batchtensor.nested.split_along_batch","text":"<pre><code>split_along_batch(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; outputs = split_along_batch(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_seq","title":"batchtensor.nested.split_along_seq","text":"<pre><code>split_along_seq(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(2, 5), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; outputs = split_along_seq(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sum_along_batch","title":"batchtensor.nested.sum_along_batch","text":"<pre><code>sum_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sum_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = sum_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([20, 25]), 'b': tensor(10)}\n&gt;&gt;&gt; out = sum_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[20, 25]]), 'b': tensor([10])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sum_along_seq","title":"batchtensor.nested.sum_along_seq","text":"<pre><code>sum_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sum_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = sum_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([10, 35]), 'b': tensor([10])}\n&gt;&gt;&gt; out = sum_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[10], [35]]), 'b': tensor([[10]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.to","title":"batchtensor.nested.to","text":"<pre><code>to(data: Any, *args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Perform Tensor dtype and/or device conversion.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>args</code> <code>Any</code> <p>Positional arguments for <code>torch.Tensor.to</code>.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments for <code>torch.Tensor.to</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data after conversion.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = to(data, dtype=torch.float)\n&gt;&gt;&gt; out\n{'a': tensor([[0., 1.], [2., 3.], [4., 5.], [6., 7.], [8., 9.]]),\n 'b': tensor([4., 3., 2., 1., 0.])}\n</code></pre>"},{"location":"refs/tensor/","title":"Tensor","text":""},{"location":"refs/tensor/#batchtensor.tensor","title":"batchtensor.tensor","text":"<p>Contain functions to manipulate tensors.</p>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_batch","title":"batchtensor.tensor.amax_along_batch","text":"<pre><code>amax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = amax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([8, 9])\n&gt;&gt;&gt; out = amax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_seq","title":"batchtensor.tensor.amax_along_seq","text":"<pre><code>amax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = amax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 9])\n&gt;&gt;&gt; out = amax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_batch","title":"batchtensor.tensor.amin_along_batch","text":"<pre><code>amin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = amin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 1])\n&gt;&gt;&gt; out = amin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_seq","title":"batchtensor.tensor.amin_along_seq","text":"<pre><code>amin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = amin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 5])\n&gt;&gt;&gt; out = amin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_batch","title":"batchtensor.tensor.argmax_along_batch","text":"<pre><code>argmax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = argmax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4, 4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_seq","title":"batchtensor.tensor.argmax_along_seq","text":"<pre><code>argmax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = argmax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_batch","title":"batchtensor.tensor.argmin_along_batch","text":"<pre><code>argmin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = argmin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_seq","title":"batchtensor.tensor.argmin_along_seq","text":"<pre><code>argmin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = argmin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_batch","title":"batchtensor.tensor.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the batch dimension</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; out = argsort_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])\n&gt;&gt;&gt; out = argsort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_seq","title":"batchtensor.tensor.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[2, 1, 4, 0, 3],\n        [0, 4, 3, 2, 1]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 0, 4, 1, 2],\n        [1, 2, 3, 4, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_batch","title":"batchtensor.tensor.cat_along_batch","text":"<pre><code>cat_along_batch(\n    tensors: list[Tensor] | tuple[Tensor, ...]\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>Specifies the batches to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_batch\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11, 12], [13, 14, 15]]),\n... ]\n&gt;&gt;&gt; out = cat_along_batch(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2],\n        [ 4,  5,  6],\n        [10, 11, 12],\n        [13, 14, 15]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_seq","title":"batchtensor.tensor.cat_along_seq","text":"<pre><code>cat_along_seq(\n    tensors: list[Tensor] | tuple[Tensor, ...]\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>Specifies the batches to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_seq\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11], [12, 13]]),\n... ]\n&gt;&gt;&gt; out = cat_along_seq(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2, 10, 11],\n        [ 4,  5,  6, 12, 13]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_batch","title":"batchtensor.tensor.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; outputs = chunk_along_batch(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_seq","title":"batchtensor.tensor.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; outputs = chunk_along_seq(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumprod_along_batch","title":"batchtensor.tensor.cumprod_along_batch","text":"<pre><code>cumprod_along_batch(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative product of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative product of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumprod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n&gt;&gt;&gt; out = cumprod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[   1,    2], [   3,    8], [  15,   48], [ 105,  384], [ 945, 3840]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumprod_along_seq","title":"batchtensor.tensor.cumprod_along_seq","text":"<pre><code>cumprod_along_seq(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative product of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative product of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumprod_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; out = cumprod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[    1,     2,     6,    24,   120],\n        [    6,    42,   336,  3024, 30240]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumsum_along_batch","title":"batchtensor.tensor.cumsum_along_batch","text":"<pre><code>cumsum_along_batch(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative sum of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative sum of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = cumsum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[ 0,  1], [ 2,  4], [ 6,  9], [12, 16], [20, 25]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumsum_along_seq","title":"batchtensor.tensor.cumsum_along_seq","text":"<pre><code>cumsum_along_seq(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative sum of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative sum of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = cumsum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  3,  6, 10],\n        [ 5, 11, 18, 26, 35]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_batch","title":"batchtensor.tensor.index_select_along_batch","text":"<pre><code>index_select_along_batch(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [8, 9]])\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[8, 9],\n        [6, 7],\n        [4, 5],\n        [2, 3],\n        [0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_seq","title":"batchtensor.tensor.index_select_along_seq","text":"<pre><code>index_select_along_seq(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 4],\n        [7, 9]])\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[4, 3, 2, 1, 0],\n        [9, 8, 7, 6, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_batch","title":"batchtensor.tensor.max_along_batch","text":"<pre><code>max_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = max_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_seq","title":"batchtensor.tensor.max_along_seq","text":"<pre><code>max_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = max_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_batch","title":"batchtensor.tensor.mean_along_batch","text":"<pre><code>mean_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10, dtype=torch.float).view(5, 2)\n&gt;&gt;&gt; out = mean_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4., 5.])\n&gt;&gt;&gt; out = mean_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4., 5.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_seq","title":"batchtensor.tensor.mean_along_seq","text":"<pre><code>mean_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10, dtype=torch.float).view(2, 5)\n&gt;&gt;&gt; out = mean_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([2., 7.])\n&gt;&gt;&gt; out = mean_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[2.], [7.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_batch","title":"batchtensor.tensor.median_along_batch","text":"<pre><code>median_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = median_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_seq","title":"batchtensor.tensor.median_along_seq","text":"<pre><code>median_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = median_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_batch","title":"batchtensor.tensor.min_along_batch","text":"<pre><code>min_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = min_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_seq","title":"batchtensor.tensor.min_along_seq","text":"<pre><code>min_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = min_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_batch","title":"batchtensor.tensor.permute_along_batch","text":"<pre><code>permute_along_batch(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the batch dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = permute_along_batch(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [2, 3],\n        [6, 7],\n        [0, 1],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_seq","title":"batchtensor.tensor.permute_along_seq","text":"<pre><code>permute_along_seq(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the sequence dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = permute_along_seq(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 1, 3, 0, 4],\n        [7, 6, 8, 5, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_batch","title":"batchtensor.tensor.prod_along_batch","text":"<pre><code>prod_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = prod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([  0, 945])\n&gt;&gt;&gt; out = prod_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[  0, 945]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_seq","title":"batchtensor.tensor.prod_along_seq","text":"<pre><code>prod_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = prod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([    0, 15120])\n&gt;&gt;&gt; out = prod_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[    0], [15120]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.repeat_along_seq","title":"batchtensor.tensor.repeat_along_seq","text":"<pre><code>repeat_along_seq(tensor: Tensor, repeats: int) -&gt; Tensor\n</code></pre> <p>Repeat the data along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>repeats</code> <code>int</code> <p>Specifies the number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A new tensor with the data repeated along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import repeat_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = repeat_along_seq(tensor, 2)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_batch","title":"batchtensor.tensor.select_along_batch","text":"<pre><code>select_along_batch(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = select_along_batch(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([4, 5])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_seq","title":"batchtensor.tensor.select_along_seq","text":"<pre><code>select_along_seq(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = select_along_seq(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([2, 7])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_batch","title":"batchtensor.tensor.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = shuffle_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_seq","title":"batchtensor.tensor.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = shuffle_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_batch","title":"batchtensor.tensor.slice_along_batch","text":"<pre><code>slice_along_batch(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = slice_along_batch(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [6, 7],\n        [8, 9]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [4, 5],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_seq","title":"batchtensor.tensor.slice_along_seq","text":"<pre><code>slice_along_seq(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [9, 8, 7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[2, 3, 4],\n        [7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2],\n        [9, 8, 7]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 2, 4],\n        [9, 7, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sort_along_batch","title":"batchtensor.tensor.sort_along_batch","text":"<pre><code>sort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; sort\n</code></pre> <p>Sort the elements of the input tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sort</code> <p>A namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; out = sort_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]]),\nindices=tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]))\n&gt;&gt;&gt; out = sort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[8, 9], [5, 7], [4, 6], [2, 3], [0, 1]]),\nindices=tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sort_along_seq","title":"batchtensor.tensor.sort_along_seq","text":"<pre><code>sort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; sort\n</code></pre> <p>Sort the elements of the input tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sort</code> <p>A namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = sort_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[0, 3, 5, 7, 8], [1, 2, 4, 6, 9]]),\nindices=tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]))\n&gt;&gt;&gt; out = sort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[8, 7, 5, 3, 0], [9, 6, 4, 2, 1]]),\nindices=tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_batch","title":"batchtensor.tensor.split_along_batch","text":"<pre><code>split_along_batch(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; outputs = split_along_batch(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_seq","title":"batchtensor.tensor.split_along_seq","text":"<pre><code>split_along_seq(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; outputs = split_along_seq(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_batch","title":"batchtensor.tensor.sum_along_batch","text":"<pre><code>sum_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = sum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([20, 25])\n&gt;&gt;&gt; out = sum_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[20, 25]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_seq","title":"batchtensor.tensor.sum_along_seq","text":"<pre><code>sum_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = sum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([10, 35])\n&gt;&gt;&gt; out = sum_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[10], [35]])\n</code></pre>"},{"location":"uguide/tensor/","title":"Tensor","text":""},{"location":"uguide/tensor/#batch","title":"Batch","text":"<p><code>batchtensor</code> provides functions to manipulate tensors representing a batch of examples. The functions assume the tensors have the following shape: <code>batch_size, *</code> where <code>*</code> means any dimensions.</p>"},{"location":"uguide/tensor/#sequence","title":"Sequence","text":"<p><code>batchtensor</code> provides functions to manipulate tensors representing a batch of sequences. The functions assume the tensors have the following shape: <code>batch_size, seq_len, *</code> where <code>*</code> means any dimensions.</p>"}]}