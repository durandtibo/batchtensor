{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p><code>batchtensor</code> is a lightweight library built on top of PyTorch to manipulate nested data structures with PyTorch tensors. This library provides functions for tensors where the first dimension is the batch dimension. It also provides functions for tensors representing a batch of sequences where the first dimension is the batch dimension and the second dimension is the sequence dimension.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Let's imagine you have a batch which is represented by a dictionary with three tensors, and you want to take the first 2 items. <code>batchtensor</code> provides the function <code>slice_along_batch</code> that allows slicing all the tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n...     \"c\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n... }\n&gt;&gt;&gt; slice_along_batch(batch, stop=2)\n{'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3]), 'c': tensor([1., 2.])}\n</code></pre> <p>Similarly, it is possible to split a batch into multiple batches by using the function <code>split_along_batch</code>:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n...     \"c\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n... }\n&gt;&gt;&gt; split_along_batch(batch, split_size_or_sections=2)\n({'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3]), 'c': tensor([1., 2.])},\n {'a': tensor([[4, 9], [8, 1]]), 'b': tensor([2, 1]), 'c': tensor([3., 4.])},\n {'a': tensor([[5, 7]]), 'b': tensor([0]), 'c': tensor([5.])})\n</code></pre> <p>Please check the documentation to see all the implemented functions.</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>batchtensor</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>batchtensor</code> to a new version will possibly break any code that was using the old version of <code>batchtensor</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>batchtensor</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"examples/","title":"Examples and Tutorials","text":"<p>This page provides practical examples of using <code>batchtensor</code> in real-world scenarios.</p>"},{"location":"examples/#example-1-processing-image-batches","title":"Example 1: Processing Image Batches","text":"<p>Working with batches of images and their metadata:</p> <pre><code>import torch\nfrom batchtensor.nested import (\n    slice_along_batch,\n    shuffle_along_batch,\n    split_along_batch,\n)\n\n# Create a batch of images with metadata\nbatch = {\n    \"images\": torch.randn(100, 3, 224, 224),  # 100 RGB images\n    \"labels\": torch.randint(0, 10, (100,)),    # Classification labels\n    \"filenames\": [\"img_{:03d}.jpg\".format(i) for i in range(100)],\n    \"augmented\": torch.randint(0, 2, (100,), dtype=torch.bool),\n}\n\n# Shuffle the entire batch while maintaining correspondence\nshuffled = shuffle_along_batch(batch)\n\n# Take the first 32 samples for a mini-batch\nmini_batch = slice_along_batch(batch, stop=32)\n\n# Split into train/val/test sets (60/20/20)\ntrain, val, test = split_along_batch(batch, split_size_or_sections=[60, 20, 20])\n\nprint(f\"Train set: {train['images'].shape[0]} images\")\nprint(f\"Val set: {val['images'].shape[0]} images\")\nprint(f\"Test set: {test['images'].shape[0]} images\")\n</code></pre>"},{"location":"examples/#example-2-sequence-data-with-padding","title":"Example 2: Sequence Data with Padding","text":"<p>Handling sequences of variable length:</p> <pre><code>import torch\nfrom batchtensor.nested import (\n    cat_along_seq,\n    slice_along_seq,\n    select_along_seq,\n)\n\n# Batch of sequences with padding\nbatch = {\n    \"tokens\": torch.tensor([\n        [[1], [2], [3], [0]],  # sequence length 3, padded\n        [[4], [5], [0], [0]],  # sequence length 2, padded\n        [[6], [7], [8], [9]],  # sequence length 4, no padding\n    ]),\n    \"lengths\": torch.tensor([3, 2, 4]),  # actual lengths\n}\n\n# Take first 3 timesteps\ntruncated = slice_along_seq(batch, stop=3)\nprint(f\"Truncated shape: {truncated['tokens'].shape}\")\n\n# Select specific timesteps\nfirst_and_last = select_along_seq(batch, torch.tensor([0, 3]))\nprint(f\"Selected shape: {first_and_last['tokens'].shape}\")\n</code></pre>"},{"location":"examples/#example-3-multi-task-learning","title":"Example 3: Multi-Task Learning","text":"<p>Managing data for multiple tasks:</p> <pre><code>import torch\nfrom batchtensor.nested import (\n    to_device,\n    to_dtype,\n    index_select_along_batch,\n)\n\n# Multi-task batch\nbatch = {\n    \"shared_features\": torch.randn(64, 512),\n    \"task1\": {\n        \"labels\": torch.randint(0, 10, (64,)),\n        \"weights\": torch.rand(64),\n    },\n    \"task2\": {\n        \"labels\": torch.randint(0, 2, (64,)),\n        \"weights\": torch.rand(64),\n    },\n}\n\n# Move everything to GPU (if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_gpu = to_device(batch, device)\n\n# Convert all weights to float16 for mixed precision\nbatch_gpu[\"task1\"] = to_dtype(batch_gpu[\"task1\"], dtype=torch.float16)\nbatch_gpu[\"task2\"] = to_dtype(batch_gpu[\"task2\"], dtype=torch.float16)\n\n# Sample a subset for task-specific training\ntask1_indices = torch.randperm(64)[:32]\ntask1_batch = index_select_along_batch(batch_gpu, task1_indices)\n</code></pre>"},{"location":"examples/#example-4-data-augmentation-pipeline","title":"Example 4: Data Augmentation Pipeline","text":"<p>Creating augmented views of data:</p> <pre><code>import torch\nfrom batchtensor.nested import cat_along_batch\nfrom batchtensor.recursive import recursive_apply\n\ndef augment_image(image):\n    \"\"\"Simple augmentation: random crop and flip.\"\"\"\n    # Random horizontal flip\n    if torch.rand(1) &gt; 0.5:\n        image = torch.flip(image, dims=[-1])\n    # Random crop (simplified)\n    return image\n\n# Original batch\noriginal = {\n    \"images\": torch.randn(32, 3, 224, 224),\n    \"labels\": torch.randint(0, 1000, (32,)),\n}\n\n# Create augmented version\naugmented = {\n    \"images\": recursive_apply(\n        original[\"images\"],\n        lambda x: augment_image(x) if isinstance(x, torch.Tensor) else x\n    ),\n    \"labels\": original[\"labels\"],  # Labels stay the same\n}\n\n# Combine original and augmented\ncombined = cat_along_batch([original, augmented])\nprint(f\"Combined batch size: {combined['images'].shape[0]}\")\n</code></pre>"},{"location":"examples/#example-5-temporal-data-processing","title":"Example 5: Temporal Data Processing","text":"<p>Working with time series data:</p> <pre><code>import torch\nfrom batchtensor.nested import (\n    split_along_seq,\n    chunk_along_seq,\n    repeat_along_seq,\n)\n\n# Time series batch (batch_size=16, seq_len=100, features=10)\nbatch = {\n    \"sequences\": torch.randn(16, 100, 10),\n    \"timestamps\": torch.arange(100).unsqueeze(0).repeat(16, 1),\n}\n\n# Split into past and future\npast, future = split_along_seq(batch, split_size_or_sections=[80, 20])\nprint(f\"Past: {past['sequences'].shape}, Future: {future['sequences'].shape}\")\n\n# Create overlapping windows\nwindows = chunk_along_seq(batch, chunks=10)\nprint(f\"Number of windows: {len(windows)}\")\n\n# Repeat sequences for ensemble predictions\nensemble_batch = repeat_along_seq(batch, 5)\nprint(f\"Ensemble shape: {ensemble_batch['sequences'].shape}\")\n</code></pre>"},{"location":"examples/#example-6-batch-normalization","title":"Example 6: Batch Normalization","text":"<p>Computing statistics across the batch:</p> <pre><code>import torch\nfrom batchtensor.nested import (\n    mean_along_batch,\n    std_along_batch,\n)\nfrom batchtensor.recursive import recursive_apply\n\n# Batch with different feature types\nbatch = {\n    \"continuous\": torch.randn(256, 10),\n    \"embeddings\": torch.randn(256, 50),\n}\n\n# Compute mean and std for normalization\nmeans = mean_along_batch(batch)\nstds = std_along_batch(batch)\n\n# Normalize\ndef normalize_tensor(x, mean, std):\n    return (x - mean) / (std + 1e-8)\n\nnormalized = {}\nfor key in batch:\n    normalized[key] = normalize_tensor(batch[key], means[key], stds[key])\n\nprint(\"Normalized batch mean (should be ~0):\")\nprint(mean_along_batch(normalized))\n</code></pre>"},{"location":"examples/#example-7-debugging-with-utils","title":"Example 7: Debugging with Utils","text":"<p>Inspecting complex nested structures:</p> <pre><code>import torch\nfrom batchtensor.utils import bfs_tensor, dfs_tensor\n\n# Complex nested batch\nbatch = {\n    \"vision\": {\n        \"rgb\": torch.randn(8, 3, 64, 64),\n        \"depth\": torch.randn(8, 1, 64, 64),\n    },\n    \"text\": {\n        \"tokens\": torch.randint(0, 1000, (8, 50)),\n        \"attention_mask\": torch.ones(8, 50),\n    },\n    \"metadata\": {\n        \"ids\": torch.arange(8),\n        \"timestamps\": torch.randn(8),\n    },\n}\n\nprint(\"=\" * 60)\nprint(\"Batch Structure (BFS):\")\nprint(\"=\" * 60)\nfor path, tensor in bfs_tensor(batch):\n    path_str = \".\".join(path)\n    memory_mb = tensor.element_size() * tensor.numel() / (1024 ** 2)\n    print(f\"{path_str:30s} {str(tensor.shape):20s} {memory_mb:8.2f} MB\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Total memory usage:\")\nprint(\"=\" * 60)\ntotal_memory = sum(\n    tensor.element_size() * tensor.numel()\n    for _, tensor in bfs_tensor(batch)\n)\nprint(f\"{total_memory / (1024 ** 2):.2f} MB\")\n</code></pre>"},{"location":"examples/#example-8-custom-recursive-operations","title":"Example 8: Custom Recursive Operations","text":"<p>Creating custom operations with the recursive module:</p> <pre><code>import torch\nfrom batchtensor.recursive import recursive_apply\n\ndef clip_gradients(data, max_norm=1.0):\n    \"\"\"Clip gradients in nested structures.\"\"\"\n    def clip_fn(x):\n        if isinstance(x, torch.Tensor) and x.requires_grad and x.grad is not None:\n            torch.nn.utils.clip_grad_norm_([x], max_norm)\n        return x\n\n    return recursive_apply(data, clip_fn)\n\n# Model parameters as nested dict\nparams = {\n    \"encoder\": {\n        \"weights\": torch.randn(100, 50, requires_grad=True),\n        \"bias\": torch.randn(100, requires_grad=True),\n    },\n    \"decoder\": {\n        \"weights\": torch.randn(50, 10, requires_grad=True),\n        \"bias\": torch.randn(10, requires_grad=True),\n    },\n}\n\n# Simulate some gradients\nfor _, tensor in bfs_tensor(params):\n    if tensor.requires_grad:\n        tensor.grad = torch.randn_like(tensor) * 10  # Large gradients\n\n# Clip all gradients\nparams = clip_gradients(params, max_norm=1.0)\n</code></pre>"},{"location":"examples/#example-9-data-loading-pipeline","title":"Example 9: Data Loading Pipeline","text":"<p>Integrating with PyTorch DataLoader:</p> <pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom batchtensor.nested import cat_along_batch\n\nclass NestedDataset(Dataset):\n    def __init__(self, size=1000):\n        self.size = size\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return {\n            \"input\": torch.randn(10),\n            \"target\": torch.randint(0, 2, (1,)).item(),\n            \"metadata\": {\n                \"id\": idx,\n                \"weight\": torch.rand(1).item(),\n            },\n        }\n\n# Custom collate function\ndef nested_collate(batch):\n    \"\"\"Collate nested dictionaries.\"\"\"\n    # Convert list of dicts to dict of lists\n    keys = batch[0].keys()\n    collated = {}\n\n    for key in keys:\n        values = [item[key] for item in batch]\n        if isinstance(values[0], dict):\n            # Recursively collate nested dicts\n            collated[key] = nested_collate(values)\n        elif isinstance(values[0], torch.Tensor):\n            collated[key] = torch.stack(values)\n        else:\n            collated[key] = torch.tensor(values)\n\n    return collated\n\n# Create dataloader\ndataset = NestedDataset(size=100)\ndataloader = DataLoader(\n    dataset,\n    batch_size=16,\n    collate_fn=nested_collate,\n    shuffle=True,\n)\n\n# Use in training loop\nfor batch in dataloader:\n    print(f\"Batch input shape: {batch['input'].shape}\")\n    print(f\"Batch target shape: {batch['target'].shape}\")\n    print(f\"Batch metadata IDs: {batch['metadata']['id'].shape}\")\n    break  # Just show first batch\n</code></pre>"},{"location":"examples/#example-10-validation-and-testing","title":"Example 10: Validation and Testing","text":"<p>Batch processing for model evaluation:</p> <pre><code>import torch\nfrom batchtensor.nested import (\n    chunk_along_batch,\n    cat_along_batch,\n    argmax_along_batch,\n)\n\ndef evaluate_model(model, data, batch_size=32):\n    \"\"\"Evaluate model on nested data in chunks.\"\"\"\n    # Split large dataset into batches\n    batches = chunk_along_batch(data, chunks=len(data[\"features\"]) // batch_size)\n\n    predictions = []\n\n    for batch in batches:\n        # Forward pass (simplified)\n        with torch.no_grad():\n            logits = {\n                \"class_logits\": torch.randn(batch[\"features\"].shape[0], 10),\n            }\n        predictions.append(logits)\n\n    # Combine all predictions\n    all_predictions = cat_along_batch(predictions)\n\n    # Get predicted classes\n    predicted_classes = argmax_along_batch(all_predictions)\n\n    return predicted_classes\n\n# Large test dataset\ntest_data = {\n    \"features\": torch.randn(1000, 128),\n    \"labels\": torch.randint(0, 10, (1000,)),\n}\n\n# Evaluate in batches\npredictions = evaluate_model(None, test_data, batch_size=32)\nprint(f\"Predictions shape: {predictions['class_logits'].shape}\")\n</code></pre>"},{"location":"examples/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Maintain Structure Consistency: Keep the same nested structure throughout your pipeline</li> <li>Use Batch Operations: Leverage batch operations instead of loops when possible</li> <li>Memory Management: Move data to GPU only when needed, use appropriate dtypes</li> <li>Validation: Use utils functions to validate batch dimensions and structure</li> <li>Error Handling: Wrap operations in try-except blocks for production code</li> <li>Documentation: Document the expected structure of your nested batches</li> <li>Testing: Test with small batches first before scaling up</li> </ol>"},{"location":"examples/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Installation and setup</li> <li>Nested Operations Guide - Detailed nested operations</li> <li>Recursive Module Guide - Custom recursive operations</li> <li>Utils Guide - Debugging and inspection utilities</li> </ul>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install batchtensor\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>batchtensor</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'batchtensor[all]'\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>batchtensor</code> from source, you can follow the steps below.</p>"},{"location":"get_started/#prerequisites","title":"Prerequisites","text":"<p>This project uses uv for dependency management. First, install <code>uv</code>:</p> <pre><code>pip install uv\n</code></pre>"},{"location":"get_started/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone git@github.com:durandtibo/batchtensor.git\ncd batchtensor\n</code></pre>"},{"location":"get_started/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>It is recommended to create a Python 3.10+ virtual environment. You can use <code>conda</code>:</p> <pre><code>make conda\nconda activate batchtensor\n</code></pre> <p>Or create a virtual environment with <code>uv</code>:</p> <pre><code>uv venv\nsource .venv/bin/activate  # On Unix/macOS\n# or\n.venv\\Scripts\\activate  # On Windows\n</code></pre>"},{"location":"get_started/#install-dependencies","title":"Install dependencies","text":"<p>Install all dependencies using <code>uv</code>:</p> <pre><code>uv sync --frozen\n</code></pre> <p>Or use the Makefile:</p> <pre><code>make install\n</code></pre> <p>To install with documentation dependencies:</p> <pre><code>make install-all\n</code></pre>"},{"location":"get_started/#verify-the-installation","title":"Verify the installation","text":"<p>Run the test suite to verify everything is working:</p> <pre><code>make unit-test-cov\n</code></pre> <p>Or use <code>uv</code> directly:</p> <pre><code>uv run pytest tests/\n</code></pre>"},{"location":"refs/nested/","title":"Nested Module","text":""},{"location":"refs/nested/#batchtensor.nested","title":"batchtensor.nested","text":"<p>Contain functions to manipulate nested data.</p>"},{"location":"refs/nested/#batchtensor.nested.abs","title":"batchtensor.nested.abs","text":"<pre><code>abs(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the absolute value of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The absolute value of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import abs\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[-4, -3], [-2, -1], [0, 1], [2, 3], [4, 5]]),\n...     \"b\": torch.tensor([2, 1, 0, -1, -2]),\n... }\n&gt;&gt;&gt; out = abs(data)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3], [2, 1], [0, 1], [2, 3], [4, 5]]), 'b': tensor([2, 1, 0, 1, 2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.acos","title":"batchtensor.nested.acos","text":"<pre><code>acos(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse cosine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import acos\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = acos(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.acosh","title":"batchtensor.nested.acosh","text":"<pre><code>acosh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse hyperbolic cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse hyperbolic cosine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import acosh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = acosh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_batch","title":"batchtensor.nested.amax_along_batch","text":"<pre><code>amax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = amax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([8, 9]), 'b': tensor(4)}\n&gt;&gt;&gt; out = amax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_seq","title":"batchtensor.nested.amax_along_seq","text":"<pre><code>amax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = amax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 9]), 'b': tensor([4])}\n&gt;&gt;&gt; out = amax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [9]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_batch","title":"batchtensor.nested.amin_along_batch","text":"<pre><code>amin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = amin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 1]), 'b': tensor(0)}\n&gt;&gt;&gt; out = amin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_seq","title":"batchtensor.nested.amin_along_seq","text":"<pre><code>amin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = amin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 5]), 'b': tensor([0])}\n&gt;&gt;&gt; out = amin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [5]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmax_along_batch","title":"batchtensor.nested.argmax_along_batch","text":"<pre><code>argmax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argmax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 4]), 'b': tensor(0)}\n&gt;&gt;&gt; out = argmax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 4]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmax_along_seq","title":"batchtensor.nested.argmax_along_seq","text":"<pre><code>argmax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = argmax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 4]), 'b': tensor([0])}\n&gt;&gt;&gt; out = argmax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [4]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmin_along_batch","title":"batchtensor.nested.argmin_along_batch","text":"<pre><code>argmin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmin_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argmin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 0]), 'b': tensor(4)}\n&gt;&gt;&gt; out = argmin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 0]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argmin_along_seq","title":"batchtensor.nested.argmin_along_seq","text":"<pre><code>argmin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmin_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = argmin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 0]), 'b': tensor([4])}\n&gt;&gt;&gt; out = argmin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [0]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_batch","title":"batchtensor.nested.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the batch dimension</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argsort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]), 'b': tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = argsort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_seq","title":"batchtensor.nested.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort each tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = argsort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]), 'b': tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argsort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.as_tensor","title":"batchtensor.nested.as_tensor","text":"<pre><code>as_tensor(\n    data: Any,\n    dtype: dtype | None = None,\n    device: device | None = None,\n) -&gt; Any\n</code></pre> <p>Create a new nested data structure with <code>torch.Tensor</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a <code>torch.Tensor</code> compatible value.</p> required <code>dtype</code> <code>dtype | None</code> <p>The desired data type of returned tensors. If <code>None</code>, it infers data type from data.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>The device of the constructed tensors. If <code>None</code> and data is a tensor then the device of data is used. If <code>None</code> and data is not a tensor then the result tensor is constructed on the current device.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A nested data structure with <code>torch.Tensor</code>s. The output data has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from batchtensor.nested import as_tensor\n&gt;&gt;&gt; data = {\"a\": np.ones((2, 5), dtype=np.float32), \"b\": np.arange(5), \"c\": 42}\n&gt;&gt;&gt; out = as_tensor(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]),\n 'b': tensor([0, 1, 2, 3, 4]),\n 'c': tensor(42)}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.asin","title":"batchtensor.nested.asin","text":"<pre><code>asin(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the arcsine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The arcsine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import asin\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = asin(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.asinh","title":"batchtensor.nested.asinh","text":"<pre><code>asinh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse hyperbolic sine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse hyperbolic sine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import asinh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = asinh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.atan","title":"batchtensor.nested.atan","text":"<pre><code>atan(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the arctangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The arctangent of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import atan\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = atan(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.atanh","title":"batchtensor.nested.atanh","text":"<pre><code>atanh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the inverse hyperbolic tangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse hyperbolic tangent of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import atanh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = atanh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_batch","title":"batchtensor.nested.cat_along_batch","text":"<pre><code>cat_along_batch(\n    data: Sequence[dict[Hashable, Tensor]],\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_batch\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7, 8, 9]]), \"b\": torch.tensor([[17, 18, 19]])},\n... ]\n&gt;&gt;&gt; out = cat_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [4, 5, 6], [7, 8, 9]]),\n 'b': tensor([[10, 11, 12], [13, 14, 15], [17, 18, 19]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_seq","title":"batchtensor.nested.cat_along_seq","text":"<pre><code>cat_along_seq(\n    data: Sequence[dict[Hashable, Tensor]],\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_seq\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7], [8]]), \"b\": torch.tensor([[17], [18]])},\n... ]\n&gt;&gt;&gt; out = cat_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 7], [4, 5, 6, 8]]),\n 'b': tensor([[10, 11, 12, 17], [13, 14, 15, 18]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_batch","title":"batchtensor.nested.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; outputs = chunk_along_batch(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_seq","title":"batchtensor.nested.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; outputs = chunk_along_seq(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.clamp","title":"batchtensor.nested.clamp","text":"<pre><code>clamp(\n    data: Any,\n    min: float | None = None,\n    max: float | None = None,\n) -&gt; Any\n</code></pre> <p>Clamp all elements in input into the range <code>[min, max]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>min</code> <code>float | None</code> <p>The lower-bound of the range to be clamped to.</p> <code>None</code> <code>max</code> <code>float | None</code> <p>The upper-bound of the range to be clamped to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The clamp value of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import clamp\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = clamp(data, min=1, max=5)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 2], [3, 4], [5, 5], [5, 5], [5, 5]]), 'b': tensor([5, 4, 3, 2, 1])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cos","title":"batchtensor.nested.cos","text":"<pre><code>cos(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cosine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cos\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = cos(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cosh","title":"batchtensor.nested.cosh","text":"<pre><code>cosh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the hyperbolic cosine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The inverse cosine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cosh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = cosh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumprod_along_batch","title":"batchtensor.nested.cumprod_along_batch","text":"<pre><code>cumprod_along_batch(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative product of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative product of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumprod_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = cumprod_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[   1,    2], [   3,    8], [  15,   48], [ 105,  384], [ 945, 3840]]), 'b': tensor([ 4, 12, 24, 24,  0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumprod_along_seq","title":"batchtensor.nested.cumprod_along_seq","text":"<pre><code>cumprod_along_seq(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative product of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative product of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumprod_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = cumprod_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[    1,     2,     6,    24,   120], [    6,    42,   336,  3024, 30240]]), 'b': tensor([[ 4, 12, 24, 24,  0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumsum_along_batch","title":"batchtensor.nested.cumsum_along_batch","text":"<pre><code>cumsum_along_batch(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative sum of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative sum of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = cumsum_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[ 0,  1], [ 2,  4], [ 6,  9], [12, 16], [20, 25]]), 'b': tensor([ 4,  7,  9, 10, 10])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cumsum_along_seq","title":"batchtensor.nested.cumsum_along_seq","text":"<pre><code>cumsum_along_seq(data: Any) -&gt; Any\n</code></pre> <p>Return the cumulative sum of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The cumulative sum of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = cumsum_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[ 0,  1,  3,  6, 10], [ 5, 11, 18, 26, 35]]), 'b': tensor([[ 4,  7,  9, 10, 10]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.exp","title":"batchtensor.nested.exp","text":"<pre><code>exp(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the exponential of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The exponential of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import exp\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = exp(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.exp2","title":"batchtensor.nested.exp2","text":"<pre><code>exp2(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the base two exponential of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The base two exponential of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import exp2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = exp2(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.expm1","title":"batchtensor.nested.expm1","text":"<pre><code>expm1(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the exponential of the elements minus 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The exponential of the elements minus 1. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import expm1\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = expm1(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.from_numpy","title":"batchtensor.nested.from_numpy","text":"<pre><code>from_numpy(data: Any) -&gt; Any\n</code></pre> <p>Create a new nested data structure where the <code>numpy.ndarray</code>s are converted to <code>torch.Tensor</code>s.</p> Note <p>The returned <code>torch.Tensor</code>s and <code>numpy.ndarray</code>s share the same memory. Modifications to the <code>torch.Tensor</code>s will be reflected in the <code>numpy.ndarray</code>s and vice versa.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a <code>numpy.ndarray</code>.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A nested data structure with <code>torch.Tensor</code>s instead of <code>numpy.ndarray</code>s. The output data has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from batchtensor.nested import from_numpy\n&gt;&gt;&gt; data = {\"a\": np.ones((2, 5), dtype=np.float32), \"b\": np.arange(5)}\n&gt;&gt;&gt; out = from_numpy(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_batch","title":"batchtensor.nested.index_select_along_batch","text":"<pre><code>index_select_along_batch(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_batch\n&gt;&gt;&gt; tensors = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [8, 9]]), 'b': tensor([2, 0])}\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9], [6, 7], [4, 5], [2, 3], [0, 1]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_seq","title":"batchtensor.nested.index_select_along_seq","text":"<pre><code>index_select_along_seq(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_seq\n&gt;&gt;&gt; tensors = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 4], [7, 9]]), 'b': tensor([[2, 0]])}\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3, 2, 1, 0], [9, 8, 7, 6, 5]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log","title":"batchtensor.nested.log","text":"<pre><code>log(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the natural logarithm of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The natural logarithm of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log10","title":"batchtensor.nested.log10","text":"<pre><code>log10(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the logarithm to the base 10 of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The with the logarithm to the base 10 of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log10\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log10(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log1p","title":"batchtensor.nested.log1p","text":"<pre><code>log1p(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the natural logarithm of <code>(1 + input)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The natural logarithm of <code>(1 + input)</code>. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log1p\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log1p(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.log2","title":"batchtensor.nested.log2","text":"<pre><code>log2(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the logarithm to the base 2 of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The logarithm to the base 2 of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import log2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = log2(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.max_along_batch","title":"batchtensor.nested.max_along_batch","text":"<pre><code>max_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the maximum values and  the second tensor, which must have dtype long, with their  indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import max_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = max_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4])),\n'b': torch.return_types.max(\nvalues=tensor(4),\nindices=tensor(0))}\n&gt;&gt;&gt; out = max_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]])),\n'b': torch.return_types.max(\nvalues=tensor([4]),\nindices=tensor([0]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.max_along_seq","title":"batchtensor.nested.max_along_seq","text":"<pre><code>max_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import max_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = max_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4])),\n'b': torch.return_types.max(\nvalues=tensor([4]),\nindices=tensor([0]))}\n&gt;&gt;&gt; out = max_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]])),\n'b': torch.return_types.max(\nvalues=tensor([[4]]),\nindices=tensor([[0]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.mean_along_batch","title":"batchtensor.nested.mean_along_batch","text":"<pre><code>mean_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The mean of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import mean_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0], dtype=torch.float),\n... }\n&gt;&gt;&gt; out = mean_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([4., 5.]), 'b': tensor(2.)}\n&gt;&gt;&gt; out = mean_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4., 5.]]), 'b': tensor([2.])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.mean_along_seq","title":"batchtensor.nested.mean_along_seq","text":"<pre><code>mean_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import mean_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]], dtype=torch.float),\n... }\n&gt;&gt;&gt; out = mean_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([2., 7.]), 'b': tensor([2.])}\n&gt;&gt;&gt; out = mean_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[2.], [7.]]), 'b': tensor([[2.]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.median_along_batch","title":"batchtensor.nested.median_along_batch","text":"<pre><code>median_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import median_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = median_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2])),\n'b': torch.return_types.median(\nvalues=tensor(2),\nindices=tensor(2))}\n&gt;&gt;&gt; out = median_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]])),\n'b': torch.return_types.median(\nvalues=tensor([2]),\nindices=tensor([2]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.median_along_seq","title":"batchtensor.nested.median_along_seq","text":"<pre><code>median_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import median_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = median_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2])),\n'b': torch.return_types.median(\nvalues=tensor([2]),\nindices=tensor([2]))}\n&gt;&gt;&gt; out = median_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]])),\n'b': torch.return_types.median(\nvalues=tensor([[2]]),\nindices=tensor([[2]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.min_along_batch","title":"batchtensor.nested.min_along_batch","text":"<pre><code>min_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import min_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = min_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0])),\n'b': torch.return_types.min(\nvalues=tensor(0),\nindices=tensor(4))}\n&gt;&gt;&gt; out = min_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]])),\n'b': torch.return_types.min(\nvalues=tensor([0]),\nindices=tensor([4]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.min_along_seq","title":"batchtensor.nested.min_along_seq","text":"<pre><code>min_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import min_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = min_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0])),\n'b': torch.return_types.min(\nvalues=tensor([0]),\nindices=tensor([4]))}\n&gt;&gt;&gt; out = min_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]])),\n'b': torch.return_types.min(\nvalues=tensor([[0]]),\nindices=tensor([[4]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_batch","title":"batchtensor.nested.permute_along_batch","text":"<pre><code>permute_along_batch(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the batch dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = permute_along_batch(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [2, 3], [6, 7], [0, 1], [8, 9]]), 'b': tensor([2, 3, 1, 4, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_seq","title":"batchtensor.nested.permute_along_seq","text":"<pre><code>permute_along_seq(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = permute_along_seq(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 3, 0, 4], [7, 6, 8, 5, 9]]), 'b': tensor([[2, 3, 1, 4, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.prod_along_batch","title":"batchtensor.nested.prod_along_batch","text":"<pre><code>prod_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The product of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import prod_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([5, 4, 3, 2, 1]),\n... }\n&gt;&gt;&gt; out = prod_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([  0, 945]), 'b': tensor(120)}\n&gt;&gt;&gt; out = prod_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[  0, 945]]), 'b': tensor([120])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.prod_along_seq","title":"batchtensor.nested.prod_along_seq","text":"<pre><code>prod_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The product of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import prod_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[5, 4, 3, 2, 1]]),\n... }\n&gt;&gt;&gt; out = prod_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([    0, 15120]), 'b': tensor([120])}\n&gt;&gt;&gt; out = prod_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[    0], [15120]]), 'b': tensor([[120]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.repeat_along_seq","title":"batchtensor.nested.repeat_along_seq","text":"<pre><code>repeat_along_seq(data: Any, repeats: int) -&gt; Any\n</code></pre> <p>Repeat all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>repeats</code> <code>int</code> <p>The number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The tensors repeated along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import repeat_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = repeat_along_seq(data, 2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4], [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]]),\n 'b': tensor([[4, 3, 2, 1, 0, 4, 3, 2, 1, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_batch","title":"batchtensor.nested.select_along_batch","text":"<pre><code>select_along_batch(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = select_along_batch(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([4, 5]), 'b': tensor(2)}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_seq","title":"batchtensor.nested.select_along_seq","text":"<pre><code>select_along_seq(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = select_along_seq(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([2, 7]), 'b': tensor([2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_batch","title":"batchtensor.nested.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    data: Any, generator: Generator | None = None\n) -&gt; Any\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = shuffle_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_seq","title":"batchtensor.nested.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    data: Any, generator: Generator | None = None\n) -&gt; Any\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = shuffle_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([[...]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sin","title":"batchtensor.nested.sin","text":"<pre><code>sin(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the sine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sin\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = sin(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sinh","title":"batchtensor.nested.sinh","text":"<pre><code>sinh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the hyperbolic sine of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The hyperbolic sine of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sinh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = sinh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_batch","title":"batchtensor.nested.slice_along_batch","text":"<pre><code>slice_along_batch(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = slice_along_batch(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [6, 7], [8, 9]]), 'b': tensor([2, 1, 0])}\n&gt;&gt;&gt; out = slice_along_batch(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [2, 3], [4, 5]]), 'b': tensor([4, 3, 2])}\n&gt;&gt;&gt; out = slice_along_batch(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [4, 5], [8, 9]]), 'b': tensor([4, 2, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_seq","title":"batchtensor.nested.slice_along_seq","text":"<pre><code>slice_along_seq(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = slice_along_seq(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 3, 4], [7, 8, 9]]), 'b': tensor([[2, 1, 0]])}\n&gt;&gt;&gt; out = slice_along_seq(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [5, 6, 7]]), 'b': tensor([[4, 3, 2]])}\n&gt;&gt;&gt; out = slice_along_seq(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 2, 4], [5, 7, 9]]), 'b': tensor([[4, 2, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sort_along_batch","title":"batchtensor.nested.sort_along_batch","text":"<pre><code>sort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Sort the elements of the input tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A similar object where each tensor is replaced by a namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = sort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]]),\nindices=tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])),\n'b': torch.return_types.sort(\nvalues=tensor([0, 1, 2, 3, 4]),\nindices=tensor([4, 3, 2, 1, 0]))}\n&gt;&gt;&gt; out = sort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[8, 9], [5, 7], [4, 6], [2, 3], [0, 1]]),\nindices=tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])),\n'b': torch.return_types.sort(\nvalues=tensor([4, 3, 2, 1, 0]),\nindices=tensor([0, 1, 2, 3, 4]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sort_along_seq","title":"batchtensor.nested.sort_along_seq","text":"<pre><code>sort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Sort the elements of the input tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A similar object where each tensor is replaced by a namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = sort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[0, 3, 5, 7, 8], [1, 2, 4, 6, 9]]),\nindices=tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]])),\n'b': torch.return_types.sort(\nvalues=tensor([[0, 1, 2, 3, 4]]),\nindices=tensor([[4, 3, 2, 1, 0]]))}\n&gt;&gt;&gt; out = sort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': torch.return_types.sort(\nvalues=tensor([[8, 7, 5, 3, 0], [9, 6, 4, 2, 1]]),\nindices=tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]])),\n'b': torch.return_types.sort(\nvalues=tensor([[4, 3, 2, 1, 0]]),\nindices=tensor([[0, 1, 2, 3, 4]]))}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_batch","title":"batchtensor.nested.split_along_batch","text":"<pre><code>split_along_batch(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; outputs = split_along_batch(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_seq","title":"batchtensor.nested.split_along_seq","text":"<pre><code>split_along_seq(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; outputs = split_along_seq(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sum_along_batch","title":"batchtensor.nested.sum_along_batch","text":"<pre><code>sum_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sum_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = sum_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([20, 25]), 'b': tensor(10)}\n&gt;&gt;&gt; out = sum_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[20, 25]]), 'b': tensor([10])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.sum_along_seq","title":"batchtensor.nested.sum_along_seq","text":"<pre><code>sum_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sum_along_seq\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),\n...     \"b\": torch.tensor([[4, 3, 2, 1, 0]]),\n... }\n&gt;&gt;&gt; out = sum_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([10, 35]), 'b': tensor([10])}\n&gt;&gt;&gt; out = sum_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[10], [35]]), 'b': tensor([[10]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.tan","title":"batchtensor.nested.tan","text":"<pre><code>tan(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the tangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The tangent of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import tan\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = tan(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.tanh","title":"batchtensor.nested.tanh","text":"<pre><code>tanh(data: Any) -&gt; Any\n</code></pre> <p>Return new tensors with the hyperbolic tangent of each element.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The hyperbolic tangent of the elements. The output has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import tanh\n&gt;&gt;&gt; data = {\"a\": torch.randn(5, 2), \"b\": torch.rand(5)}\n&gt;&gt;&gt; out = tanh(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.to","title":"batchtensor.nested.to","text":"<pre><code>to(data: Any, *args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Perform Tensor dtype and/or device conversion.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>args</code> <code>Any</code> <p>Positional arguments for <code>torch.Tensor.to</code>.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments for <code>torch.Tensor.to</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The data after conversion.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = to(data, dtype=torch.float)\n&gt;&gt;&gt; out\n{'a': tensor([[0., 1.], [2., 3.], [4., 5.], [6., 7.], [8., 9.]]),\n 'b': tensor([4., 3., 2., 1., 0.])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.to_numpy","title":"batchtensor.nested.to_numpy","text":"<pre><code>to_numpy(data: Any) -&gt; Any\n</code></pre> <p>Create a new nested data structure where the <code>torch.Tensor</code>s are converted to <code>numpy.ndarray</code>s.</p> Note <p>The returned <code>torch.Tensor</code>s and <code>numpy.ndarray</code>s share the same memory. Modifications to the <code>torch.Tensor</code>s will be reflected in the <code>numpy.ndarray</code>s and vice versa.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a <code>torch.Tensor</code>.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A nested data structure with <code>numpy.ndarray</code>s instead of <code>torch.Tensor</code>s. The output data has the same structure as the input.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from batchtensor.nested import to_numpy\n&gt;&gt;&gt; data = {\"a\": torch.ones(2, 5), \"b\": torch.tensor([0, 1, 2, 3, 4])}\n&gt;&gt;&gt; out = to_numpy(data)\n&gt;&gt;&gt; out\n{'a': array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]], dtype=float32), 'b': array([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/recursive/","title":"Recursive Module API Reference","text":""},{"location":"refs/recursive/#batchtensor.recursive","title":"batchtensor.recursive","text":"<p>Contain features to easily work on nested objects.</p>"},{"location":"refs/recursive/#batchtensor.recursive.ApplyState","title":"batchtensor.recursive.ApplyState  <code>dataclass</code>","text":"<p>Store the current state.</p>"},{"location":"refs/recursive/#batchtensor.recursive.AutoApplier","title":"batchtensor.recursive.AutoApplier","text":"<p>               Bases: <code>BaseApplier[Any]</code></p> <p>Implement an applier that can automatically call other appliers based on the data type.</p>"},{"location":"refs/recursive/#batchtensor.recursive.AutoApplier.add_applier","title":"batchtensor.recursive.AutoApplier.add_applier  <code>classmethod</code>","text":"<pre><code>add_applier(\n    data_type: type,\n    applier: BaseApplier,\n    exist_ok: bool = False,\n) -&gt; None\n</code></pre> <p>Add an applier for a given data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type</code> <p>The data type for this test.</p> required <code>applier</code> <code>BaseApplier</code> <p>The applier object.</p> required <code>exist_ok</code> <code>bool</code> <p>If <code>False</code>, <code>RuntimeError</code> is raised if the data type already exists. This parameter should be set to <code>True</code> to overwrite the applier for a type.</p> <code>False</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if a applier is already registered for the data type and <code>exist_ok=False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import AutoApplier, SequenceApplier\n&gt;&gt;&gt; AutoApplier.add_applier(list, SequenceApplier(), exist_ok=True)\n</code></pre>"},{"location":"refs/recursive/#batchtensor.recursive.AutoApplier.find_applier","title":"batchtensor.recursive.AutoApplier.find_applier  <code>classmethod</code>","text":"<pre><code>find_applier(data_type: Any) -&gt; BaseApplier\n</code></pre> <p>Find the applier associated to an object.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Any</code> <p>The data type to get.</p> required <p>Returns:</p> Type Description <code>BaseApplier</code> <p>The applier associated to the data type.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import AutoApplier\n&gt;&gt;&gt; AutoApplier.find_applier(list)\nSequenceApplier()\n</code></pre>"},{"location":"refs/recursive/#batchtensor.recursive.AutoApplier.has_applier","title":"batchtensor.recursive.AutoApplier.has_applier  <code>classmethod</code>","text":"<pre><code>has_applier(data_type: type) -&gt; bool\n</code></pre> <p>Indicate if an applier is registered for the given data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>type</code> <p>The data type to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if an applier is registered, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import AutoApplier\n&gt;&gt;&gt; AutoApplier.has_applier(list)\nTrue\n&gt;&gt;&gt; AutoApplier.has_applier(str)\nFalse\n</code></pre>"},{"location":"refs/recursive/#batchtensor.recursive.BaseApplier","title":"batchtensor.recursive.BaseApplier","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Define the base class to implement a recursive applier.</p>"},{"location":"refs/recursive/#batchtensor.recursive.BaseApplier.apply","title":"batchtensor.recursive.BaseApplier.apply  <code>abstractmethod</code>","text":"<pre><code>apply(data: T, func: Callable, state: ApplyState) -&gt; T\n</code></pre> <p>Recursively apply a function on all the items in a nested data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>T</code> <p>The input data.</p> required <code>func</code> <code>Callable</code> <p>The function to apply on each item.</p> required <code>state</code> <code>ApplyState</code> <p>The current state.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The transformed data.</p>"},{"location":"refs/recursive/#batchtensor.recursive.DefaultApplier","title":"batchtensor.recursive.DefaultApplier","text":"<p>               Bases: <code>BaseApplier[Any]</code></p> <p>Define the default applier.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import DefaultApplier, AutoApplier, ApplyState\n&gt;&gt;&gt; state = ApplyState(applier=AutoApplier())\n&gt;&gt;&gt; applier = DefaultApplier()\n&gt;&gt;&gt; applier\nDefaultApplier()\n&gt;&gt;&gt; out = applier.apply([1, \"abc\"], str, state)\n&gt;&gt;&gt; out\n\"[1, 'abc']\"\n</code></pre>"},{"location":"refs/recursive/#batchtensor.recursive.MappingApplier","title":"batchtensor.recursive.MappingApplier","text":"<p>               Bases: <code>BaseApplier[T]</code></p> <p>Define an applier for mappings/dictionaries.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import MappingApplier, AutoApplier, ApplyState\n&gt;&gt;&gt; state = ApplyState(applier=AutoApplier())\n&gt;&gt;&gt; applier = MappingApplier()\n&gt;&gt;&gt; applier\nMappingApplier()\n&gt;&gt;&gt; out = applier.apply({\"a\": 1, \"b\": \"abc\"}, str, state)\n&gt;&gt;&gt; out\n{'a': '1', 'b': 'abc'}\n</code></pre>"},{"location":"refs/recursive/#batchtensor.recursive.SequenceApplier","title":"batchtensor.recursive.SequenceApplier","text":"<p>               Bases: <code>BaseApplier[T]</code></p> <p>Define a applier for sequences/lists/tuples.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import SequenceApplier, AutoApplier, ApplyState\n&gt;&gt;&gt; state = ApplyState(applier=AutoApplier())\n&gt;&gt;&gt; applier = SequenceApplier()\n&gt;&gt;&gt; applier\nSequenceApplier()\n&gt;&gt;&gt; out = applier.apply([1, \"abc\"], str, state)\n&gt;&gt;&gt; out\n['1', 'abc']\n</code></pre>"},{"location":"refs/recursive/#batchtensor.recursive.recursive_apply","title":"batchtensor.recursive.recursive_apply","text":"<pre><code>recursive_apply(data: Any, func: Callable) -&gt; Any\n</code></pre> <p>Recursively apply a function on all the items in a nested data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data.</p> required <code>func</code> <code>Callable</code> <p>The function to apply on each item.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The transformed data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; out = recursive_apply({\"a\": 1, \"b\": \"abc\"}, str)\n&gt;&gt;&gt; out\n{'a': '1', 'b': 'abc'}\n</code></pre>"},{"location":"refs/tensor/","title":"Tensor Module","text":""},{"location":"refs/tensor/#batchtensor.tensor","title":"batchtensor.tensor","text":"<p>Contain functions to manipulate tensors.</p>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_batch","title":"batchtensor.tensor.amax_along_batch","text":"<pre><code>amax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = amax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([8, 9])\n&gt;&gt;&gt; out = amax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_seq","title":"batchtensor.tensor.amax_along_seq","text":"<pre><code>amax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = amax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 9])\n&gt;&gt;&gt; out = amax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_batch","title":"batchtensor.tensor.amin_along_batch","text":"<pre><code>amin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = amin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 1])\n&gt;&gt;&gt; out = amin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_seq","title":"batchtensor.tensor.amin_along_seq","text":"<pre><code>amin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = amin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 5])\n&gt;&gt;&gt; out = amin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_batch","title":"batchtensor.tensor.argmax_along_batch","text":"<pre><code>argmax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = argmax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4, 4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_seq","title":"batchtensor.tensor.argmax_along_seq","text":"<pre><code>argmax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = argmax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_batch","title":"batchtensor.tensor.argmin_along_batch","text":"<pre><code>argmin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = argmin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_seq","title":"batchtensor.tensor.argmin_along_seq","text":"<pre><code>argmin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = argmin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_batch","title":"batchtensor.tensor.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the batch dimension</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; out = argsort_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])\n&gt;&gt;&gt; out = argsort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_seq","title":"batchtensor.tensor.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[2, 1, 4, 0, 3],\n        [0, 4, 3, 2, 1]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 0, 4, 1, 2],\n        [1, 2, 3, 4, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_batch","title":"batchtensor.tensor.cat_along_batch","text":"<pre><code>cat_along_batch(\n    tensors: list[Tensor] | tuple[Tensor, ...],\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>The tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_batch\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11, 12], [13, 14, 15]]),\n... ]\n&gt;&gt;&gt; out = cat_along_batch(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2],\n        [ 4,  5,  6],\n        [10, 11, 12],\n        [13, 14, 15]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_seq","title":"batchtensor.tensor.cat_along_seq","text":"<pre><code>cat_along_seq(\n    tensors: list[Tensor] | tuple[Tensor, ...],\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>The tensors to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_seq\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11], [12, 13]]),\n... ]\n&gt;&gt;&gt; out = cat_along_seq(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2, 10, 11],\n        [ 4,  5,  6, 12, 13]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_batch","title":"batchtensor.tensor.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; outputs = chunk_along_batch(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_seq","title":"batchtensor.tensor.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; outputs = chunk_along_seq(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumprod_along_batch","title":"batchtensor.tensor.cumprod_along_batch","text":"<pre><code>cumprod_along_batch(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative product of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative product of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumprod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n&gt;&gt;&gt; out = cumprod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[   1,    2], [   3,    8], [  15,   48], [ 105,  384], [ 945, 3840]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumprod_along_seq","title":"batchtensor.tensor.cumprod_along_seq","text":"<pre><code>cumprod_along_seq(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative product of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative product of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumprod_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; out = cumprod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[    1,     2,     6,    24,   120],\n        [    6,    42,   336,  3024, 30240]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumsum_along_batch","title":"batchtensor.tensor.cumsum_along_batch","text":"<pre><code>cumsum_along_batch(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative sum of elements of input in the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative sum of elements of input in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = cumsum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[ 0,  1], [ 2,  4], [ 6,  9], [12, 16], [20, 25]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cumsum_along_seq","title":"batchtensor.tensor.cumsum_along_seq","text":"<pre><code>cumsum_along_seq(tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Return the cumulative sum of elements of input in the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The cumulative sum of elements of input in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cumsum_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = cumsum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  3,  6, 10],\n        [ 5, 11, 18, 26, 35]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_batch","title":"batchtensor.tensor.index_select_along_batch","text":"<pre><code>index_select_along_batch(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [8, 9]])\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[8, 9],\n        [6, 7],\n        [4, 5],\n        [2, 3],\n        [0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_seq","title":"batchtensor.tensor.index_select_along_seq","text":"<pre><code>index_select_along_seq(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 4],\n        [7, 9]])\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[4, 3, 2, 1, 0],\n        [9, 8, 7, 6, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_batch","title":"batchtensor.tensor.max_along_batch","text":"<pre><code>max_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = max_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_seq","title":"batchtensor.tensor.max_along_seq","text":"<pre><code>max_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = max_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_batch","title":"batchtensor.tensor.mean_along_batch","text":"<pre><code>mean_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0], [2.0, 3.0], [4.0, 5.0], [6.0, 7.0], [8.0, 9.0]])\n&gt;&gt;&gt; out = mean_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4., 5.])\n&gt;&gt;&gt; out = mean_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4., 5.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_seq","title":"batchtensor.tensor.mean_along_seq","text":"<pre><code>mean_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0, 9.0]])\n&gt;&gt;&gt; out = mean_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([2., 7.])\n&gt;&gt;&gt; out = mean_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[2.], [7.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_batch","title":"batchtensor.tensor.median_along_batch","text":"<pre><code>median_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = median_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_seq","title":"batchtensor.tensor.median_along_seq","text":"<pre><code>median_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = median_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_batch","title":"batchtensor.tensor.min_along_batch","text":"<pre><code>min_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = min_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_seq","title":"batchtensor.tensor.min_along_seq","text":"<pre><code>min_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = min_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_batch","title":"batchtensor.tensor.permute_along_batch","text":"<pre><code>permute_along_batch(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the batch dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = permute_along_batch(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [2, 3],\n        [6, 7],\n        [0, 1],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_seq","title":"batchtensor.tensor.permute_along_seq","text":"<pre><code>permute_along_seq(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the sequence dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = permute_along_seq(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 1, 3, 0, 4],\n        [7, 6, 8, 5, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_batch","title":"batchtensor.tensor.prod_along_batch","text":"<pre><code>prod_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = prod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([  0, 945])\n&gt;&gt;&gt; out = prod_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[  0, 945]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_seq","title":"batchtensor.tensor.prod_along_seq","text":"<pre><code>prod_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = prod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([    0, 15120])\n&gt;&gt;&gt; out = prod_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[    0], [15120]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.repeat_along_seq","title":"batchtensor.tensor.repeat_along_seq","text":"<pre><code>repeat_along_seq(tensor: Tensor, repeats: int) -&gt; Tensor\n</code></pre> <p>Repeat the data along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>repeats</code> <code>int</code> <p>The number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A new tensor with the data repeated along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import repeat_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = repeat_along_seq(tensor, 2)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_batch","title":"batchtensor.tensor.select_along_batch","text":"<pre><code>select_along_batch(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = select_along_batch(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([4, 5])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_seq","title":"batchtensor.tensor.select_along_seq","text":"<pre><code>select_along_seq(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = select_along_seq(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([2, 7])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_batch","title":"batchtensor.tensor.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = shuffle_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_seq","title":"batchtensor.tensor.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>An optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = shuffle_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_batch","title":"batchtensor.tensor.slice_along_batch","text":"<pre><code>slice_along_batch(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [6, 7],\n        [8, 9]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [4, 5],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_seq","title":"batchtensor.tensor.slice_along_seq","text":"<pre><code>slice_along_seq(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>The index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>The index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>The increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [9, 8, 7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[2, 3, 4],\n        [7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2],\n        [9, 8, 7]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 2, 4],\n        [9, 7, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sort_along_batch","title":"batchtensor.tensor.sort_along_batch","text":"<pre><code>sort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; sort\n</code></pre> <p>Sort the elements of the input tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sort</code> <p>A namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; out = sort_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[0, 1], [2, 3], [4, 6], [5, 7], [8, 9]]),\nindices=tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]))\n&gt;&gt;&gt; out = sort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[8, 9], [5, 7], [4, 6], [2, 3], [0, 1]]),\nindices=tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sort_along_seq","title":"batchtensor.tensor.sort_along_seq","text":"<pre><code>sort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; sort\n</code></pre> <p>Sort the elements of the input tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.sort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>sort</code> <p>A namedtuple of (values, indices), where the values are the sorted values and indices are the indices of the elements in the original input tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = sort_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[0, 3, 5, 7, 8], [1, 2, 4, 6, 9]]),\nindices=tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]))\n&gt;&gt;&gt; out = sort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntorch.return_types.sort(\nvalues=tensor([[8, 7, 5, 3, 0], [9, 6, 4, 2, 1]]),\nindices=tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_batch","title":"batchtensor.tensor.split_along_batch","text":"<pre><code>split_along_batch(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; outputs = split_along_batch(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_seq","title":"batchtensor.tensor.split_along_seq","text":"<pre><code>split_along_seq(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; outputs = split_along_seq(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_batch","title":"batchtensor.tensor.sum_along_batch","text":"<pre><code>sum_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])\n&gt;&gt;&gt; out = sum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([20, 25])\n&gt;&gt;&gt; out = sum_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[20, 25]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_seq","title":"batchtensor.tensor.sum_along_seq","text":"<pre><code>sum_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n&gt;&gt;&gt; out = sum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([10, 35])\n&gt;&gt;&gt; out = sum_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[10], [35]])\n</code></pre>"},{"location":"refs/utils/","title":"Utils Module API Reference","text":""},{"location":"refs/utils/#batchtensor.utils","title":"batchtensor.utils","text":"<p>Contain utility functions.</p>"},{"location":"refs/utils/#batchtensor.utils.bfs_tensor","title":"batchtensor.utils.bfs_tensor","text":"<pre><code>bfs_tensor(data: Any) -&gt; Generator[Tensor]\n</code></pre> <p>Implement a Breadth-First Search (BFS) iterator over the <code>torch.Tensor</code>s.</p> <p>This function assumes the underlying data has a tree-like structure.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to iterate on.</p> required <p>Yields:</p> Type Description <code>Generator[Tensor]</code> <p>The next <code>torch.Tensor</code> in the data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils.bfs import bfs_tensor\n&gt;&gt;&gt; list(bfs_tensor([\"abc\", torch.ones(2, 3), 42, torch.tensor([0, 1, 2, 3, 4])]))\n[tensor([[1., 1., 1.], [1., 1., 1.]]), tensor([0, 1, 2, 3, 4])]\n</code></pre>"},{"location":"refs/utils/#batchtensor.utils.dfs_tensor","title":"batchtensor.utils.dfs_tensor","text":"<pre><code>dfs_tensor(data: Any) -&gt; Generator[Tensor]\n</code></pre> <p>Implement a Depth-First Search (DFS) iterator over the <code>torch.Tensor</code>s.</p> <p>This function assumes the underlying data has a tree-like structure.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The data to iterate on.</p> required <p>Yields:</p> Type Description <code>Generator[Tensor]</code> <p>The next <code>torch.Tensor</code> in the data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils.dfs import dfs_tensor\n&gt;&gt;&gt; list(dfs_tensor([\"abc\", torch.ones(2, 3), 42, torch.tensor([0, 1, 2, 3, 4])]))\n[tensor([[1., 1., 1.], [1., 1., 1.]]), tensor([0, 1, 2, 3, 4])]\n</code></pre>"},{"location":"uguide/nested/","title":"Nested Data Manipulation","text":"<p>The <code>batchtensor.nested</code> module provides functions to manipulate nested data structures containing PyTorch tensors. These functions work with dictionaries, lists, and other nested structures where tensors share batch or sequence dimensions.</p>"},{"location":"uguide/nested/#overview","title":"Overview","text":"<p>When working with complex data pipelines, you often have batches represented as nested structures (e.g., dictionaries of tensors). The nested module makes it easy to apply operations across all tensors in these structures.</p>"},{"location":"uguide/nested/#slicing-operations","title":"Slicing Operations","text":""},{"location":"uguide/nested/#slicing-along-batch-dimension","title":"Slicing Along Batch Dimension","text":"<p>Extract a subset of the batch from all tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]]),\n...     \"labels\": torch.tensor([0, 1, 0, 1]),\n...     \"weights\": torch.tensor([1.0, 0.5, 0.8, 1.2]),\n... }\n&gt;&gt;&gt; # Take first 2 items\n&gt;&gt;&gt; slice_along_batch(batch, stop=2)\n{'features': tensor([[1, 2], [3, 4]]), 'labels': tensor([0, 1]), 'weights': tensor([1.0000, 0.5000])}\n</code></pre>"},{"location":"uguide/nested/#slicing-along-sequence-dimension","title":"Slicing Along Sequence Dimension","text":"<p>For sequential data with shape <code>(batch_size, seq_len, *)</code>:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_seq\n&gt;&gt;&gt; batch = {\n...     \"tokens\": torch.tensor([[[1], [2], [3], [4]], [[5], [6], [7], [8]]]),\n...     \"attention\": torch.tensor([[1.0, 0.9, 0.8, 0.7], [1.0, 0.95, 0.9, 0.85]]),\n... }\n&gt;&gt;&gt; # Take first 2 timesteps\n&gt;&gt;&gt; slice_along_seq(batch, stop=2)\n{'tokens': tensor([[[1], [2]], [[5], [6]]]), 'attention': tensor([[1.0000, 0.9000], [1.0000, 0.9500]])}\n</code></pre>"},{"location":"uguide/nested/#chunking","title":"Chunking","text":"<p>Split tensors into equal chunks:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; chunks = chunk_along_batch(batch, chunks=3)\n&gt;&gt;&gt; len(chunks)\n3\n&gt;&gt;&gt; chunks[0]\n{'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])}\n</code></pre>"},{"location":"uguide/nested/#splitting","title":"Splitting","text":"<p>Split tensors by size:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; splits = split_along_batch(batch, split_size_or_sections=2)\n&gt;&gt;&gt; len(splits)\n3\n&gt;&gt;&gt; splits[0]\n{'a': tensor([[2, 6], [0, 3]]), 'b': tensor([4, 3])}\n</code></pre>"},{"location":"uguide/nested/#indexing-operations","title":"Indexing Operations","text":"<p>Select specific indices from all tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; indices = torch.tensor([2, 0])\n&gt;&gt;&gt; index_select_along_batch(batch, indices)\n{'features': tensor([[5, 6], [1, 2]]), 'labels': tensor([2, 0])}\n</code></pre>"},{"location":"uguide/nested/#joining-operations","title":"Joining Operations","text":""},{"location":"uguide/nested/#concatenation","title":"Concatenation","text":"<p>Combine multiple batches:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_batch\n&gt;&gt;&gt; batch1 = {\"a\": torch.tensor([[1, 2]]), \"b\": torch.tensor([10])}\n&gt;&gt;&gt; batch2 = {\"a\": torch.tensor([[3, 4]]), \"b\": torch.tensor([20])}\n&gt;&gt;&gt; cat_along_batch([batch1, batch2])\n{'a': tensor([[1, 2], [3, 4]]), 'b': tensor([10, 20])}\n</code></pre>"},{"location":"uguide/nested/#repetition","title":"Repetition","text":"<p>Repeat sequences:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import repeat_along_seq\n&gt;&gt;&gt; batch = {\n...     \"seq\": torch.tensor([[[1], [2]], [[3], [4]]]),\n... }\n&gt;&gt;&gt; repeat_along_seq(batch, 2)\n{'seq': tensor([[[1], [2], [1], [2]], [[3], [4], [3], [4]]])}\n</code></pre>"},{"location":"uguide/nested/#reduction-operations","title":"Reduction Operations","text":""},{"location":"uguide/nested/#sum-mean-min-max","title":"Sum, Mean, Min, Max","text":"<p>Compute statistics along dimensions:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import (\n...     sum_along_batch,\n...     mean_along_batch,\n...     amax_along_batch,\n...     amin_along_batch,\n... )\n&gt;&gt;&gt; batch = {\n...     \"scores\": torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),\n...     \"counts\": torch.tensor([10, 20, 30]),\n... }\n&gt;&gt;&gt; sum_along_batch(batch)\n{'scores': tensor([9., 12.]), 'counts': tensor(60)}\n&gt;&gt;&gt; mean_along_batch(batch)\n{'scores': tensor([3., 4.]), 'counts': tensor(20.)}\n</code></pre>"},{"location":"uguide/nested/#argmax-and-argmin","title":"ArgMax and ArgMin","text":"<p>Find indices of extrema:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argmax_along_batch, argmin_along_batch\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 5], [3, 2], [4, 6]])}\n&gt;&gt;&gt; argmax_along_batch(batch)\n{'values': tensor([2, 2])}\n&gt;&gt;&gt; argmin_along_batch(batch)\n{'values': tensor([0, 1])}\n</code></pre>"},{"location":"uguide/nested/#comparison-and-sorting","title":"Comparison and Sorting","text":"<p>Sort tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sort_along_batch\n&gt;&gt;&gt; batch = {\n...     \"values\": torch.tensor([[3, 1], [1, 4], [2, 2]]),\n... }\n&gt;&gt;&gt; values, indices = sort_along_batch(batch)\n&gt;&gt;&gt; values\n{'values': tensor([[1, 1], [2, 2], [3, 4]])}\n&gt;&gt;&gt; indices\n{'values': tensor([[1, 0], [2, 2], [0, 1]])}\n</code></pre>"},{"location":"uguide/nested/#mathematical-operations","title":"Mathematical Operations","text":""},{"location":"uguide/nested/#cumulative-operations","title":"Cumulative Operations","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cumsum_along_batch, cumprod_along_batch\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 2], [3, 4], [5, 6]])}\n&gt;&gt;&gt; cumsum_along_batch(batch)\n{'values': tensor([[1, 2], [4, 6], [9, 12]])}\n</code></pre>"},{"location":"uguide/nested/#trigonometric-functions","title":"Trigonometric Functions","text":"<p>Apply trigonometric functions element-wise:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import sin, cos, tan\n&gt;&gt;&gt; batch = {\"angles\": torch.tensor([[0.0, 1.57], [3.14, 4.71]])}\n&gt;&gt;&gt; sin(batch)\n{'angles': tensor([[0.0000, 1.0000], [0.0016, -1.0000]])}\n</code></pre>"},{"location":"uguide/nested/#pointwise-operations","title":"Pointwise Operations","text":"<p>Apply element-wise operations:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import abs, exp, log, sqrt\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[-1.0, 2.0], [-3.0, 4.0]])}\n&gt;&gt;&gt; abs(batch)\n{'values': tensor([[1., 2.], [3., 4.]])}\n</code></pre>"},{"location":"uguide/nested/#permutation-operations","title":"Permutation Operations","text":""},{"location":"uguide/nested/#shuffling","title":"Shuffling","text":"<p>Randomly permute batch items:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; # Results will be random\n&gt;&gt;&gt; shuffle_along_batch(batch)  # doctest: +SKIP\n{'features': tensor([[5, 6], [1, 2], [3, 4]]), 'labels': tensor([2, 0, 1])}\n</code></pre>"},{"location":"uguide/nested/#permuting","title":"Permuting","text":"<p>Apply a specific permutation:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_batch\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"labels\": torch.tensor([0, 1, 2]),\n... }\n&gt;&gt;&gt; permutation = torch.tensor([2, 0, 1])\n&gt;&gt;&gt; permute_along_batch(batch, permutation)\n{'features': tensor([[5, 6], [1, 2], [3, 4]]), 'labels': tensor([2, 0, 1])}\n</code></pre>"},{"location":"uguide/nested/#type-conversion","title":"Type Conversion","text":""},{"location":"uguide/nested/#change-device","title":"Change Device","text":"<p>Move all tensors to a specific device:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to_device\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.tensor([[1, 2], [3, 4]]),\n...     \"labels\": torch.tensor([0, 1]),\n... }\n&gt;&gt;&gt; # Move to CPU (already there in this example)\n&gt;&gt;&gt; to_device(batch, torch.device(\"cpu\"))\n{'features': tensor([[1, 2], [3, 4]]), 'labels': tensor([0, 1])}\n</code></pre>"},{"location":"uguide/nested/#change-data-type","title":"Change Data Type","text":"<p>Convert tensor dtypes:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import to_dtype\n&gt;&gt;&gt; batch = {\"values\": torch.tensor([[1, 2], [3, 4]], dtype=torch.int32)}\n&gt;&gt;&gt; to_dtype(batch, dtype=torch.float32)\n{'values': tensor([[1., 2.], [3., 4.]])}\n</code></pre>"},{"location":"uguide/nested/#numpy-conversion","title":"NumPy Conversion","text":"<p>Convert between PyTorch tensors and NumPy arrays:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import from_numpy, to_numpy\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # From NumPy\n&gt;&gt;&gt; numpy_data = {\n...     \"a\": np.array([[1, 2], [3, 4]]),\n...     \"b\": np.array([5, 6]),\n... }\n&gt;&gt;&gt; batch = from_numpy(numpy_data)  # doctest: +SKIP\n&gt;&gt;&gt; batch  # doctest: +SKIP\n{'a': tensor([[1, 2], [3, 4]]), 'b': tensor([5, 6])}\n&gt;&gt;&gt; # To NumPy\n&gt;&gt;&gt; to_numpy(batch)  # doctest: +SKIP\n{'a': array([[1, 2], [3, 4]]), 'b': array([5, 6])}\n</code></pre>"},{"location":"uguide/nested/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Dimensions: Ensure all tensors in your nested structure have compatible batch/sequence dimensions</li> <li>Memory Efficiency: Many operations like <code>chunk</code> and <code>slice</code> return views when possible, not copies</li> <li>Type Safety: Use the same data types across tensors in a batch for predictable behavior</li> <li>Error Handling: Functions will raise clear errors if tensors have incompatible shapes</li> </ol>"},{"location":"uguide/nested/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Operations are applied recursively to nested structures</li> <li>Views are used when possible to avoid copying data</li> <li>All functions leverage PyTorch's efficient tensor operations</li> <li>For very deep nesting, consider flattening your data structure</li> </ul>"},{"location":"uguide/nested/#see-also","title":"See Also","text":"<ul> <li>Tensor Operations - Operations for single tensors</li> <li>Recursive Module - Low-level recursive utilities</li> <li>API Reference - Complete function reference</li> </ul>"},{"location":"uguide/recursive/","title":"Recursive Operations","text":"<p>The <code>batchtensor.recursive</code> module provides low-level utilities for applying functions recursively to nested data structures. This module is used internally by higher-level functions but can also be used directly for custom operations.</p>"},{"location":"uguide/recursive/#overview","title":"Overview","text":"<p>The recursive module enables you to apply any function to elements within nested structures like dictionaries, lists, and tuples. It uses a pluggable applier system that handles different data types appropriately.</p>"},{"location":"uguide/recursive/#basic-usage","title":"Basic Usage","text":""},{"location":"uguide/recursive/#the-recursive_apply-function","title":"The <code>recursive_apply</code> Function","text":"<p>The core function for recursive operations:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; def double(x):\n...     return x * 2\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([1, 2, 3]),\n...     \"b\": torch.tensor([4, 5, 6]),\n... }\n&gt;&gt;&gt; recursive_apply(data, double)\n{'a': tensor([2, 4, 6]), 'b': tensor([ 8, 10, 12])}\n</code></pre>"},{"location":"uguide/recursive/#nested-structures","title":"Nested Structures","text":"<p>Works with deeply nested data:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; data = {\n...     \"level1\": {\n...         \"level2\": torch.tensor([1, 2]),\n...         \"level2b\": torch.tensor([3, 4]),\n...     },\n...     \"other\": torch.tensor([5, 6]),\n... }\n&gt;&gt;&gt; recursive_apply(data, lambda x: x + 10)\n{'level1': {'level2': tensor([11, 12]), 'level2b': tensor([13, 14])}, 'other': tensor([15, 16])}\n</code></pre>"},{"location":"uguide/recursive/#lists-and-tuples","title":"Lists and Tuples","text":"<p>Handles sequences naturally:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; data = [\n...     torch.tensor([1, 2]),\n...     torch.tensor([3, 4]),\n... ]\n&gt;&gt;&gt; recursive_apply(data, lambda x: x ** 2)\n[tensor([1, 4]), tensor([9, 16])]\n</code></pre>"},{"location":"uguide/recursive/#applier-system","title":"Applier System","text":"<p>The recursive module uses an applier pattern to handle different data types. Each applier knows how to traverse a specific type of data structure.</p>"},{"location":"uguide/recursive/#autoapplier","title":"AutoApplier","text":"<p>The <code>AutoApplier</code> automatically selects the correct applier based on data type:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import AutoApplier\n&gt;&gt;&gt; # Check if an applier is registered for a type\n&gt;&gt;&gt; AutoApplier.has_applier(dict)\nTrue\n&gt;&gt;&gt; AutoApplier.has_applier(list)\nTrue\n&gt;&gt;&gt; AutoApplier.has_applier(str)\nFalse\n</code></pre>"},{"location":"uguide/recursive/#built-in-appliers","title":"Built-in Appliers","text":"<p>Several appliers are provided:</p> <ul> <li><code>DefaultApplier</code>: Applies function directly to the data (leaf nodes)</li> <li><code>MappingApplier</code>: Handles dict-like objects</li> <li><code>SequenceApplier</code>: Handles list and tuple objects</li> <li><code>AutoApplier</code>: Automatically chooses the appropriate applier</li> </ul> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import (\n...     DefaultApplier,\n...     MappingApplier,\n...     SequenceApplier,\n... )\n&gt;&gt;&gt; # View an applier\n&gt;&gt;&gt; MappingApplier()\nMappingApplier()\n&gt;&gt;&gt; SequenceApplier()\nSequenceApplier()\n</code></pre>"},{"location":"uguide/recursive/#custom-appliers","title":"Custom Appliers","text":"<p>You can create custom appliers for your own types by inheriting from <code>BaseApplier</code>:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import BaseApplier, AutoApplier, ApplyState\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; class MyClass:\n...     def __init__(self, value):\n...         self.value = value\n&gt;&gt;&gt; class MyClassApplier(BaseApplier):\n...     def apply(self, data, func, state):\n...         # Apply function to the value attribute\n...         data.value = func(data.value)\n...         return data\n&gt;&gt;&gt; # Register the custom applier\n&gt;&gt;&gt; AutoApplier.add_applier(MyClass, MyClassApplier(), exist_ok=True)\n&gt;&gt;&gt; # Now use it\n&gt;&gt;&gt; obj = MyClass(torch.tensor([1, 2, 3]))\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; result = recursive_apply(obj, lambda x: x * 2)\n&gt;&gt;&gt; result.value\ntensor([2, 4, 6])\n</code></pre>"},{"location":"uguide/recursive/#applystate","title":"ApplyState","text":"<p>The <code>ApplyState</code> class tracks the recursion state during application:</p> <pre><code>&gt;&gt;&gt; from batchtensor.recursive import ApplyState\n&gt;&gt;&gt; state = ApplyState()\n&gt;&gt;&gt; state\nApplyState()\n</code></pre> <p>This is mainly used internally to manage recursion context, but you can access it if needed for advanced use cases.</p>"},{"location":"uguide/recursive/#practical-examples","title":"Practical Examples","text":""},{"location":"uguide/recursive/#apply-function-to-specific-keys","title":"Apply Function to Specific Keys","text":"<p>Filter which tensors get modified:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; def scale_features(data):\n...     if isinstance(data, dict):\n...         result = {}\n...         for key, value in data.items():\n...             if key.startswith(\"feature\"):\n...                 result[key] = recursive_apply(value, lambda x: x * 0.1)\n...             else:\n...                 result[key] = value\n...         return result\n...     return data\n&gt;&gt;&gt; batch = {\n...     \"feature1\": torch.tensor([10.0, 20.0]),\n...     \"feature2\": torch.tensor([30.0, 40.0]),\n...     \"label\": torch.tensor([0, 1]),\n... }\n&gt;&gt;&gt; scale_features(batch)\n{'feature1': tensor([1., 2.]), 'feature2': tensor([3., 4.]), 'label': tensor([0, 1])}\n</code></pre>"},{"location":"uguide/recursive/#normalize-nested-data","title":"Normalize Nested Data","text":"<p>Apply normalization recursively:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; def normalize(x):\n...     if isinstance(x, torch.Tensor) and x.dtype in [torch.float32, torch.float64]:\n...         return (x - x.mean()) / (x.std() + 1e-8)\n...     return x\n&gt;&gt;&gt; data = {\n...     \"input\": torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]),\n...     \"metadata\": {\n...         \"counts\": torch.tensor([10, 20, 30], dtype=torch.int64),\n...         \"scores\": torch.tensor([0.5, 1.0, 1.5]),\n...     },\n... }\n&gt;&gt;&gt; recursive_apply(data, normalize)  # doctest: +SKIP\n{'input': tensor([-1.4142, -0.7071,  0.0000,  0.7071,  1.4142]), 'metadata': {'counts': tensor([10, 20, 30]), 'scores': tensor([-1.2247,  0.0000,  1.2247])}}\n</code></pre>"},{"location":"uguide/recursive/#collect-statistics","title":"Collect Statistics","text":"<p>Gather information from nested structures:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; shapes = []\n&gt;&gt;&gt; def collect_shapes(x):\n...     if isinstance(x, torch.Tensor):\n...         shapes.append(x.shape)\n...     return x\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[1, 2], [3, 4]]),\n...     \"b\": torch.tensor([5, 6, 7]),\n... }\n&gt;&gt;&gt; _ = recursive_apply(data, collect_shapes)\n&gt;&gt;&gt; shapes\n[torch.Size([2, 2]), torch.Size([3])]\n</code></pre>"},{"location":"uguide/recursive/#advanced-usage","title":"Advanced Usage","text":""},{"location":"uguide/recursive/#combining-with-higher-level-functions","title":"Combining with Higher-Level Functions","text":"<p>The recursive module is used internally by nested operations:</p> <pre><code>&gt;&gt;&gt; # This is how nested functions work internally\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; # The nested module uses recursive_apply\n&gt;&gt;&gt; def slice_all(data, start, stop):\n...     return recursive_apply(\n...         data,\n...         partial(lambda x, s, e: x[s:e], s=start, e=stop)\n...     )\n&gt;&gt;&gt; batch = {\n...     \"a\": torch.tensor([[1, 2], [3, 4], [5, 6]]),\n...     \"b\": torch.tensor([7, 8, 9]),\n... }\n&gt;&gt;&gt; slice_all(batch, 0, 2)\n{'a': tensor([[1, 2], [3, 4]]), 'b': tensor([7, 8])}\n</code></pre>"},{"location":"uguide/recursive/#type-safe-operations","title":"Type-Safe Operations","text":"<p>Ensure operations only apply to tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; def safe_operation(x):\n...     if isinstance(x, torch.Tensor):\n...         return x.float()\n...     return x\n&gt;&gt;&gt; data = {\n...     \"tensor\": torch.tensor([1, 2, 3], dtype=torch.int32),\n...     \"string\": \"metadata\",\n...     \"number\": 42,\n... }\n&gt;&gt;&gt; recursive_apply(data, safe_operation)\n{'tensor': tensor([1., 2., 3.]), 'string': 'metadata', 'number': 42}\n</code></pre>"},{"location":"uguide/recursive/#best-practices","title":"Best Practices","text":"<ol> <li>Keep Functions Pure: Functions passed to <code>recursive_apply</code> should not have side effects on the structure itself</li> <li>Handle Multiple Types: If your function might encounter different types, add type checks</li> <li>Use Partial Functions: When you need to pass additional arguments, use <code>functools.partial</code></li> <li>Register Custom Appliers Early: Add custom appliers at module initialization</li> <li>Avoid Deep Recursion: Very deeply nested structures may hit recursion limits</li> </ol>"},{"location":"uguide/recursive/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The applier pattern adds minimal overhead</li> <li>Most time is spent in your custom function</li> <li>For large batches, vectorized operations are much faster than recursive calls</li> <li>Consider flattening data if recursion depth is very large</li> </ul>"},{"location":"uguide/recursive/#common-patterns","title":"Common Patterns","text":""},{"location":"uguide/recursive/#transform-and-filter","title":"Transform and Filter","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; # Only modify float tensors\n&gt;&gt;&gt; def to_half_precision(x):\n...     if isinstance(x, torch.Tensor) and x.dtype == torch.float32:\n...         return x.half()\n...     return x\n&gt;&gt;&gt; data = {\n...     \"floats\": torch.tensor([1.0, 2.0], dtype=torch.float32),\n...     \"ints\": torch.tensor([1, 2], dtype=torch.int32),\n... }\n&gt;&gt;&gt; recursive_apply(data, to_half_precision)\n{'floats': tensor([1., 2.], dtype=torch.float16), 'ints': tensor([1, 2], dtype=torch.int32)}\n</code></pre>"},{"location":"uguide/recursive/#clone-nested-data","title":"Clone Nested Data","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.recursive import recursive_apply\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([1, 2, 3]),\n...     \"b\": {\"c\": torch.tensor([4, 5])},\n... }\n&gt;&gt;&gt; cloned = recursive_apply(data, lambda x: x.clone() if isinstance(x, torch.Tensor) else x)\n&gt;&gt;&gt; cloned[\"a\"][0] = 100\n&gt;&gt;&gt; data[\"a\"][0]  # Original unchanged\ntensor(1)\n</code></pre>"},{"location":"uguide/recursive/#see-also","title":"See Also","text":"<ul> <li>Nested Operations - High-level operations using recursive utilities</li> <li>Utils Module - Tensor traversal utilities</li> <li>API Reference - Complete function reference</li> </ul>"},{"location":"uguide/tensor/","title":"Tensor","text":""},{"location":"uguide/tensor/#batch","title":"Batch","text":"<p><code>batchtensor</code> provides functions to manipulate tensors representing a batch of examples. The functions assume the tensors have the following shape: <code>batch_size, *</code> where <code>*</code> means any dimensions.</p>"},{"location":"uguide/tensor/#sequence","title":"Sequence","text":"<p><code>batchtensor</code> provides functions to manipulate tensors representing a batch of sequences. The functions assume the tensors have the following shape: <code>batch_size, seq_len, *</code> where <code>*</code> means any dimensions.</p>"},{"location":"uguide/utils/","title":"Utility Functions","text":"<p>The <code>batchtensor.utils</code> module provides utility functions for traversing and analyzing nested tensor structures. These functions are useful for debugging, introspection, and understanding the structure of complex nested data.</p>"},{"location":"uguide/utils/#overview","title":"Overview","text":"<p>The utils module includes functions for:</p> <ul> <li>Traversing nested structures using breadth-first search (BFS)</li> <li>Traversing nested structures using depth-first search (DFS)</li> <li>Analyzing the structure and contents of nested data</li> </ul>"},{"location":"uguide/utils/#breadth-first-search-bfs","title":"Breadth-First Search (BFS)","text":"<p>The <code>bfs_tensor</code> function traverses a nested structure in breadth-first order, visiting all items at the current depth before moving to the next depth level.</p>"},{"location":"uguide/utils/#basic-usage","title":"Basic Usage","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([1, 2, 3]),\n...     \"b\": torch.tensor([4, 5, 6]),\n... }\n&gt;&gt;&gt; for path, value in bfs_tensor(data):\n...     print(f\"{path}: {value.shape}\")\n('a',): torch.Size([3])\n('b',): torch.Size([3])\n</code></pre>"},{"location":"uguide/utils/#nested-structures","title":"Nested Structures","text":"<p>BFS processes siblings before children:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; data = {\n...     \"level1_a\": torch.tensor([1, 2]),\n...     \"level1_b\": {\n...         \"level2_a\": torch.tensor([3, 4]),\n...         \"level2_b\": torch.tensor([5, 6]),\n...     },\n... }\n&gt;&gt;&gt; for path, value in bfs_tensor(data):\n...     print(f\"Path: {path}, Shape: {value.shape}\")\nPath: ('level1_a',), Shape: torch.Size([2])\nPath: ('level1_b', 'level2_a'), Shape: torch.Size([2])\nPath: ('level1_b', 'level2_b'), Shape: torch.Size([2])\n</code></pre>"},{"location":"uguide/utils/#understanding-the-traversal-order","title":"Understanding the Traversal Order","text":"<p>BFS visits nodes level by level:</p> <pre><code>Structure:           Traversal Order:\n    root                   1\n   /  |  \\                /  |  \\\n  A   B   C              2   3   4\n     / \\                    / \\\n    D   E                  5   6\n</code></pre> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; data = {\n...     \"A\": torch.tensor([1]),\n...     \"B\": {\n...         \"D\": torch.tensor([2]),\n...         \"E\": torch.tensor([3]),\n...     },\n...     \"C\": torch.tensor([4]),\n... }\n&gt;&gt;&gt; paths = [path for path, _ in bfs_tensor(data)]\n&gt;&gt;&gt; paths\n[('A',), ('B', 'D'), ('B', 'E'), ('C',)]\n</code></pre>"},{"location":"uguide/utils/#depth-first-search-dfs","title":"Depth-First Search (DFS)","text":"<p>The <code>dfs_tensor</code> function traverses a nested structure in depth-first order, exploring as far as possible along each branch before backtracking.</p>"},{"location":"uguide/utils/#basic-usage_1","title":"Basic Usage","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import dfs_tensor\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([1, 2, 3]),\n...     \"b\": torch.tensor([4, 5, 6]),\n... }\n&gt;&gt;&gt; for path, value in dfs_tensor(data):\n...     print(f\"{path}: {value.shape}\")\n('a',): torch.Size([3])\n('b',): torch.Size([3])\n</code></pre>"},{"location":"uguide/utils/#nested-structures_1","title":"Nested Structures","text":"<p>DFS explores deeply before moving to siblings:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import dfs_tensor\n&gt;&gt;&gt; data = {\n...     \"level1_a\": torch.tensor([1, 2]),\n...     \"level1_b\": {\n...         \"level2_a\": torch.tensor([3, 4]),\n...         \"level2_b\": torch.tensor([5, 6]),\n...     },\n... }\n&gt;&gt;&gt; for path, value in dfs_tensor(data):\n...     print(f\"Path: {path}, Shape: {value.shape}\")\nPath: ('level1_a',), Shape: torch.Size([2])\nPath: ('level1_b', 'level2_a'), Shape: torch.Size([2])\nPath: ('level1_b', 'level2_b'), Shape: torch.Size([2])\n</code></pre>"},{"location":"uguide/utils/#understanding-the-traversal-order_1","title":"Understanding the Traversal Order","text":"<p>DFS explores depth before breadth:</p> <pre><code>Structure:           Traversal Order:\n    root                   1\n   /  |  \\                /  |  \\\n  A   B   C              2   4   6\n     / \\                    / \\\n    D   E                  3   5\n</code></pre> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import dfs_tensor\n&gt;&gt;&gt; data = {\n...     \"A\": torch.tensor([1]),\n...     \"B\": {\n...         \"D\": torch.tensor([2]),\n...         \"E\": torch.tensor([3]),\n...     },\n...     \"C\": torch.tensor([4]),\n... }\n&gt;&gt;&gt; paths = [path for path, _ in dfs_tensor(data)]\n&gt;&gt;&gt; paths\n[('A',), ('B', 'D'), ('B', 'E'), ('C',)]\n</code></pre>"},{"location":"uguide/utils/#practical-applications","title":"Practical Applications","text":""},{"location":"uguide/utils/#inspecting-batch-structure","title":"Inspecting Batch Structure","text":"<p>View all tensors in a batch:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; batch = {\n...     \"inputs\": {\n...         \"tokens\": torch.randn(32, 128),\n...         \"attention_mask\": torch.ones(32, 128),\n...     },\n...     \"targets\": torch.randint(0, 1000, (32,)),\n...     \"metadata\": {\n...         \"ids\": torch.arange(32),\n...     },\n... }\n&gt;&gt;&gt; print(\"Batch structure:\")\nBatch structure:\n&gt;&gt;&gt; for path, tensor in bfs_tensor(batch):\n...     print(f\"  {'.'.join(path)}: shape={tensor.shape}, dtype={tensor.dtype}\")\n  inputs.tokens: shape=torch.Size([32, 128]), dtype=torch.float32\n  inputs.attention_mask: shape=torch.Size([32, 128]), dtype=torch.float32\n  targets: shape=torch.Size([32]), dtype=torch.int64\n  metadata.ids: shape=torch.Size([32]), dtype=torch.int64\n</code></pre>"},{"location":"uguide/utils/#collecting-statistics","title":"Collecting Statistics","text":"<p>Gather information about all tensors:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import dfs_tensor\n&gt;&gt;&gt; data = {\n...     \"features\": torch.randn(100, 10),\n...     \"labels\": torch.randint(0, 2, (100,)),\n...     \"weights\": torch.rand(100),\n... }\n&gt;&gt;&gt; total_params = 0\n&gt;&gt;&gt; for path, tensor in dfs_tensor(data):\n...     num_elements = tensor.numel()\n...     total_params += num_elements\n...     print(f\"{'.'.join(path)}: {num_elements} elements\")\nfeatures: 1000 elements\nlabels: 100 elements\nweights: 100 elements\n&gt;&gt;&gt; print(f\"Total elements: {total_params}\")\nTotal elements: 1200\n</code></pre>"},{"location":"uguide/utils/#finding-specific-tensors","title":"Finding Specific Tensors","text":"<p>Search for tensors matching criteria:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; batch = {\n...     \"image\": torch.randn(3, 224, 224),\n...     \"mask\": torch.randint(0, 2, (224, 224)),\n...     \"label\": torch.tensor(5),\n... }\n&gt;&gt;&gt; # Find 2D or higher tensors\n&gt;&gt;&gt; print(\"Multi-dimensional tensors:\")\nMulti-dimensional tensors:\n&gt;&gt;&gt; for path, tensor in bfs_tensor(batch):\n...     if tensor.ndim &gt;= 2:\n...         print(f\"  {'.'.join(path)}: {tensor.shape}\")\n  image: torch.Size([3, 224, 224])\n  mask: torch.Size([224, 224])\n</code></pre>"},{"location":"uguide/utils/#validating-batch-dimensions","title":"Validating Batch Dimensions","text":"<p>Check that all tensors have consistent batch size:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import dfs_tensor\n&gt;&gt;&gt; def validate_batch_size(data, expected_batch_size):\n...     for path, tensor in dfs_tensor(data):\n...         if tensor.shape[0] != expected_batch_size:\n...             return False, f\"{'.'.join(path)} has batch size {tensor.shape[0]}\"\n...     return True, \"All tensors have correct batch size\"\n&gt;&gt;&gt; good_batch = {\n...     \"a\": torch.randn(32, 10),\n...     \"b\": torch.randn(32, 5),\n... }\n&gt;&gt;&gt; validate_batch_size(good_batch, 32)\n(True, 'All tensors have correct batch size')\n&gt;&gt;&gt; bad_batch = {\n...     \"a\": torch.randn(32, 10),\n...     \"b\": torch.randn(16, 5),  # Wrong batch size\n... }\n&gt;&gt;&gt; validate_batch_size(bad_batch, 32)\n(False, 'b has batch size 16')\n</code></pre>"},{"location":"uguide/utils/#memory-usage-analysis","title":"Memory Usage Analysis","text":"<p>Calculate memory footprint:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; def calculate_memory(data):\n...     total_bytes = 0\n...     for path, tensor in bfs_tensor(data):\n...         bytes_per_element = tensor.element_size()\n...         num_elements = tensor.numel()\n...         tensor_bytes = bytes_per_element * num_elements\n...         total_bytes += tensor_bytes\n...         print(f\"{'.'.join(path)}: {tensor_bytes / 1024:.2f} KB\")\n...     return total_bytes / (1024 * 1024)\n&gt;&gt;&gt; batch = {\n...     \"features\": torch.randn(1000, 512),  # float32\n...     \"labels\": torch.randint(0, 10, (1000,)),  # int64\n... }\n&gt;&gt;&gt; total_mb = calculate_memory(batch)\nfeatures: 2000.00 KB\nlabels: 7.81 KB\n&gt;&gt;&gt; print(f\"Total: {total_mb:.2f} MB\")\nTotal: 1.96 MB\n</code></pre>"},{"location":"uguide/utils/#comparing-bfs-vs-dfs","title":"Comparing BFS vs DFS","text":""},{"location":"uguide/utils/#when-to-use-bfs","title":"When to Use BFS","text":"<ul> <li>When you want to process all items at the same nesting level together</li> <li>For level-order processing</li> <li>When shallow nodes are more important than deep ones</li> <li>Memory-efficient for wide but shallow structures</li> </ul>"},{"location":"uguide/utils/#when-to-use-dfs","title":"When to Use DFS","text":"<ul> <li>When you want to fully explore each branch before moving to the next</li> <li>For recursive-style processing</li> <li>When deep nodes are more important</li> <li>Memory-efficient for deep but narrow structures</li> </ul>"},{"location":"uguide/utils/#example-comparison","title":"Example Comparison","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor, dfs_tensor\n&gt;&gt;&gt; data = {\n...     \"A\": torch.tensor([1]),\n...     \"B\": {\n...         \"C\": {\n...             \"D\": torch.tensor([2]),\n...         },\n...         \"E\": torch.tensor([3]),\n...     },\n... }\n&gt;&gt;&gt; print(\"BFS order:\")\nBFS order:\n&gt;&gt;&gt; for path, _ in bfs_tensor(data):\n...     print(f\"  {'.'.join(path)}\")\n  A\n  B.C.D\n  B.E\n&gt;&gt;&gt; print(\"DFS order:\")\nDFS order:\n&gt;&gt;&gt; for path, _ in dfs_tensor(data):\n...     print(f\"  {'.'.join(path)}\")\n  A\n  B.C.D\n  B.E\n</code></pre>"},{"location":"uguide/utils/#working-with-lists-and-tuples","title":"Working with Lists and Tuples","text":"<p>Both functions handle list and tuple structures:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; data = [\n...     torch.tensor([1, 2]),\n...     [\n...         torch.tensor([3, 4]),\n...         torch.tensor([5, 6]),\n...     ],\n... ]\n&gt;&gt;&gt; for path, tensor in bfs_tensor(data):\n...     print(f\"Index path {path}: {tensor.tolist()}\")\nIndex path (0,): [1, 2]\nIndex path (1, 0): [3, 4]\nIndex path (1, 1): [5, 6]\n</code></pre>"},{"location":"uguide/utils/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Traversal: Use BFS for level-order needs, DFS for branch-complete processing</li> <li>Path Tuples: Paths are returned as tuples that can be used to access nested elements</li> <li>Generator Efficiency: Both functions are generators, so they're memory efficient</li> <li>Type Checking: Functions work with any nested structure, not just dicts</li> <li>Immutable Paths: Don't modify the data structure during traversal</li> </ol>"},{"location":"uguide/utils/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Both functions are generators and don't load all data into memory</li> <li>DFS typically uses less memory for deep structures</li> <li>BFS is better for structures with many siblings at each level</li> <li>For very large nested structures, consider iterating in chunks</li> </ul>"},{"location":"uguide/utils/#common-patterns","title":"Common Patterns","text":""},{"location":"uguide/utils/#print-structure","title":"Print Structure","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import bfs_tensor\n&gt;&gt;&gt; def print_structure(data, max_depth=None):\n...     for path, tensor in bfs_tensor(data):\n...         if max_depth is None or len(path) &lt;= max_depth:\n...             indent = \"  \" * (len(path) - 1)\n...             name = path[-1]\n...             print(f\"{indent}{name}: {tensor.shape}\")\n&gt;&gt;&gt; batch = {\n...     \"inputs\": {\"x\": torch.randn(32, 10)},\n...     \"targets\": torch.randint(0, 10, (32,)),\n... }\n&gt;&gt;&gt; print_structure(batch)\nx: torch.Size([32, 10])\ntargets: torch.Size([32])\n</code></pre>"},{"location":"uguide/utils/#extract-all-tensors","title":"Extract All Tensors","text":"<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.utils import dfs_tensor\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([1, 2]),\n...     \"b\": {\"c\": torch.tensor([3, 4])},\n... }\n&gt;&gt;&gt; tensors = [tensor for _, tensor in dfs_tensor(data)]\n&gt;&gt;&gt; len(tensors)\n2\n</code></pre>"},{"location":"uguide/utils/#see-also","title":"See Also","text":"<ul> <li>Nested Operations - Operations on nested structures</li> <li>Recursive Module - Low-level recursive utilities</li> <li>API Reference - Complete function reference</li> </ul>"}]}