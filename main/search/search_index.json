{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":""},{"location":"#api-stability","title":"API stability","text":"<p> While <code>batchtensor</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>batchtensor</code> to a new version will possibly break any code that was using the old version of <code>batchtensor</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>batchtensor</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install batchtensor\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>batchtensor</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'batchtensor[all]'\n</code></pre> <p>This command also installed NumPy and PyTorch. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only NumPy is installed:</p> <pre><code>pip install batchtensor numpy\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>batchtensor</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/batchtensor.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate batchtensor\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>batchtensor</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/nested/","title":"Nested","text":""},{"location":"refs/nested/#batchtensor.nested","title":"batchtensor.nested","text":"<p>Contain functions to manipulate nested data.</p>"},{"location":"refs/nested/#batchtensor.nested.amax_along_batch","title":"batchtensor.nested.amax_along_batch","text":"<pre><code>amax_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = amax_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([8, 9]), 'b': tensor(4)}\n&gt;&gt;&gt; out = amax_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9]]), 'b': tensor([4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amax_along_seq","title":"batchtensor.nested.amax_along_seq","text":"<pre><code>amax_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The maximum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amax_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = amax_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([4, 9]), 'b': tensor([4])}\n&gt;&gt;&gt; out = amax_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[4], [9]]), 'b': tensor([[4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_batch","title":"batchtensor.nested.amin_along_batch","text":"<pre><code>amin_along_batch(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = amin_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 1]), 'b': tensor(0)}\n&gt;&gt;&gt; out = amin_along_batch(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1]]), 'b': tensor([0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.amin_along_seq","title":"batchtensor.nested.amin_along_seq","text":"<pre><code>amin_along_seq(data: Any, keepdim: bool = False) -&gt; Any\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The minimum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import amin_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = amin_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([0, 5]), 'b': tensor([0])}\n&gt;&gt;&gt; out = amin_along_seq(data, keepdim=True)\n&gt;&gt;&gt; out\n{'a': tensor([[0], [5]]), 'b': tensor([[0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_batch","title":"batchtensor.nested.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the batch dimension</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_batch\n&gt;&gt;&gt; data = {\n...     \"a\": torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]]),\n...     \"b\": torch.tensor([4, 3, 2, 1, 0]),\n... }\n&gt;&gt;&gt; out = argsort_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]]), 'b': tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = argsort_along_batch(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.argsort_along_seq","title":"batchtensor.nested.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    data: Any, descending: bool = False, **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Return the indices that sort each tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The indices that sort each tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import argsort_along_seq\n&gt;&gt;&gt; data = {'a': torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]]), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argsort_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 4, 0, 3], [0, 4, 3, 2, 1]]), 'b': tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = argsort_along_seq(data, descending=True)\n&gt;&gt;&gt; out\n{'a': tensor([[3, 0, 4, 1, 2], [1, 2, 3, 4, 0]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_batch","title":"batchtensor.nested.cat_along_batch","text":"<pre><code>cat_along_batch(\n    data: Sequence[dict[Hashable, Tensor]]\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_batch\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7, 8, 9]]), \"b\": torch.tensor([[17, 18, 19]])},\n... ]\n&gt;&gt;&gt; out = cat_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [4, 5, 6], [7, 8, 9]]),\n 'b': tensor([[10, 11, 12], [13, 14, 15], [17, 18, 19]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.cat_along_seq","title":"batchtensor.nested.cat_along_seq","text":"<pre><code>cat_along_seq(\n    data: Sequence[dict[Hashable, Tensor]]\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[dict[Hashable, Tensor]]</code> <p>The input data to concatenate. The dictionaries must have the same keys.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The concatenated tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import cat_along_seq\n&gt;&gt;&gt; data = [\n...     {\n...         \"a\": torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...         \"b\": torch.tensor([[10, 11, 12], [13, 14, 15]]),\n...     },\n...     {\"a\": torch.tensor([[7], [8]]), \"b\": torch.tensor([[17], [18]])},\n... ]\n&gt;&gt;&gt; out = cat_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 7], [4, 5, 6, 8]]),\n 'b': tensor([[10, 11, 12, 17], [13, 14, 15, 18]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_batch","title":"batchtensor.nested.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; outputs = chunk_along_batch(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.chunk_along_seq","title":"batchtensor.nested.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    data: dict[Hashable, Tensor], chunks: int\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import chunk_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; outputs = chunk_along_seq(data, chunks=3)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_batch","title":"batchtensor.nested.index_select_along_batch","text":"<pre><code>index_select_along_batch(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_batch\n&gt;&gt;&gt; tensors = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [8, 9]]), 'b': tensor([2, 0])}\n&gt;&gt;&gt; out = index_select_along_batch(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[8, 9], [6, 7], [4, 5], [2, 3], [0, 1]]), 'b': tensor([0, 1, 2, 3, 4])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.index_select_along_seq","title":"batchtensor.nested.index_select_along_seq","text":"<pre><code>index_select_along_seq(data: Any, index: Tensor) -&gt; Any\n</code></pre> <p>Return the tensors which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The indexed tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import index_select_along_seq\n&gt;&gt;&gt; tensors = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 4], [7, 9]]), 'b': tensor([[2, 0]])}\n&gt;&gt;&gt; out = index_select_along_seq(tensors, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 3, 2, 1, 0], [9, 8, 7, 6, 5]]), 'b': tensor([[0, 1, 2, 3, 4]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_batch","title":"batchtensor.nested.permute_along_batch","text":"<pre><code>permute_along_batch(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the batch dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = permute_along_batch(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [2, 3], [6, 7], [0, 1], [8, 9]]), 'b': tensor([2, 3, 1, 4, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.permute_along_seq","title":"batchtensor.nested.permute_along_seq","text":"<pre><code>permute_along_seq(data: Any, permutation: Tensor) -&gt; Any\n</code></pre> <p>Permute all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The data with permuted tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import permute_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(2, 5), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = permute_along_seq(data, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\n{'a': tensor([[2, 1, 3, 0, 4], [7, 6, 8, 5, 9]]), 'b': tensor([[2, 3, 1, 4, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.repeat_along_seq","title":"batchtensor.nested.repeat_along_seq","text":"<pre><code>repeat_along_seq(data: Any, repeats: int) -&gt; Any\n</code></pre> <p>Repeat all the tensors along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>repeats</code> <code>int</code> <p>Specifies the number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The tensors repeated along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import repeat_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = repeat_along_seq(data, 2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4], [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]]),\n 'b': tensor([[4, 3, 2, 1, 0, 4, 3, 2, 1, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_batch","title":"batchtensor.nested.select_along_batch","text":"<pre><code>select_along_batch(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = select_along_batch(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([4, 5]), 'b': tensor(2)}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.select_along_seq","title":"batchtensor.nested.select_along_seq","text":"<pre><code>select_along_seq(data: Any, index: int) -&gt; Any\n</code></pre> <p>Slice the tensors along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import select_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = select_along_seq(data, index=2)\n&gt;&gt;&gt; out\n{'a': tensor([2, 7]), 'b': tensor([2])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_batch","title":"batchtensor.nested.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    data: dict[Hashable, Tensor],\n    generator: Generator | None = None,\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = shuffle_along_batch(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([...])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.shuffle_along_seq","title":"batchtensor.nested.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    data: dict[Hashable, Tensor],\n    generator: Generator | None = None,\n) -&gt; dict[Hashable, Tensor]\n</code></pre> <p>Shuffle all the tensors along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[Hashable, Tensor]</code> <p>The data with shuffled tensors along the sequence dimension. The output data has the same structure as the input data.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import shuffle_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(2, 5), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = shuffle_along_seq(data)\n&gt;&gt;&gt; out\n{'a': tensor([[...]]), 'b': tensor([[...]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_batch","title":"batchtensor.nested.slice_along_batch","text":"<pre><code>slice_along_batch(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; out = slice_along_batch(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[4, 5], [6, 7], [8, 9]]), 'b': tensor([2, 1, 0])}\n&gt;&gt;&gt; out = slice_along_batch(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [2, 3], [4, 5]]), 'b': tensor([4, 3, 2])}\n&gt;&gt;&gt; out = slice_along_batch(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1], [4, 5], [8, 9]]), 'b': tensor([4, 2, 0])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.slice_along_seq","title":"batchtensor.nested.slice_along_seq","text":"<pre><code>slice_along_seq(\n    data: Any,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Any\n</code></pre> <p>Slice all the tensors along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The input data. Each item must be a tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Any</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import slice_along_seq\n&gt;&gt;&gt; data = {'a': torch.arange(10).view(2, 5), 'b': torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; out = slice_along_seq(data, start=2)\n&gt;&gt;&gt; out\n{'a': tensor([[2, 3, 4], [7, 8, 9]]), 'b': tensor([[2, 1, 0]])}\n&gt;&gt;&gt; out = slice_along_seq(data, stop=3)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 1, 2], [5, 6, 7]]), 'b': tensor([[4, 3, 2]])}\n&gt;&gt;&gt; out = slice_along_seq(data, step=2)\n&gt;&gt;&gt; out\n{'a': tensor([[0, 2, 4], [5, 7, 9]]), 'b': tensor([[4, 2, 0]])}\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_batch","title":"batchtensor.nested.split_along_batch","text":"<pre><code>split_along_batch(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension of the tensors. All the tensors should have the     same batch size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_batch\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(5, 2), \"b\": torch.tensor([4, 3, 2, 1, 0])}\n&gt;&gt;&gt; outputs = split_along_batch(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [2, 3]]), 'b': tensor([4, 3])},\n {'a': tensor([[4, 5], [6, 7]]), 'b': tensor([2, 1])},\n {'a': tensor([[8, 9]]), 'b': tensor([0])})\n</code></pre>"},{"location":"refs/nested/#batchtensor.nested.split_along_seq","title":"batchtensor.nested.split_along_seq","text":"<pre><code>split_along_seq(\n    data: dict[Hashable, Tensor],\n    split_size_or_sections: int | Sequence[int],\n) -&gt; tuple[dict[Hashable, Tensor], ...]\n</code></pre> <p>Split all the tensors into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension of the tensors. All the tensors should have the     same sequence size.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[Hashable, Tensor]</code> <p>The input data. Each item must be a tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>tuple[dict[Hashable, Tensor], ...]</code> <p>The data chuncks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.nested import split_along_seq\n&gt;&gt;&gt; data = {\"a\": torch.arange(10).view(2, 5), \"b\": torch.tensor([[4, 3, 2, 1, 0]])}\n&gt;&gt;&gt; outputs = split_along_seq(data, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n({'a': tensor([[0, 1], [5, 6]]), 'b': tensor([[4, 3]])},\n {'a': tensor([[2, 3], [7, 8]]), 'b': tensor([[2, 1]])},\n {'a': tensor([[4], [9]]), 'b': tensor([[0]])})\n</code></pre>"},{"location":"refs/tensor/","title":"Tensor","text":""},{"location":"refs/tensor/#batchtensor.tensor","title":"batchtensor.tensor","text":"<p>Contain functions to manipulate tensors.</p>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_batch","title":"batchtensor.tensor.amax_along_batch","text":"<pre><code>amax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = amax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([8, 9])\n&gt;&gt;&gt; out = amax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amax_along_seq","title":"batchtensor.tensor.amax_along_seq","text":"<pre><code>amax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amax_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = amax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 9])\n&gt;&gt;&gt; out = amax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_batch","title":"batchtensor.tensor.amin_along_batch","text":"<pre><code>amin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = amin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 1])\n&gt;&gt;&gt; out = amin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.amin_along_seq","title":"batchtensor.tensor.amin_along_seq","text":"<pre><code>amin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The minimum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import amin_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = amin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 5])\n&gt;&gt;&gt; out = amin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_batch","title":"batchtensor.tensor.argmax_along_batch","text":"<pre><code>argmax_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = argmax_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4, 4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmax_along_seq","title":"batchtensor.tensor.argmax_along_seq","text":"<pre><code>argmax_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the maximum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the maximum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmax_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = argmax_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([4, 4])\n&gt;&gt;&gt; out = argmax_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4], [4]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_batch","title":"batchtensor.tensor.argmin_along_batch","text":"<pre><code>argmin_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = argmin_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argmin_along_seq","title":"batchtensor.tensor.argmin_along_seq","text":"<pre><code>argmin_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the indices of the minimum value of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices of the minimum value of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argmin_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = argmin_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([0, 0])\n&gt;&gt;&gt; out = argmin_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[0], [0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_batch","title":"batchtensor.tensor.argsort_along_batch","text":"<pre><code>argsort_along_batch(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the batch dimension in ascending order by value.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the batch dimension</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_batch\n&gt;&gt;&gt; tensor = torch.tensor([[2, 6], [0, 3], [4, 9], [8, 1], [5, 7]])\n&gt;&gt;&gt; out = argsort_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[1, 3], [0, 1], [2, 0], [4, 4], [3, 2]])\n&gt;&gt;&gt; out = argsort_along_batch(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 2], [4, 4], [2, 0], [0, 1], [1, 3]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.argsort_along_seq","title":"batchtensor.tensor.argsort_along_seq","text":"<pre><code>argsort_along_seq(\n    tensor: Tensor, descending: bool = False, **kwargs: Any\n) -&gt; Tensor\n</code></pre> <p>Return the indices that sort a tensor along the sequence dimension in ascending order by value.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>descending</code> <code>bool</code> <p>Controls the sorting order (ascending or descending).</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>Additional keywords arguments for <code>torch.argsort</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices that sort a tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import argsort_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[7, 3, 0, 8, 5], [1, 9, 6, 4, 2]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[2, 1, 4, 0, 3],\n        [0, 4, 3, 2, 1]])\n&gt;&gt;&gt; out = argsort_along_seq(tensor, descending=True)\n&gt;&gt;&gt; out\ntensor([[3, 0, 4, 1, 2],\n        [1, 2, 3, 4, 0]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_batch","title":"batchtensor.tensor.cat_along_batch","text":"<pre><code>cat_along_batch(\n    tensors: list[Tensor] | tuple[Tensor, ...]\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the batch dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>Specifies the batches to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_batch\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11, 12], [13, 14, 15]]),\n... ]\n&gt;&gt;&gt; out = cat_along_batch(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2],\n        [ 4,  5,  6],\n        [10, 11, 12],\n        [13, 14, 15]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.cat_along_seq","title":"batchtensor.tensor.cat_along_seq","text":"<pre><code>cat_along_seq(\n    tensors: list[Tensor] | tuple[Tensor, ...]\n) -&gt; Tensor\n</code></pre> <p>Concatenate the given tensors in the sequence dimension.</p> <p>All tensors must either have the same data type and shape (except in the concatenating dimension) or be empty.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>list[Tensor] | tuple[Tensor, ...]</code> <p>Specifies the batches to concatenate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The concatenated tensors along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import cat_along_seq\n&gt;&gt;&gt; tensors = [\n...     torch.tensor([[0, 1, 2], [4, 5, 6]]),\n...     torch.tensor([[10, 11], [12, 13]]),\n... ]\n&gt;&gt;&gt; out = cat_along_seq(tensors)\n&gt;&gt;&gt; out\ntensor([[ 0,  1,  2, 10, 11],\n        [ 4,  5,  6, 12, 13]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_batch","title":"batchtensor.tensor.chunk_along_batch","text":"<pre><code>chunk_along_batch(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; outputs = chunk_along_batch(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.chunk_along_seq","title":"batchtensor.tensor.chunk_along_seq","text":"<pre><code>chunk_along_seq(\n    tensor: Tensor, chunks: int\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the input tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>chunks</code> <code>int</code> <p>Number of chunks to return.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, ...]</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import chunk_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; outputs = chunk_along_seq(tensor, chunks=3)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_batch","title":"batchtensor.tensor.index_select_along_batch","text":"<pre><code>index_select_along_batch(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the batch dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [8, 9]])\n&gt;&gt;&gt; out = index_select_along_batch(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[8, 9],\n        [6, 7],\n        [4, 5],\n        [2, 3],\n        [0, 1]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.index_select_along_seq","title":"batchtensor.tensor.index_select_along_seq","text":"<pre><code>index_select_along_seq(\n    tensor: Tensor, index: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return a new tensor which indexes the <code>input</code> tensor along the sequence dimension using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>Tensor</code> <p>The 1-D tensor containing the indices to index.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indexed tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import index_select_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([2, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 4],\n        [7, 9]])\n&gt;&gt;&gt; out = index_select_along_seq(tensor, torch.tensor([4, 3, 2, 1, 0]))\n&gt;&gt;&gt; out\ntensor([[4, 3, 2, 1, 0],\n        [9, 8, 7, 6, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_batch","title":"batchtensor.tensor.max_along_batch","text":"<pre><code>max_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = max_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([8, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[8, 9]]),\nindices=tensor([[4, 4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.max_along_seq","title":"batchtensor.tensor.max_along_seq","text":"<pre><code>max_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; max\n</code></pre> <p>Return the maximum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>max</code> <p>The first tensor will be populated with the maximum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import max_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = max_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([4, 9]),\nindices=tensor([4, 4]))\n&gt;&gt;&gt; out = max_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.max(\nvalues=tensor([[4], [9]]),\nindices=tensor([[4], [4]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_batch","title":"batchtensor.tensor.mean_along_batch","text":"<pre><code>mean_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10, dtype=torch.float).view(5, 2)\n&gt;&gt;&gt; out = mean_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([4., 5.])\n&gt;&gt;&gt; out = mean_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[4., 5.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.mean_along_seq","title":"batchtensor.tensor.mean_along_seq","text":"<pre><code>mean_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the mean of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import mean_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10, dtype=torch.float).view(2, 5)\n&gt;&gt;&gt; out = mean_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([2., 7.])\n&gt;&gt;&gt; out = mean_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[2.], [7.]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_batch","title":"batchtensor.tensor.median_along_batch","text":"<pre><code>median_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = median_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([4, 5]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[4, 5]]),\nindices=tensor([[2, 2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.median_along_seq","title":"batchtensor.tensor.median_along_seq","text":"<pre><code>median_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; median\n</code></pre> <p>Return the median of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>median</code> <p>The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import median_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = median_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([2, 7]),\nindices=tensor([2, 2]))\n&gt;&gt;&gt; out = median_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.median(\nvalues=tensor([[2], [7]]),\nindices=tensor([[2], [2]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_batch","title":"batchtensor.tensor.min_along_batch","text":"<pre><code>min_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = min_along_batch(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 1]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0, 1]]),\nindices=tensor([[0, 0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.min_along_seq","title":"batchtensor.tensor.min_along_seq","text":"<pre><code>min_along_seq(tensor: Tensor, keepdim: bool = False) -&gt; min\n</code></pre> <p>Return the minimum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>min</code> <p>The first tensor will be populated with the minimum values and the second tensor, which must have dtype long, with their indices in the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import min_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = min_along_seq(tensor)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([0, 5]),\nindices=tensor([0, 0]))\n&gt;&gt;&gt; out = min_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntorch.return_types.min(\nvalues=tensor([[0], [5]]),\nindices=tensor([[0], [0]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_batch","title":"batchtensor.tensor.permute_along_batch","text":"<pre><code>permute_along_batch(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the batch dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the batch dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the batch dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = permute_along_batch(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [2, 3],\n        [6, 7],\n        [0, 1],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.permute_along_seq","title":"batchtensor.tensor.permute_along_seq","text":"<pre><code>permute_along_seq(\n    tensor: Tensor, permutation: Tensor\n) -&gt; Tensor\n</code></pre> <p>Permute the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>permutation</code> <code>Tensor</code> <p>The 1-D tensor containing the indices of the permutation. The shape should match the sequence dimension of the tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with permuted data along the sequence dimension.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the shape of the permutation does not match the sequence dimension of the tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import permute_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = permute_along_seq(tensor, torch.tensor([2, 1, 3, 0, 4]))\n&gt;&gt;&gt; out\ntensor([[2, 1, 3, 0, 4],\n        [7, 6, 8, 5, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_batch","title":"batchtensor.tensor.prod_along_batch","text":"<pre><code>prod_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = prod_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([  0, 945])\n&gt;&gt;&gt; out = prod_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[  0, 945]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.prod_along_seq","title":"batchtensor.tensor.prod_along_seq","text":"<pre><code>prod_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the product of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The product of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import prod_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = prod_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([    0, 15120])\n&gt;&gt;&gt; out = prod_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[    0], [15120]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.repeat_along_seq","title":"batchtensor.tensor.repeat_along_seq","text":"<pre><code>repeat_along_seq(tensor: Tensor, repeats: int) -&gt; Tensor\n</code></pre> <p>Repeat the data along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>repeats</code> <code>int</code> <p>Specifies the number of times to repeat the data along the sequence dimension.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A new tensor with the data repeated along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import repeat_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = repeat_along_seq(tensor, 2)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9, 5, 6, 7, 8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_batch","title":"batchtensor.tensor.select_along_batch","text":"<pre><code>select_along_batch(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the batch dimension at the given index.</p> <p>This function returns a view of the original tensor with the batch dimension removed.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = select_along_batch(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([4, 5])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.select_along_seq","title":"batchtensor.tensor.select_along_seq","text":"<pre><code>select_along_seq(tensor: Tensor, index: int) -&gt; Tensor\n</code></pre> <p>Slice the input tensor along the sequence dimension at the given index.</p> <p>This function returns a view of the original tensor with the sequence dimension removed.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>index</code> <code>int</code> <p>The index to select with.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import select_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = select_along_seq(tensor, index=2)\n&gt;&gt;&gt; out\ntensor([2, 7])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_batch","title":"batchtensor.tensor.shuffle_along_batch","text":"<pre><code>shuffle_along_batch(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = shuffle_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.shuffle_along_seq","title":"batchtensor.tensor.shuffle_along_seq","text":"<pre><code>shuffle_along_seq(\n    tensor: Tensor, generator: Generator | None = None\n) -&gt; Tensor\n</code></pre> <p>Shuffle the tensor along the batch dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to split.</p> required <code>generator</code> <code>Generator | None</code> <p>Specifies an optional random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The shuffled tensor.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import shuffle_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = shuffle_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([[...]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_batch","title":"batchtensor.tensor.slice_along_batch","text":"<pre><code>slice_along_batch(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = slice_along_batch(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[4, 5],\n        [6, 7],\n        [8, 9]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n&gt;&gt;&gt; out = slice_along_batch(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 1],\n        [4, 5],\n        [8, 9]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.slice_along_seq","title":"batchtensor.tensor.slice_along_seq","text":"<pre><code>slice_along_seq(\n    tensor: Tensor,\n    start: int = 0,\n    stop: int | None = None,\n    step: int = 1,\n) -&gt; Tensor\n</code></pre> <p>Slice the tensor along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>start</code> <code>int</code> <p>Specifies the index where the slicing of object starts.</p> <code>0</code> <code>stop</code> <code>int | None</code> <p>Specifies the index where the slicing of object stops. <code>None</code> means last.</p> <code>None</code> <code>step</code> <code>int</code> <p>Specifies the increment between each index for slicing.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sliced tensor along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import slice_along_seq\n&gt;&gt;&gt; tensor = torch.tensor([[0, 1, 2, 3, 4], [9, 8, 7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, start=2)\n&gt;&gt;&gt; out\ntensor([[2, 3, 4],\n        [7, 6, 5]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, stop=3)\n&gt;&gt;&gt; out\ntensor([[0, 1, 2],\n        [9, 8, 7]])\n&gt;&gt;&gt; out = slice_along_seq(tensor, step=2)\n&gt;&gt;&gt; out\ntensor([[0, 2, 4],\n        [9, 7, 5]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_batch","title":"batchtensor.tensor.split_along_batch","text":"<pre><code>split_along_batch(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the batch dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; outputs = split_along_batch(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [2, 3]]),\n tensor([[4, 5], [6, 7]]),\n tensor([[8, 9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.split_along_seq","title":"batchtensor.tensor.split_along_seq","text":"<pre><code>split_along_seq(\n    tensor: Tensor,\n    split_size_or_sections: int | Sequence[int],\n) -&gt; Tensor\n</code></pre> <p>Split the tensor into chunks along the sequence dimension.</p> <p>Each chunk is a view of the original tensor.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>split_size_or_sections</code> <code>int | Sequence[int]</code> <p>Size of a single chunk or list of sizes for each chunk</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor chunks.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import split_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; outputs = split_along_seq(tensor, split_size_or_sections=2)\n&gt;&gt;&gt; outputs\n(tensor([[0, 1], [5, 6]]),\n tensor([[2, 3], [7, 8]]),\n tensor([[4], [9]]))\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_batch","title":"batchtensor.tensor.sum_along_batch","text":"<pre><code>sum_along_batch(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the batch dimension.</p> Note <p>This function assumes the batch dimension is the first     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the batch dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_batch\n&gt;&gt;&gt; tensor = torch.arange(10).view(5, 2)\n&gt;&gt;&gt; out = sum_along_batch(tensor)\n&gt;&gt;&gt; out\ntensor([20, 25])\n&gt;&gt;&gt; out = sum_along_batch(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[20, 25]])\n</code></pre>"},{"location":"refs/tensor/#batchtensor.tensor.sum_along_seq","title":"batchtensor.tensor.sum_along_seq","text":"<pre><code>sum_along_seq(\n    tensor: Tensor, keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Return the sum of all elements along the sequence dimension.</p> Note <p>This function assumes the sequence dimension is the second     dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of all elements along the sequence dimension.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from batchtensor.tensor import sum_along_seq\n&gt;&gt;&gt; tensor = torch.arange(10).view(2, 5)\n&gt;&gt;&gt; out = sum_along_seq(tensor)\n&gt;&gt;&gt; out\ntensor([10, 35])\n&gt;&gt;&gt; out = sum_along_seq(tensor, keepdim=True)\n&gt;&gt;&gt; out\ntensor([[10], [35]])\n</code></pre>"}]}